{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'SPServer'...\n"
     ]
    }
   ],
   "source": [
    "subprocess.run([\"git\", \"clone\", \"https://github.com/structuralbioinformatics/SPServer\"])\n",
    "\n",
    "for item in os.listdir(\"SPServer\"):\n",
    "    source = os.path.join(\"SPServer\", item)\n",
    "    destination = os.path.join(\".\", item)\n",
    "    if os.path.exists(destination):\n",
    "        if os.path.isdir(destination):\n",
    "            shutil.rmtree(destination)\n",
    "        else:\n",
    "            os.remove(destination)\n",
    "    shutil.move(source, destination)\n",
    "\n",
    "shutil.rmtree(\"SPServer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: 2to3 in /Users/zain/miniconda3/lib/python3.11/site-packages (1.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pip', 'install', '2to3'], returncode=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"pip\", \"install\", \"2to3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/bin/2to3:3: DeprecationWarning: lib2to3 package is deprecated and may not be able to parse Python 3.10+\n",
      "  from lib2to3.main import main\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: No changes to ./SPServerFold.py\n",
      "RefactoringTool: Refactored ./SPServerPPI.py\n",
      "RefactoringTool: Refactored ./cif2pdb.py\n",
      "RefactoringTool: Refactored ./BioLib/__init__.py\n",
      "RefactoringTool: Refactored ./BioLib/Algebra/Transforms.py\n",
      "RefactoringTool: Refactored ./BioLib/Algebra/__init__.py\n",
      "RefactoringTool: Refactored ./BioLib/Docking/FTDock.py\n",
      "RefactoringTool: Refactored ./BioLib/Docking/HEXDock.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ./SPServerPPI.py\t(original)\n",
      "+++ ./SPServerPPI.py\t(refactored)\n",
      "@@ -208,7 +208,7 @@\n",
      "                 elif structure_file[-6:] == '.l.pdb':\n",
      "                     ligands.append(structure_file)\n",
      "                 else:\n",
      "-                    print('Incorrect file!:{}. It must end with .r.pdb or .l.pdb\\n'.format(structure_file))\n",
      "+                    print(('Incorrect file!:{}. It must end with .r.pdb or .l.pdb\\n'.format(structure_file)))\n",
      "                     self.print_error(2, structure_file)\n",
      "                     self.write_output_file(self.xml_errors, self.xml_err_file)\n",
      "                     self.write_output_file(self.xml_result, self.xml_out_file) # Output an empty results file\n",
      "@@ -228,7 +228,7 @@\n",
      "                         if fileExist(receptor) and fileExist(ligand):\n",
      "                             pairs.append([receptor, ligand])\n",
      "                         else:\n",
      "-                            print('The receptor {} has not a corresponding ligand! Please, name the receptor and ligand equally.\\n'.format(file))\n",
      "+                            print(('The receptor {} has not a corresponding ligand! Please, name the receptor and ligand equally.\\n'.format(file)))\n",
      "                             self.print_error(3, pdb_name)\n",
      "                             self.write_output_file(self.xml_errors, self.xml_err_file)\n",
      "                             self.write_output_file(self.xml_result, self.xml_out_file) # Output an empty results file\n",
      "--- ./cif2pdb.py\t(original)\n",
      "+++ ./cif2pdb.py\t(refactored)\n",
      "@@ -29,7 +29,7 @@\n",
      "     \n",
      "     # Print info about renaming of chains\n",
      "     if verbose:\n",
      "-        for new,old in chainmap.items():\n",
      "+        for new,old in list(chainmap.items()):\n",
      "             if new != old:\n",
      "                 logging.info(\"Renaming chain {0} to {1}\".format(old,new))\n",
      " \n",
      "--- ./BioLib/__init__.py\t(original)\n",
      "+++ ./BioLib/__init__.py\t(refactored)\n",
      "@@ -1,8 +1,8 @@\n",
      "-from Algebra import *\n",
      "-from Docking import *\n",
      "-from Fold import *\n",
      "-from ILoops import *\n",
      "+from .Algebra import *\n",
      "+from .Docking import *\n",
      "+from .Fold import *\n",
      "+from .ILoops import *\n",
      " #from Linker import *\n",
      "-from Sequence import *\n",
      "-from Structure import *\n",
      "-from Tools import *\n",
      "+from .Sequence import *\n",
      "+from .Structure import *\n",
      "+from .Tools import *\n",
      "--- ./BioLib/Algebra/Transforms.py\t(original)\n",
      "+++ ./BioLib/Algebra/Transforms.py\t(refactored)\n",
      "@@ -30,7 +30,7 @@\n",
      "                 'rzxy': (1, 1, 0, 1), 'ryxy': (1, 1, 1, 1), 'ryxz': (2, 0, 0, 1),\n",
      "                 'rzxz': (2, 0, 1, 1), 'rxyz': (2, 1, 0, 1), 'rzyz': (2, 1, 1, 1) }\n",
      " \n",
      "-_TUPLE2AXES = dict((v, k) for k, v in _AXES2TUPLE.items())\n",
      "+_TUPLE2AXES = dict((v, k) for k, v in list(_AXES2TUPLE.items()))\n",
      " \n",
      " # axis sequences for Euler angles\n",
      " _NEXT_AXIS = [1, 2, 0, 1]\n",
      "--- ./BioLib/Algebra/__init__.py\t(original)\n",
      "+++ ./BioLib/Algebra/__init__.py\t(refactored)\n",
      "@@ -1 +1 @@\n",
      "-from Transforms import *\n",
      "+from .Transforms import *\n",
      "--- ./BioLib/Docking/FTDock.py\t(original)\n",
      "+++ ./BioLib/Docking/FTDock.py\t(refactored)\n",
      "@@ -130,7 +130,7 @@\n",
      "         ligandRMSD = {}\n",
      "         for decoy in self.get_decoys():\n",
      "             ligandRMSD[decoy.get_id()] = self.get_ligand_RMSD(static, mobile, native, decoy)\n",
      "-            print str(decoy.get_id())+'\\t'+str(ligandRMSD[decoy.get_id()])\n",
      "+            print(str(decoy.get_id())+'\\t'+str(ligandRMSD[decoy.get_id()]))\n",
      "         return ligandRMSD\n",
      "     \n",
      "     def print_RMSD(self, static, mobile, native, RMSDFile):\n",
      "@@ -139,7 +139,7 @@\n",
      "         '''\n",
      "         RMSDfo = open(RMSDFile, 'w')\n",
      "         ligandRMSD = self.get_decoys_ligand_RMSD(static, mobile, native)\n",
      "-        for key, value in sorted(ligandRMSD.iterkeys(), key=lambda (k,v): (v,k)):\n",
      "+        for key, value in sorted(iter(ligandRMSD.keys()), key=lambda k_v: (k_v[1],k_v[0])):\n",
      "             RMSDfo.write('%d\\t%.3f\\n' % (key, value))\n",
      "         RMSDfo.close()\n",
      " \n",
      "--- ./BioLib/Docking/HEXDock.py\t(original)\n",
      "+++ ./BioLib/Docking/HEXDock.py\t(refactored)\n",
      "@@ -65,9 +65,9 @@\n",
      "                         compatible = False\n",
      "             if compatible:\n",
      "                 filtered_decoys.append(decoy)\n",
      "-                print \"decoy %d appended\" % decoy.get_cluster()\n",
      "+                print(\"decoy %d appended\" % decoy.get_cluster())\n",
      "             else:\n",
      "-                print \"decoy %d skipped\" % decoy.get_cluster()\n",
      "+                print(\"decoy %d skipped\" % decoy.get_cluster())\n",
      "         self.decoys = filtered_decoys\n",
      " \n",
      "     def print_structures(self, static_structure, mobile_structure, pdbFile, singlePDB=False):\n",
      "--- ./BioLib/Docking/ScoringFunctions.py\t(original)\n",
      "+++ ./BioLib/Docking/ScoringFunctions.py\t(refactored)\n",
      "@@ -46,13 +46,13 @@\n",
      "                       'polar-polar'     : 0,\n",
      "                       'polar-apolar'    : 0,\n",
      "                       'apolar-apolar'   : 0 }\n",
      "-        for x in xrange(len(self.contact_distances)):\n",
      "+        for x in range(len(self.contact_distances)):\n",
      "             res_type1 = self.contact_pos1[x].get_type()\n",
      "             res_type2 = self.contact_pos2[x].get_type()\n",
      "-            if ic_scores.has_key(self.res_types[res_type1]+'-'+self.res_types[res_type2]):\n",
      "+            if self.res_types[res_type1]+'-'+self.res_types[res_type2] in ic_scores:\n",
      "                 ic_scores[self.res_types[res_type1]+'-'+self.res_types[res_type2]] += 1\n",
      "                 ic_scores['Total'] += 1\n",
      "-            elif ic_scores.has_key(self.res_types[res_type2]+'-'+self.res_types[res_type1]):\n",
      "+            elif self.res_types[res_type2]+'-'+self.res_types[res_type1] in ic_scores:\n",
      "                 ic_scores[self.res_types[res_type2]+'-'+self.res_types[res_type1]] += 1\n",
      "                 ic_scores['Total'] += 1\n",
      "         return ic_scores\n",
      "@@ -78,7 +78,7 @@\n",
      "         Calculate the absolute electrostatic energy\n",
      "         '''\n",
      "         Elec_energy = 0\n",
      "-        for x in xrange(len(self.contact_distances)):\n",
      "+        for x in range(len(self.contact_distances)):\n",
      "             q1 = self.contact_pos1[x].get_charge()\n",
      "             q2 = self.contact_pos2[x].get_charge()\n",
      "             r2  = self.contact_distances[x]**2\n",
      "@@ -90,9 +90,9 @@\n",
      "         Calculate the normalized electrostatic energy\n",
      "         '''\n",
      "         Elec_distribution = []\n",
      "-        for x in xrange(randoms):\n",
      "+        for x in range(randoms):\n",
      "             Elec_energy = 0\n",
      "-            for x in xrange(len(self.contact_distances)):\n",
      "+            for x in range(len(self.contact_distances)):\n",
      "                 random_pos1 = random.choice(self.interaction.get_structure1().get_residues())\n",
      "                 random_pos2 = random.choice(self.interaction.get_structure2().get_residues())\n",
      "                 q1 = random_pos1.get_charge()\n",
      "@@ -123,7 +123,7 @@\n",
      "         '''\n",
      "         Polar_score = 0\n",
      "         Apolar_score = 0\n",
      "-        for x in xrange(len(self.contact_distances)):\n",
      "+        for x in range(len(self.contact_distances)):\n",
      "             if self.contact_pos1[x].is_polar() and self.contact_pos2[x].is_polar():\n",
      "                 Polar_score += 1\n",
      "             if not self.contact_pos1[x].is_polar() and not self.contact_pos2[x].is_polar():\n",
      "@@ -136,10 +136,10 @@\n",
      "         '''\n",
      "         Polar_distribution = []\n",
      "         Apolar_distribution = []\n",
      "-        for x in xrange(randoms):\n",
      "+        for x in range(randoms):\n",
      "             Polar_score = 0\n",
      "             Apolar_score = 0\n",
      "-            for x in xrange(len(self.contact_distances)):\n",
      "+            for x in range(len(self.contact_distances)):\n",
      "                 random_pos1 = random.choice(self.interaction.get_structure1().get_residues())\n",
      "                 random_pos2 = random.choice(self.interaction.get_structure2().get_residues())\n",
      "                 if self.random_pos1.is_polar() and self.random_pos2.is_polar():\n",
      "--- ./BioLib/Docking/SplitPotentials.py\t(original)\n",
      "+++ ./BioLib/Docking/SplitPotentials.py\t(refactored)\n",
      "@@ -43,7 +43,7 @@\n",
      "         Getters\n",
      "     \"\"\"\n",
      "     def get_value_by_label(self, label):\n",
      "-        if self.original.has_key(label):\n",
      "+        if label in self.original:\n",
      "             return self.original[label]\n",
      "         \n",
      "     def get_value_by_label_str(self,label):\n",
      "@@ -59,7 +59,7 @@\n",
      "         return \"\\t\".join(outarray)\n",
      "     \n",
      "     def get_normalized_value_by_label(self, label):\n",
      "-        if self.normalized.has_key(label):\n",
      "+        if label in self.normalized:\n",
      "             return self.normalized[label]\n",
      "         \n",
      "     def get_normalized_value_by_label_str(self,label):\n",
      "@@ -85,7 +85,7 @@\n",
      "     \"\"\"\n",
      "     def set_random_vector(self, vector, label):    \n",
      " \n",
      "-        if not self.normalized.has_key(label):\n",
      "+        if label not in self.normalized:\n",
      "             return\n",
      "         \n",
      "         mean = numpy.mean(vector)\n",
      "@@ -176,7 +176,7 @@\n",
      "         ori_ppDist,         cof_ppDist          = 0, 0\n",
      "         ori_combined,       cof_combined,       max_combined,       min_combined,           men_combined        = 0, 0, 0, 0 ,0\n",
      " \n",
      "-        for x in xrange(len(self.str1_contact_pos)):\n",
      "+        for x in range(len(self.str1_contact_pos)):\n",
      "             residue1 = self.realInteraction.get_structure1().get_residue_by_num(self.str1_contact_pos[x])\n",
      "             residue2 = self.realInteraction.get_structure2().get_residue_by_num(self.str2_contact_pos[x])\n",
      "             \n",
      "@@ -288,7 +288,7 @@\n",
      "         rdU_combined        = numpy.zeros((randoms+1), float)\n",
      "         rdL_combined        = numpy.zeros((randoms+1), float)\n",
      "         \n",
      "-        for rad in xrange(randoms):\n",
      "+        for rad in range(randoms):\n",
      "             # Create random selection of positions for str1 and str2\n",
      "             # We pick positions by array position as we do not care what are we picking and will avoid\n",
      "             #    errors in case of gaps in the structure.\n",
      "@@ -298,7 +298,7 @@\n",
      "                                                  range=len(self.realInteraction.get_structure1().get_residues()))\n",
      "             rad_str2 = self.randomise_interaface(list=self.str2_contact_pos, \n",
      "                                                  range=len(self.realInteraction.get_structure2().get_residues()))\n",
      "-            for x in xrange(len(rad_str1)):\n",
      "+            for x in range(len(rad_str1)):\n",
      "                 residue1 = self.realInteraction.get_structure1().get_residues()[rad_str1[x]]\n",
      "                 residue2 = self.realInteraction.get_structure2().get_residues()[rad_str2[x]]\n",
      "                 \n",
      "@@ -395,18 +395,18 @@\n",
      "     \"\"\"\n",
      "     def get_ppResidues(self,k1,k2,distance):\n",
      "         d = int(math.floor(distance))\n",
      "-        if self.ppResidues.has_key(k1) and self.ppResidues[k1].has_key(k2):\n",
      "+        if k1 in self.ppResidues and k2 in self.ppResidues[k1]:\n",
      "             return self.ppResidues[k1][k2][d]\n",
      "-        elif self.ppResidues.has_key(k2) and self.ppResidues[k2].has_key(k1):\n",
      "+        elif k2 in self.ppResidues and k1 in self.ppResidues[k2]:\n",
      "             return self.ppResidues[k2][k1][d]\n",
      "         else:\n",
      "             return 0\n",
      "         \n",
      "     def get_ppResidues_stat(self,k1,k2,stat):\n",
      "         data = None\n",
      "-        if self.ppResidues.has_key(k1) and self.ppResidues[k1].has_key(k2):\n",
      "+        if k1 in self.ppResidues and k2 in self.ppResidues[k1]:\n",
      "             data = numpy.array(self.ppResidues[k1][k2], dtype='float64')\n",
      "-        elif self.ppResidues.has_key(k2) and self.ppResidues[k2].has_key(k1):\n",
      "+        elif k2 in self.ppResidues and k1 in self.ppResidues[k2]:\n",
      "             data = numpy.array(self.ppResidues[k2][k1], dtype='float64')\n",
      "         else:\n",
      "             return 0\n",
      "@@ -420,18 +420,18 @@\n",
      "         \n",
      "     def get_ppEnvironment(self,k1,k2,distance):\n",
      "         d = int(math.floor(distance))\n",
      "-        if self.ppEnvironment.has_key(k1) and self.ppEnvironment[k1].has_key(k2):\n",
      "+        if k1 in self.ppEnvironment and k2 in self.ppEnvironment[k1]:\n",
      "             return self.ppEnvironment[k1][k2][d]\n",
      "-        elif self.ppEnvironment.has_key(k2) and self.ppEnvironment[k2].has_key(k1):\n",
      "+        elif k2 in self.ppEnvironment and k1 in self.ppEnvironment[k2]:\n",
      "             return self.ppEnvironment[k2][k1][d]\n",
      "         else:\n",
      "             return 0\n",
      " \n",
      "     def get_ppEnvironment_stat(self,k1,k2,stat):\n",
      "         data = None\n",
      "-        if self.ppEnvironment.has_key(k1) and self.ppEnvironment[k1].has_key(k2):\n",
      "+        if k1 in self.ppEnvironment and k2 in self.ppEnvironment[k1]:\n",
      "             data = numpy.array(self.ppEnvironment[k1][k2], dtype='float64')\n",
      "-        elif self.ppEnvironment.has_key(k2) and self.ppEnvironment[k2].has_key(k1):\n",
      "+        elif k2 in self.ppEnvironment and k1 in self.ppEnvironment[k2]:\n",
      "             data = numpy.array(self.ppEnvironment[k2][k1], dtype='float64')\n",
      "         else:\n",
      "             return 0\n",
      "@@ -449,18 +449,18 @@\n",
      "     \n",
      "     def get_ppRE(self,k1,k2,distance):\n",
      "         d = int(math.floor(distance))\n",
      "-        if self.ppRE.has_key(k1) and self.ppRE[k1].has_key(k2):\n",
      "+        if k1 in self.ppRE and k2 in self.ppRE[k1]:\n",
      "             return self.ppRE[k1][k2][d]\n",
      "-        elif self.ppRE.has_key(k2) and self.ppRE[k2].has_key(k1):\n",
      "+        elif k2 in self.ppRE and k1 in self.ppRE[k2]:\n",
      "             return self.ppRE[k2][k1][d]\n",
      "         else:\n",
      "             return 0\n",
      " \n",
      "     def get_ppRE_stat(self,k1,k2,stat):\n",
      "         data = None\n",
      "-        if self.ppRE.has_key(k1) and self.ppRE[k1].has_key(k2):\n",
      "+        if k1 in self.ppRE and k2 in self.ppRE[k1]:\n",
      "             data = numpy.array(self.ppRE[k1][k2], dtype='float64')\n",
      "-        elif self.ppRE.has_key(k2) and self.ppRE[k2].has_key(k1):\n",
      "+        elif k2 in self.ppRE and k1 in self.ppRE[k2]:\n",
      "             data = numpy.array(self.ppRE[k2][k1], dtype='float64')\n",
      "         else:\n",
      "             return 0\n",
      "@@ -490,7 +490,7 @@\n",
      "         rand = []\n",
      "         done = {}\n",
      "         for pos in list:\n",
      "-            if done.has_key(pos):\n",
      "+            if pos in done:\n",
      "                 rand.append(done[pos])\n",
      "             else:\n",
      "                 r = random.randint(0,(range-1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: No changes to ./BioLib/Docking/PATCHDock.py\n",
      "RefactoringTool: Refactored ./BioLib/Docking/ScoringFunctions.py\n",
      "RefactoringTool: Refactored ./BioLib/Docking/SplitPotentials.py\n",
      "RefactoringTool: Refactored ./BioLib/Docking/SplitPotentialsPPI.py\n",
      "RefactoringTool: No changes to ./BioLib/Docking/ZDock.py\n",
      "RefactoringTool: Refactored ./BioLib/Docking/__init__.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ./BioLib/Docking/SplitPotentialsPPI.py\t(original)\n",
      "+++ ./BioLib/Docking/SplitPotentialsPPI.py\t(refactored)\n",
      "@@ -47,7 +47,7 @@\n",
      "         '''\n",
      "         # Compute global energies\n",
      "         real_interactions = interaction.get_interacting_residues(c_type=self.c_type, max_distance=self.cutoff+1, uniq=False)\n",
      "-        real_energies = self.calculate_energies(zip(real_interactions[0], real_interactions[1], real_interactions[2]))\n",
      "+        real_energies = self.calculate_energies(list(zip(real_interactions[0], real_interactions[1], real_interactions[2])))\n",
      "         interaction_info = [real_energies]\n",
      "         if funnel:\n",
      "             noninterating_exposedR = self.__get_noninteracting_residues(list(set(real_interactions[0])), interaction.get_structure1())\n",
      "@@ -60,15 +60,15 @@\n",
      "             native_energies = {}\n",
      "             external_energies = {}\n",
      "             partial_energies = {}\n",
      "-            for random in xrange(randoms):\n",
      "+            for random in range(randoms):\n",
      "                 rand_nonint_R = self.randomize_residue_interface(noninterating_exposedR, real_interactions[0])\n",
      "                 rand_nonint_L = self.randomize_residue_interface(noninterating_exposedL, real_interactions[1])\n",
      "                 rand_int_R = self.randomize_residue_interface(list(set(real_interactions[0])), real_interactions[0])\n",
      "                 rand_int_L = self.randomize_residue_interface(list(set(real_interactions[1])), real_interactions[1])\n",
      "-                rand_native_energies = self.calculate_energies(zip(rand_int_R, rand_int_L, real_interactions[2], real_interactions[0], real_interactions[1]), randomized=True)\n",
      "-                rand_partialR_energies = self.calculate_energies(zip(rand_int_R, rand_nonint_L, real_interactions[2], real_interactions[0], real_interactions[1]), randomized=True)\n",
      "-                rand_partialL_energies = self.calculate_energies(zip(rand_nonint_R, rand_int_L, real_interactions[2], real_interactions[0], real_interactions[1]), randomized=True)\n",
      "-                rand_external_energies = self.calculate_energies(zip(rand_nonint_R, rand_nonint_L, real_interactions[2], real_interactions[0], real_interactions[1]), randomized=True)\n",
      "+                rand_native_energies = self.calculate_energies(list(zip(rand_int_R, rand_int_L, real_interactions[2], real_interactions[0], real_interactions[1])), randomized=True)\n",
      "+                rand_partialR_energies = self.calculate_energies(list(zip(rand_int_R, rand_nonint_L, real_interactions[2], real_interactions[0], real_interactions[1])), randomized=True)\n",
      "+                rand_partialL_energies = self.calculate_energies(list(zip(rand_nonint_R, rand_int_L, real_interactions[2], real_interactions[0], real_interactions[1])), randomized=True)\n",
      "+                rand_external_energies = self.calculate_energies(list(zip(rand_nonint_R, rand_nonint_L, real_interactions[2], real_interactions[0], real_interactions[1])), randomized=True)\n",
      "                 for potential_type in rand_native_energies:\n",
      "                     native_energies_lists.setdefault(potential_type, []).append(rand_native_energies[potential_type])  \n",
      "                 for potential_type in rand_partialR_energies:\n",
      "@@ -88,10 +88,10 @@\n",
      "             return interaction_info\n",
      "         # Generate random fold energies\n",
      "         rand_energies_lists = {}\n",
      "-        for random in xrange(randoms):\n",
      "+        for random in range(randoms):\n",
      "             rand_interface_R = self.randomize_residue_interface(interaction.get_structure1().get_residues(), real_interactions[0])\n",
      "             rand_interface_L = self.randomize_residue_interface(interaction.get_structure2().get_residues(), real_interactions[1])\n",
      "-            rand_energies = self.calculate_energies(zip(rand_interface_R, rand_interface_L, real_interactions[2], real_interactions[0], real_interactions[1]), randomized=True)\n",
      "+            rand_energies = self.calculate_energies(list(zip(rand_interface_R, rand_interface_L, real_interactions[2], real_interactions[0], real_interactions[1])), randomized=True)\n",
      "             for potential_type in rand_energies:\n",
      "                 rand_energies_lists.setdefault(potential_type, []).append(rand_energies[potential_type])\n",
      "         # Compute Zscores\n",
      "@@ -129,7 +129,7 @@\n",
      "         residues_interactions = []\n",
      "         for interacting_residue in interacting_residues:\n",
      "             residue_interactions = []\n",
      "-            for i in xrange(len(real_interactions[2])):\n",
      "+            for i in range(len(real_interactions[2])):\n",
      "                 if struc == 'L':\n",
      "                     if real_interactions[1][i] == interacting_residue:\n",
      "                         residue_interactions.append((real_interactions[1][i], real_interactions[0][i], real_interactions[2][i]))\n",
      "@@ -147,14 +147,14 @@\n",
      "                 if alanine_scanning:\n",
      "                     rand_fala_energies_lists = {}\n",
      "                     funnel_ala_energies = {}\n",
      "-                for random in xrange(randoms):\n",
      "-                    interfaceA, interfaceB, distances = zip(*residue_interactions)\n",
      "+                for random in range(randoms):\n",
      "+                    interfaceA, interfaceB, distances = list(zip(*residue_interactions))\n",
      "                     rand_interfaceB = self.randomize_residue_interface(noninterating_exposedB, interfaceB)\n",
      "-                    rand_funn_energies = self.calculate_energies(zip(interfaceA, rand_interfaceB, distances, interfaceA, interfaceB), local_type='R', randomized=True)\n",
      "+                    rand_funn_energies = self.calculate_energies(list(zip(interfaceA, rand_interfaceB, distances, interfaceA, interfaceB)), local_type='R', randomized=True)\n",
      "                     for potential_type in rand_funn_energies:\n",
      "                         rand_funn_energies_lists.setdefault(potential_type, []).append(rand_funn_energies[potential_type])  \n",
      "                     if alanine_scanning:\n",
      "-                        rand_fala_energies = self.calculate_energies(zip(interfaceA, rand_interfaceB, distances, interfaceA, interfaceB), local_type='R', randomized=True, alanine_mutation=True)\n",
      "+                        rand_fala_energies = self.calculate_energies(list(zip(interfaceA, rand_interfaceB, distances, interfaceA, interfaceB)), local_type='R', randomized=True, alanine_mutation=True)\n",
      "                         for potential_type in rand_fala_energies:\n",
      "                             rand_fala_energies_lists.setdefault(potential_type, []).append(rand_fala_energies[potential_type]) \n",
      "                 for potential in rand_funn_energies_lists:\n",
      "@@ -171,11 +171,11 @@\n",
      "         residues_energies = []\n",
      "         for residue in residues_interactions:\n",
      "             rand_energies_lists = {}\n",
      "-            for random in xrange(randoms):\n",
      "-                interfaceA, interfaceB, distances = zip(*residue[1])\n",
      "+            for random in range(randoms):\n",
      "+                interfaceA, interfaceB, distances = list(zip(*residue[1]))\n",
      "                 rand_interfaceA = self.randomize_residue_interface(structureA.get_residues(), interfaceA)\n",
      "                 rand_interfaceB = self.randomize_residue_interface(structureB.get_residues(), interfaceB)\n",
      "-                rand_energies = self.calculate_energies(zip(rand_interfaceA, rand_interfaceB, distances, interfaceA, interfaceB), local_type='R', randomized=True)\n",
      "+                rand_energies = self.calculate_energies(list(zip(rand_interfaceA, rand_interfaceB, distances, interfaceA, interfaceB)), local_type='R', randomized=True)\n",
      "                 for potential_type in rand_energies:\n",
      "                     rand_energies_lists.setdefault(potential_type, []).append(rand_energies[potential_type])\n",
      "             residue_info = [residue[0], residue[2], self.compute_Zscores(residue[2], rand_energies_lists)]\n",
      "@@ -218,7 +218,7 @@\n",
      "         for interacting_residue in interacting_residues:\n",
      "             residue_interactions = []\n",
      "             residue_info = []\n",
      "-            for i in xrange(len(real_interactions[2])):\n",
      "+            for i in range(len(real_interactions[2])):\n",
      "                 if struc == 'L':\n",
      "                     if real_interactions[1][i] == interacting_residue:\n",
      "                         res1 = real_interactions[1][i]\n",
      "@@ -248,14 +248,14 @@\n",
      "                     if alanine_scanning:\n",
      "                         rand_fala_energies_lists = {}\n",
      "                         funnel_ala_energies = {}\n",
      "-                    for random in xrange(randoms):\n",
      "-                        interfaceA, interfaceB, distances = zip(*residue_interactions)\n",
      "+                    for random in range(randoms):\n",
      "+                        interfaceA, interfaceB, distances = list(zip(*residue_interactions))\n",
      "                         rand_interfaceB = self.randomize_residue_interface(noninterating_exposedB, interfaceB)\n",
      "-                        rand_funn_energies = self.calculate_energies(zip(interfaceA, rand_interfaceB, distances, interfaceA, interfaceB), local_type='R', randomized=True)\n",
      "+                        rand_funn_energies = self.calculate_energies(list(zip(interfaceA, rand_interfaceB, distances, interfaceA, interfaceB)), local_type='R', randomized=True)\n",
      "                         for potential_type in rand_funn_energies:\n",
      "                             rand_funn_energies_lists.setdefault(potential_type, []).append(rand_funn_energies[potential_type])  \n",
      "                         if alanine_scanning:\n",
      "-                            rand_fala_energies = self.calculate_energies(zip(interfaceA, rand_interfaceB, distances, interfaceA, interfaceB), local_type='R', randomized=True, alanine_mutation=True)\n",
      "+                            rand_fala_energies = self.calculate_energies(list(zip(interfaceA, rand_interfaceB, distances, interfaceA, interfaceB)), local_type='R', randomized=True, alanine_mutation=True)\n",
      "                             for potential_type in rand_fala_energies:\n",
      "                                 rand_fala_energies_lists.setdefault(potential_type, []).append(rand_fala_energies[potential_type]) \n",
      "                     for potential in rand_funn_energies_lists:\n",
      "@@ -272,11 +272,11 @@\n",
      "         residues_energies = []\n",
      "         for residue in residues_interactions:\n",
      "             rand_energies_lists = {}\n",
      "-            for random in xrange(randoms):\n",
      "-                interfaceA, interfaceB, distances = zip(*residue[1])\n",
      "+            for random in range(randoms):\n",
      "+                interfaceA, interfaceB, distances = list(zip(*residue[1]))\n",
      "                 rand_interfaceA = self.randomize_residue_interface(structureA.get_residues(), interfaceA)\n",
      "                 rand_interfaceB = self.randomize_residue_interface(structureB.get_residues(), interfaceB)\n",
      "-                rand_energies = self.calculate_energies(zip(rand_interfaceA, rand_interfaceB, distances, interfaceA, interfaceB), local_type='R', randomized=True)\n",
      "+                rand_energies = self.calculate_energies(list(zip(rand_interfaceA, rand_interfaceB, distances, interfaceA, interfaceB)), local_type='R', randomized=True)\n",
      "                 for potential_type in rand_energies:\n",
      "                     rand_energies_lists.setdefault(potential_type, []).append(rand_energies[potential_type])\n",
      "             residue_info = [residue[0], residue[2], self.compute_Zscores(residue[2], rand_energies_lists), residue[1][0][1]]\n",
      "@@ -417,7 +417,7 @@\n",
      "         rand_interface = []\n",
      "         done = {}\n",
      "         for res in res_interface:\n",
      "-            if done.has_key(res):\n",
      "+            if res in done:\n",
      "                 rand_res = done[res]\n",
      "             else:\n",
      "                 rand_res = random.choice(struct_residues)\n",
      "@@ -443,9 +443,9 @@\n",
      "     def get_ppR(self, res1, res2, distance):\n",
      " \n",
      "         distance = int(math.floor(distance))\n",
      "-        if self.ppR.has_key(res1) and self.ppR[res1].has_key(res2):\n",
      "+        if res1 in self.ppR and res2 in self.ppR[res1]:\n",
      "             return float(self.ppR[res1][res2][distance])\n",
      "-        elif self.ppR.has_key(res2) and self.ppR[res2].has_key(res1):\n",
      "+        elif res2 in self.ppR and res1 in self.ppR[res2]:\n",
      "             return float(self.ppR[res2][res1][distance])\n",
      "         else:\n",
      "             return 0\n",
      "@@ -453,9 +453,9 @@\n",
      "         \n",
      "     def get_ppR_stat(self, res1, res2, stat):\n",
      " \n",
      "-        if self.ppR.has_key(res1) and self.ppR[res1].has_key(res2):\n",
      "+        if res1 in self.ppR and res2 in self.ppR[res1]:\n",
      "             data = numpy.array(self.ppR[res1][res2], dtype='float64')\n",
      "-        elif self.ppR.has_key(res2) and self.ppR[res2].has_key(res1):\n",
      "+        elif res2 in self.ppR and res1 in self.ppR[res2]:\n",
      "             data = numpy.array(self.ppR[res2][res1], dtype='float64')\n",
      "         else:\n",
      "             return 0\n",
      "@@ -470,9 +470,9 @@\n",
      "     def get_ppE(self, res1, res2, distance):\n",
      " \n",
      "         distance = int(math.floor(distance))\n",
      "-        if self.ppE.has_key(res1) and self.ppE[res1].has_key(res2):\n",
      "+        if res1 in self.ppE and res2 in self.ppE[res1]:\n",
      "             return -float(self.ppE[res1][res2][distance])\n",
      "-        elif self.ppE.has_key(res2) and self.ppE[res2].has_key(res1):\n",
      "+        elif res2 in self.ppE and res1 in self.ppE[res2]:\n",
      "             return -float(self.ppE[res2][res1][distance])\n",
      "         else:\n",
      "             return 0\n",
      "@@ -480,9 +480,9 @@\n",
      " \n",
      "     def get_ppE_stat(self, res1, res2, stat):\n",
      " \n",
      "-        if self.ppE.has_key(res1) and self.ppE[res1].has_key(res2):\n",
      "+        if res1 in self.ppE and res2 in self.ppE[res1]:\n",
      "             data = numpy.array(self.ppE[res1][res2], dtype='float64')\n",
      "-        elif self.ppE.has_key(res2) and self.ppE[res2].has_key(res1):\n",
      "+        elif res2 in self.ppE and res1 in self.ppE[res2]:\n",
      "             data = numpy.array(self.ppE[res2][res1], dtype='float64')\n",
      "         else:\n",
      "             return 0\n",
      "@@ -503,9 +503,9 @@\n",
      "         '''\n",
      "         '''\n",
      "         distance = int(math.floor(distance))\n",
      "-        if self.ppRE.has_key(res1) and self.ppRE[res1].has_key(res2):\n",
      "+        if res1 in self.ppRE and res2 in self.ppRE[res1]:\n",
      "             return float(self.ppRE[res1][res2][distance])\n",
      "-        elif self.ppRE.has_key(res2) and self.ppRE[res2].has_key(res1):\n",
      "+        elif res2 in self.ppRE and res1 in self.ppRE[res2]:\n",
      "             return float(self.ppRE[res2][res1][distance])\n",
      "         else:\n",
      "             return 0\n",
      "@@ -514,9 +514,9 @@\n",
      "     def get_ppRE_stat(self, res1, res2, stat):\n",
      "         '''\n",
      "         '''\n",
      "-        if self.ppRE.has_key(res1) and self.ppRE[res1].has_key(res2):\n",
      "+        if res1 in self.ppRE and res2 in self.ppRE[res1]:\n",
      "             data = numpy.array(self.ppRE[res1][res2], dtype='float64')\n",
      "-        elif self.ppRE.has_key(res2) and self.ppRE[res2].has_key(res1):\n",
      "+        elif res2 in self.ppRE and res1 in self.ppRE[res2]:\n",
      "             data = numpy.array(self.ppRE[res2][res1], dtype='float64')\n",
      "         else:\n",
      "             return 0\n",
      "--- ./BioLib/Docking/__init__.py\t(original)\n",
      "+++ ./BioLib/Docking/__init__.py\t(refactored)\n",
      "@@ -1,7 +1,7 @@\n",
      " #from FTDock import *\n",
      "-from HEXDock import *\n",
      "-from ZDock import *\n",
      "-from PATCHDock import *\n",
      "-from SplitPotentials import SplitPotentialsMatrix\n",
      "-from SplitPotentialsPPI import *\n",
      "-from ScoringFunctions import *\n",
      "+from .HEXDock import *\n",
      "+from .ZDock import *\n",
      "+from .PATCHDock import *\n",
      "+from .SplitPotentials import SplitPotentialsMatrix\n",
      "+from .SplitPotentialsPPI import *\n",
      "+from .ScoringFunctions import *\n",
      "--- ./BioLib/Fold/SplitPotentialsFold.py\t(original)\n",
      "+++ ./BioLib/Fold/SplitPotentialsFold.py\t(refactored)\n",
      "@@ -51,7 +51,7 @@\n",
      "             return global_energies\n",
      "         # Generate random fold energies\n",
      "         rand_energies_lists = {}\n",
      "-        for random in xrange(randoms):\n",
      "+        for random in range(randoms):\n",
      "             rand_interactions = self.randomize_residue_interactions(structure.get_residues(), real_interactions)\n",
      "             rand_energies = self.calculate_energies(rand_interactions)\n",
      "             for potential_type in rand_energies:\n",
      "@@ -85,7 +85,7 @@\n",
      "         residues_energies = []\n",
      "         for residue in residues_interactions:\n",
      "             rand_energies_lists = {}\n",
      "-            for random in xrange(randoms):\n",
      "+            for random in range(randoms):\n",
      "                 rand_interactions = self.randomize_residue_interactions(structure.get_residues(), residue[1])\n",
      "                 rand_energies = self.calculate_energies(rand_interactions, local_type='R')\n",
      "                 for potential_type in rand_energies:\n",
      "@@ -195,12 +195,12 @@\n",
      "         done = {}\n",
      "         for res_interaction in res_interactions:\n",
      "             dist = res_interaction[2]\n",
      "-            if done.has_key(res_interaction[0]):\n",
      "+            if res_interaction[0] in done:\n",
      "                 res1 = done[res_interaction[0]]\n",
      "             else:\n",
      "                 res1 = random.choice(struct_residues)\n",
      "                 done[res_interaction[0]] = res1\n",
      "-            if done.has_key(res_interaction[1]):\n",
      "+            if res_interaction[1] in done:\n",
      "                 res2 = done[res_interaction[1]]\n",
      "             else:\n",
      "                 res2 = random.choice(struct_residues)\n",
      "@@ -226,9 +226,9 @@\n",
      "     def get_ppR(self, res1, res2, distance):\n",
      " \n",
      "         distance = int(math.floor(distance))\n",
      "-        if self.ppR.has_key(res1) and self.ppR[res1].has_key(res2):\n",
      "+        if res1 in self.ppR and res2 in self.ppR[res1]:\n",
      "             return float(self.ppR[res1][res2][distance])\n",
      "-        elif self.ppR.has_key(res2) and self.ppR[res2].has_key(res1):\n",
      "+        elif res2 in self.ppR and res1 in self.ppR[res2]:\n",
      "             return float(self.ppR[res2][res1][distance])\n",
      "         else:\n",
      "             return 0\n",
      "@@ -236,9 +236,9 @@\n",
      "         \n",
      "     def get_ppR_stat(self, res1, res2, stat):\n",
      " \n",
      "-        if self.ppR.has_key(res1) and self.ppR[res1].has_key(res2):\n",
      "+        if res1 in self.ppR and res2 in self.ppR[res1]:\n",
      "             data = numpy.array(self.ppR[res1][res2], dtype='float64')\n",
      "-        elif self.ppR.has_key(res2) and self.ppR[res2].has_key(res1):\n",
      "+        elif res2 in self.ppR and res1 in self.ppR[res2]:\n",
      "             data = numpy.array(self.ppR[res2][res1], dtype='float64')\n",
      "         else:\n",
      "             return 0\n",
      "@@ -253,9 +253,9 @@\n",
      "     def get_ppE(self, res1, res2, distance):\n",
      " \n",
      "         distance = int(math.floor(distance))\n",
      "-        if self.ppE.has_key(res1) and self.ppE[res1].has_key(res2):\n",
      "+        if res1 in self.ppE and res2 in self.ppE[res1]:\n",
      "             return -float(self.ppE[res1][res2][distance])\n",
      "-        elif self.ppE.has_key(res2) and self.ppE[res2].has_key(res1):\n",
      "+        elif res2 in self.ppE and res1 in self.ppE[res2]:\n",
      "             return -float(self.ppE[res2][res1][distance])\n",
      "         else:\n",
      "             return 0\n",
      "@@ -263,9 +263,9 @@\n",
      " \n",
      "     def get_ppE_stat(self, res1, res2, stat):\n",
      " \n",
      "-        if self.ppE.has_key(res1) and self.ppE[res1].has_key(res2):\n",
      "+        if res1 in self.ppE and res2 in self.ppE[res1]:\n",
      "             data = numpy.array(self.ppE[res1][res2], dtype='float64')\n",
      "-        elif self.ppE.has_key(res2) and self.ppE[res2].has_key(res1):\n",
      "+        elif res2 in self.ppE and res1 in self.ppE[res2]:\n",
      "             data = numpy.array(self.ppE[res2][res1], dtype='float64')\n",
      "         else:\n",
      "             return 0\n",
      "@@ -286,9 +286,9 @@\n",
      "         '''\n",
      "         '''\n",
      "         distance = int(math.floor(distance))\n",
      "-        if self.ppRE.has_key(res1) and self.ppRE[res1].has_key(res2):\n",
      "+        if res1 in self.ppRE and res2 in self.ppRE[res1]:\n",
      "             return float(self.ppRE[res1][res2][distance])\n",
      "-        elif self.ppRE.has_key(res2) and self.ppRE[res2].has_key(res1):\n",
      "+        elif res2 in self.ppRE and res1 in self.ppRE[res2]:\n",
      "             return float(self.ppRE[res2][res1][distance])\n",
      "         else:\n",
      "             return 0\n",
      "@@ -297,9 +297,9 @@\n",
      "     def get_ppRE_stat(self, res1, res2, stat):\n",
      "         '''\n",
      "         '''\n",
      "-        if self.ppRE.has_key(res1) and self.ppRE[res1].has_key(res2):\n",
      "+        if res1 in self.ppRE and res2 in self.ppRE[res1]:\n",
      "             data = numpy.array(self.ppRE[res1][res2], dtype='float64')\n",
      "-        elif self.ppRE.has_key(res2) and self.ppRE[res2].has_key(res1):\n",
      "+        elif res2 in self.ppRE and res1 in self.ppRE[res2]:\n",
      "             data = numpy.array(self.ppRE[res2][res1], dtype='float64')\n",
      "         else:\n",
      "             return 0\n",
      "--- ./BioLib/Fold/__init__.py\t(original)\n",
      "+++ ./BioLib/Fold/__init__.py\t(refactored)\n",
      "@@ -1 +1 @@\n",
      "-from SplitPotentialsFold import *\n",
      "+from .SplitPotentialsFold import *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Refactored ./BioLib/Fold/SplitPotentialsFold.py\n",
      "RefactoringTool: Refactored ./BioLib/Fold/__init__.py\n",
      "RefactoringTool: Refactored ./BioLib/ILoops/ILoopsParser.py\n",
      "RefactoringTool: No changes to ./BioLib/ILoops/ILoopsParserMinidom.py\n",
      "RefactoringTool: Refactored ./BioLib/ILoops/__init__.py\n",
      "RefactoringTool: Refactored ./BioLib/Linker/linkerTools.py\n",
      "RefactoringTool: No changes to ./BioLib/Sequence/FastaParser.py\n",
      "RefactoringTool: Refactored ./BioLib/Sequence/__init__.py\n",
      "RefactoringTool: No changes to ./BioLib/Structure/Atom.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ./BioLib/ILoops/ILoopsParser.py\t(original)\n",
      "+++ ./BioLib/ILoops/ILoopsParser.py\t(refactored)\n",
      "@@ -233,8 +233,8 @@\n",
      "         if   eval(feature_codes_list)[0] in self._loops  : feature_type = LOOP\n",
      "         elif eval(feature_codes_list)[0] in self._domains: feature_type = DOMAIN\n",
      "         else: \n",
      "-            print self._domains\n",
      "-            print self._loops\n",
      "+            print(self._domains)\n",
      "+            print(self._loops)\n",
      "             raise ValueError(eval(feature_codes_list)[0])\n",
      "         \n",
      "         return [ feature_type, [ self.get_featureID(feature_code, feature_type) for feature_code in eval(feature_codes_list) ]]\n",
      "@@ -539,7 +539,7 @@\n",
      "         # CREATE ITERATOR FOR XML PARSER\n",
      "         context = iterparse(xml_file, (\"start\", \"end\"))\n",
      "         context = iter(context)\n",
      "-        event, root = context.next() #drop root, usually '<xml>'\n",
      "+        event, root = next(context) #drop root, usually '<xml>'\n",
      " \n",
      "         # ITERATE OVER ELEMENTS IN XML FILE\n",
      "         for event, element in context: \n",
      "@@ -1420,5 +1420,5 @@\n",
      "     #n_sign = parser.get_negative_signatures(\"NXT1_HUMAN\", \"Q68CW9_HUMAN\", max_protein_signatures=1)\n",
      "     end=time.time()\n",
      "     for i in loops:\n",
      "-        print repr(i)\n",
      "-    print end-start\n",
      "+        print(repr(i))\n",
      "+    print(end-start)\n",
      "--- ./BioLib/ILoops/__init__.py\t(original)\n",
      "+++ ./BioLib/ILoops/__init__.py\t(refactored)\n",
      "@@ -1 +1 @@\n",
      "-from ILoopsParser import IloopsParser\n",
      "+from .ILoopsParser import IloopsParser\n",
      "--- ./BioLib/Linker/linkerTools.py\t(original)\n",
      "+++ ./BioLib/Linker/linkerTools.py\t(refactored)\n",
      "@@ -34,7 +34,7 @@\n",
      "          compatible = 0\n",
      "     if nativeMobile:\n",
      "         rmsd = ftdock.get_ligand_RMSD(static, mobile, nativeMobile, decoy)\n",
      "-        print str(decoy.get_id())+'\\t'+str(rmsd)+'\\t'+str(compatible)\n",
      "+        print(str(decoy.get_id())+'\\t'+str(rmsd)+'\\t'+str(compatible))\n",
      "         return compatible, rmsd\n",
      "     return compatible\n",
      " \n",
      "--- ./BioLib/Sequence/__init__.py\t(original)\n",
      "+++ ./BioLib/Sequence/__init__.py\t(refactored)\n",
      "@@ -1 +1 @@\n",
      "-from FastaParser import *\n",
      "+from .FastaParser import *\n",
      "--- ./BioLib/Structure/Structure.py\t(original)\n",
      "+++ ./BioLib/Structure/Structure.py\t(refactored)\n",
      "@@ -93,7 +93,7 @@\n",
      " \n",
      "     def add_loop(self, loop):\n",
      "         loop_length = loop.get_end() - loop.get_start() + 1\n",
      "-        for i in xrange(loop_length):\n",
      "+        for i in range(loop_length):\n",
      "             position = (loop.get_start()-1)+i+self.get_first_residue().get_num()\n",
      "             if self.get_residue_by_num(position) != None:\n",
      "                 loop.add_residue(self.get_residue_by_num(position))\n",
      "@@ -159,7 +159,7 @@\n",
      "         for residue1 in self:\n",
      "             used_residues.append(residue1)\n",
      "             for residue2 in self:\n",
      "-                if residue2.get_num() not in [i for i in xrange(residue1.get_num()-gap, residue1.get_num()+gap+1)]:\n",
      "+                if residue2.get_num() not in [i for i in range(residue1.get_num()-gap, residue1.get_num()+gap+1)]:\n",
      "                     try:\n",
      "                         distance = residue1.get_residue_distance(residue2, c_type)\n",
      "                     except ResidueDistanceError as e:\n",
      "@@ -283,8 +283,8 @@\n",
      "         if self.get_number_residues() != structure.get_number_residues():\n",
      "             sys.stderr.write(\"The RMSD needs to be between the same structures!!\\n\")\n",
      "         E = 0\n",
      "-        for residue in xrange(self.get_number_residues()):\n",
      "-            for atom in xrange(self.get_residues()[residue].get_number_atoms()):\n",
      "+        for residue in range(self.get_number_residues()):\n",
      "+            for atom in range(self.get_residues()[residue].get_number_atoms()):\n",
      "                 distance = self.get_residues()[residue].get_atoms()[atom].get_distance(atom=structure.get_residues()[residue].get_atoms()[atom])\n",
      "                 E += math.pow(distance, 2)\n",
      "         E = E /self.get_number_atoms()\n",
      "--- ./BioLib/Structure/__init__.py\t(original)\n",
      "+++ ./BioLib/Structure/__init__.py\t(refactored)\n",
      "@@ -1,6 +1,6 @@\n",
      "-from Atom import Atom\n",
      "-from Interaction import Interaction\n",
      "-from Loop import Loop\n",
      "-import PDB\n",
      "-from Residue import Residue\n",
      "-from Structure import Structure\n",
      "+from .Atom import Atom\n",
      "+from .Interaction import Interaction\n",
      "+from .Loop import Loop\n",
      "+from . import PDB\n",
      "+from .Residue import Residue\n",
      "+from .Structure import Structure\n",
      "--- ./BioLib/Tools/__init__.py\t(original)\n",
      "+++ ./BioLib/Tools/__init__.py\t(refactored)\n",
      "@@ -1,3 +1,3 @@\n",
      "-from Submitters import *\n",
      "-from BioExceptions import *\n",
      "-from Dssp import execute_dssp\n",
      "+from .Submitters import *\n",
      "+from .BioExceptions import *\n",
      "+from .Dssp import execute_dssp\n",
      "--- ./CASP12_analysis/scripts/GDT_Extractor.py\t(original)\n",
      "+++ ./CASP12_analysis/scripts/GDT_Extractor.py\t(refactored)\n",
      "@@ -14,10 +14,10 @@\n",
      "     txt_file = glob.glob(\"./TXT/\" + element[:5] + \"-\" + element[5:7] + \".txt\" )[0]\n",
      "     #print (element)\n",
      "     #print (element[:5] + \"TS\" + element[9:12] + \"_\" + element[13:14] + \"-\" + element[5:7])\n",
      "-    print (\"Searching for element\", element)\n",
      "+    print((\"Searching for element\", element))\n",
      " \n",
      "     if len(element) == 11:\n",
      "-        print (element, \"Hola\")\n",
      "+        print((element, \"Hola\"))\n",
      "         GDT.append(1)\n",
      "         TM.append(1)\n",
      "         QCS.append(1)\n",
      "@@ -28,7 +28,7 @@\n",
      " \n",
      "                 try:\n",
      "                     if line[1] == (element[:5] + \"TS\" + element[9:12] + \"_\" + element[13:14] + \"-\" + element[5:7]):\n",
      "-                            print (\"Found\", element)\n",
      "+                            print((\"Found\", element))\n",
      "                             GDT.append(line[3])\n",
      "                             QCS.append(line[-6])\n",
      "                             TM.append(line[-4])\n",
      "@@ -45,8 +45,8 @@\n",
      " \n",
      " \n",
      " \n",
      "-print (len(dataframe[\"PDB\"]))\n",
      "-print (len(GDT))\n",
      "+print((len(dataframe[\"PDB\"])))\n",
      "+print((len(GDT)))\n",
      " dataframe[\"GDT\"] = GDT\n",
      " dataframe[\"QCS\"] = QCS\n",
      " dataframe[\"TM\"] = TM\n",
      "--- ./CASP12_analysis/scripts/analyze_casp_subset.py\t(original)\n",
      "+++ ./CASP12_analysis/scripts/analyze_casp_subset.py\t(refactored)\n",
      "@@ -39,7 +39,7 @@\n",
      "     for type_structure in structure_dirs:\n",
      "         structure_dir = os.path.join(data_dir, type_structure)\n",
      "         models = [model for model in os.listdir(structure_dir) if fileExist(os.path.join(structure_dir, model))]\n",
      "-        print('{} has this number of files: {}'.format(type_structure, len(models)))\n",
      "+        print(('{} has this number of files: {}'.format(type_structure, len(models))))\n",
      " \n",
      " \n",
      "     targets_with_models = set(target_to_nearnatives.keys()) | set(target_to_wrongs.keys())\n",
      "@@ -47,14 +47,14 @@\n",
      "     nearnative_wrong = nearnative_structures | wrong_structures\n",
      "     native_nearnative_wrong = targets_with_models | nearnative_structures | wrong_structures\n",
      " \n",
      "-    print('Number of near-native models: {}'.format(len(nearnative_structures)))\n",
      "-    print('Number of wrong models: {}'.format(len(wrong_structures)))\n",
      "-    print('Number of targets with near-native models: {}'.format(len(target_to_nearnatives)))\n",
      "-    print('Number of targets with wrong models: {}'.format(len(target_to_wrongs)))\n",
      "-    print('Number of targets with near-native or wrong models: {}'.format(len(targets_with_models)))\n",
      "-    print('Number of overlapped near-native and wrong models: {}'.format(len(overlap_nearnative_wrong)))\n",
      "-    print('Number of near-native + wrong models: {}'.format(len(nearnative_wrong)))\n",
      "-    print('Number of native + near-native + wrong models: {}'.format(len(native_nearnative_wrong)))\n",
      "+    print(('Number of near-native models: {}'.format(len(nearnative_structures))))\n",
      "+    print(('Number of wrong models: {}'.format(len(wrong_structures))))\n",
      "+    print(('Number of targets with near-native models: {}'.format(len(target_to_nearnatives))))\n",
      "+    print(('Number of targets with wrong models: {}'.format(len(target_to_wrongs))))\n",
      "+    print(('Number of targets with near-native or wrong models: {}'.format(len(targets_with_models))))\n",
      "+    print(('Number of overlapped near-native and wrong models: {}'.format(len(overlap_nearnative_wrong))))\n",
      "+    print(('Number of near-native + wrong models: {}'.format(len(nearnative_wrong))))\n",
      "+    print(('Number of native + near-native + wrong models: {}'.format(len(native_nearnative_wrong))))\n",
      " \n",
      "     # Write output\n",
      "     output_file = os.path.join(outputs_dir, 'casp12_subset_analysis.txt')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: No changes to ./BioLib/Structure/Interaction.py\n",
      "RefactoringTool: No changes to ./BioLib/Structure/Interaction_old.py\n",
      "RefactoringTool: No changes to ./BioLib/Structure/Loop.py\n",
      "RefactoringTool: No changes to ./BioLib/Structure/PDB.py\n",
      "RefactoringTool: No changes to ./BioLib/Structure/Residue.py\n",
      "RefactoringTool: Refactored ./BioLib/Structure/Structure.py\n",
      "RefactoringTool: Refactored ./BioLib/Structure/__init__.py\n",
      "RefactoringTool: No changes to ./BioLib/Tools/BioExceptions.py\n",
      "RefactoringTool: No changes to ./BioLib/Tools/Dssp.py\n",
      "RefactoringTool: No changes to ./BioLib/Tools/Submitters.py\n",
      "RefactoringTool: Refactored ./BioLib/Tools/__init__.py\n",
      "RefactoringTool: No changes to ./CASP12_analysis/scripts/DOPE_Extractor.py\n",
      "RefactoringTool: Refactored ./CASP12_analysis/scripts/GDT_Extractor.py\n",
      "RefactoringTool: No changes to ./CASP12_analysis/scripts/PROSA_extract.py\n",
      "RefactoringTool: No changes to ./CASP12_analysis/scripts/SPS_residues.py\n",
      "RefactoringTool: Refactored ./CASP12_analysis/scripts/analyze_casp_subset.py\n",
      "RefactoringTool: Refactored ./CASP12_analysis/scripts/make_bootstrapping.py\n",
      "RefactoringTool: Refactored ./CASP12_analysis/scripts/parse_casp_results.py\n",
      "RefactoringTool: Refactored ./CASP12_analysis/scripts/plot_global_scores.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ./CASP12_analysis/scripts/make_bootstrapping.py\t(original)\n",
      "+++ ./CASP12_analysis/scripts/make_bootstrapping.py\t(refactored)\n",
      "@@ -56,10 +56,10 @@\n",
      "     pdbs_native = set(results_df.loc[results_df['TypeStructure'] == 'Native', 'PDB'])\n",
      "     pdbs_nearnative = set(results_df.loc[results_df['TypeStructure'] == 'Near-native', 'PDB'])\n",
      "     pdbs_wrong = set(results_df.loc[results_df['TypeStructure'] == 'Wrong', 'PDB'])\n",
      "-    print('Number of native: {}'.format(len(pdbs_native)))\n",
      "-    print('Number of near-native: {}'.format(len(pdbs_nearnative)))\n",
      "-    print('Number of wrong: {}'.format(len(pdbs_wrong)))\n",
      "-    print('Number of structures: {}'.format(len(results_df.index)))\n",
      "+    print(('Number of native: {}'.format(len(pdbs_native))))\n",
      "+    print(('Number of near-native: {}'.format(len(pdbs_nearnative))))\n",
      "+    print(('Number of wrong: {}'.format(len(pdbs_wrong))))\n",
      "+    print(('Number of structures: {}'.format(len(results_df.index))))\n",
      " \n",
      "     # We get the targets that have at least 1 model of each type\n",
      "     targets_with_native = set(results_df.loc[results_df['TypeStructure'] == 'Native', 'Target'])\n",
      "@@ -79,9 +79,9 @@\n",
      "         columns = ['Repetition', 'Target', 'PDB', 'TypeStructure', 'ZES3DC', 'ZPAIR', 'PROSA', 'DOPE', 'GDT_TS', 'TM score', 'QCS']\n",
      "         bootstrapping_df = pd.DataFrame(columns=columns)\n",
      " \n",
      "-        for i in xrange(num_repetitions):\n",
      "-\n",
      "-            print(i+1)\n",
      "+        for i in range(num_repetitions):\n",
      "+\n",
      "+            print((i+1))\n",
      "             structures = []\n",
      "             scores = []\n",
      " \n",
      "@@ -121,7 +121,7 @@\n",
      "         columns = ['Repetition', 'ScoringFunction1', 'ScoringFunction2', 'PearsonCorrelation', 'P-value']\n",
      "         correlations_df = pd.DataFrame(columns=columns)\n",
      "         scoring_functions = ['ZES3DC', 'ZPAIR', 'PROSA', 'DOPE', 'GDT_TS', 'TM score', 'QCS']\n",
      "-        for i in xrange(num_repetitions):\n",
      "+        for i in range(num_repetitions):\n",
      "             repetition = i+1\n",
      "             print(repetition)\n",
      "             repetition_df = bootstrapping_df[bootstrapping_df['Repetition'] == repetition]\n",
      "--- ./CASP12_analysis/scripts/parse_casp_results.py\t(original)\n",
      "+++ ./CASP12_analysis/scripts/parse_casp_results.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " import sys, os\n",
      "-import cPickle\n",
      "+import pickle\n",
      " import matplotlib.pyplot as plt\n",
      " import numpy as np\n",
      " import optparse\n",
      "@@ -103,7 +103,7 @@\n",
      "                     spserver_results_df = spserver_results_df.append(df2) # Add the information to the main data frame\n",
      "                     index+=1\n",
      "             except:\n",
      "-                print('Incorrect results file: {}'.format(spserver_file))\n",
      "+                print(('Incorrect results file: {}'.format(spserver_file)))\n",
      "         # Write results \n",
      "         spserver_results_df = spserver_results_df.sort_values(by=['PDB'])\n",
      "         spserver_results_df.to_csv(output_df_file, sep='\\t', index=False)\n",
      "@@ -139,7 +139,7 @@\n",
      "                 df2 = pd.DataFrame([results], columns=columns, index=[index])\n",
      "                 results_df = results_df.append(df2) # Add the information to the main data frame\n",
      "             else:\n",
      "-                print('Model {} not found'.format(model))\n",
      "+                print(('Model {} not found'.format(model)))\n",
      " \n",
      "         # Remove columns with missing values\n",
      "         results_df= results_df.dropna()\n",
      "--- ./CASP12_analysis/scripts/plot_global_scores.py\t(original)\n",
      "+++ ./CASP12_analysis/scripts/plot_global_scores.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " import sys, os\n",
      "-import cPickle\n",
      "+import pickle\n",
      " import matplotlib.pyplot as plt\n",
      " import numpy as np\n",
      " import optparse\n",
      "@@ -71,7 +71,7 @@\n",
      "                 scores_alternative = results_df[alternative_scoring_function].astype(float).tolist()\n",
      "                 r_value, p_value = pearsonr(scores_spserver, scores_alternative)\n",
      "                 r_value = round(r_value, 2)\n",
      "-                print ('Correlation for energies {} and {}: {}'.format(spserver_scoring_function, alternative_scoring_function, r_value))\n",
      "+                print(('Correlation for energies {} and {}: {}'.format(spserver_scoring_function, alternative_scoring_function, r_value)))\n",
      " \n",
      "                 # Make scatter plot\n",
      "                 fig = pylab.figure(dpi=300)\n",
      "--- ./CASP12_analysis/scripts/plot_residue_scores.py\t(original)\n",
      "+++ ./CASP12_analysis/scripts/plot_residue_scores.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " import sys, os\n",
      "-import cPickle\n",
      "+import pickle\n",
      " import matplotlib.pyplot as plt\n",
      " import numpy as np\n",
      " import optparse\n",
      "@@ -105,11 +105,11 @@\n",
      " \n",
      "                 else:\n",
      "                     if not fileExist(spserver_file):\n",
      "-                        print('FILE NOT FOUND FOR STRUCTURE {}: {}'.format(structure, spserver_file))\n",
      "+                        print(('FILE NOT FOUND FOR STRUCTURE {}: {}'.format(structure, spserver_file)))\n",
      "                     elif not fileExist(dope_file):\n",
      "-                        print('FILE NOT FOUND FOR STRUCTURE {}: {}'.format(structure, dope_file))\n",
      "+                        print(('FILE NOT FOUND FOR STRUCTURE {}: {}'.format(structure, dope_file)))\n",
      "                     elif not fileExist(prosa_file):\n",
      "-                        print('FILE NOT FOUND FOR STRUCTURE {}: {}'.format(structure, prosa_file))\n",
      "+                        print(('FILE NOT FOUND FOR STRUCTURE {}: {}'.format(structure, prosa_file)))\n",
      "                     continue\n",
      "             else:\n",
      "                 residues_df = pd.read_csv(residues_file, sep='\\t', index_col=None)\n",
      "@@ -147,9 +147,9 @@\n",
      "     else:\n",
      "         # Load results\n",
      "         correlations_df = pd.read_csv(output_correlations_file, sep='\\t', index_col=None)\n",
      "-        scoring_function_to_scores = cPickle.load(open(scoring_function_to_scores_file))\n",
      "-        scoring_function_to_types = cPickle.load(open(scoring_function_to_types_file))\n",
      "-        score1_to_score2_to_rvalues = cPickle.load(open(score1_to_score2_to_rvalues_file))\n",
      "+        scoring_function_to_scores = pickle.load(open(scoring_function_to_scores_file))\n",
      "+        scoring_function_to_types = pickle.load(open(scoring_function_to_types_file))\n",
      "+        score1_to_score2_to_rvalues = pickle.load(open(score1_to_score2_to_rvalues_file))\n",
      " \n",
      " \n",
      "     # Make plots and table of correlations\n",
      "@@ -163,7 +163,7 @@\n",
      "         results = [scoring_function1]\n",
      "         for scoring_function2 in alternative_scoring_functions:\n",
      " \n",
      "-            print('Plotting: {} vs {}'.format(scoring_function1, scoring_function2))\n",
      "+            print(('Plotting: {} vs {}'.format(scoring_function1, scoring_function2)))\n",
      " \n",
      "             # #### Make scatter plot (not available because there are too many residues!) ####\n",
      "             # output_plot = os.path.join(plots_dir, 'residue_scores_scatter_{}_vs_{}.png'.format(scoring_function1, scoring_function2))\n",
      "@@ -211,7 +211,7 @@\n",
      "             mean = np.mean(correlations)\n",
      "             sdev = np.std(correlations)\n",
      "             results = results + ['{:.2f}'.format(mean), '{:.2f}'.format(sdev)]\n",
      "-            print(scoring_function1, scoring_function2, mean, sdev)\n",
      "+            print((scoring_function1, scoring_function2, mean, sdev))\n",
      "         df2 = pd.DataFrame([results], columns=columns)\n",
      "         nice_mean_correlations_df = nice_mean_correlations_df.append(df2)\n",
      "     nice_mean_correlations_df.to_csv(output_nice_mean_correlations_file, sep='\\t', index=False)\n",
      "--- ./CASP12_analysis/scripts/plot_results_per_target.py\t(original)\n",
      "+++ ./CASP12_analysis/scripts/plot_results_per_target.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " import sys, os\n",
      "-import cPickle\n",
      "+import pickle\n",
      " import matplotlib.pyplot as plt\n",
      " import numpy as np\n",
      " import optparse\n",
      "@@ -92,7 +92,7 @@\n",
      "                 r_value, p_value = pearsonr(scores_spserver, scores_metric)\n",
      "                 r_value = round(r_value, 2)\n",
      "                 r_values.append(r_value)\n",
      "-                print ('Correlation for energies {} and {}: {}'.format(spserver_scoring_function, metric, r_value))\n",
      "+                print(('Correlation for energies {} and {}: {}'.format(spserver_scoring_function, metric, r_value)))\n",
      " \n",
      "                 # Make plot\n",
      "                 output_plot = os.path.join(results_per_target_dir, '{}_{}_vs_{}.png'.format(target, spserver_scoring_function, metric))\n",
      "--- ./CASP12_analysis/scripts/run_CASP_targets_SPServer_cluster.py\t(original)\n",
      "+++ ./CASP12_analysis/scripts/run_CASP_targets_SPServer_cluster.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " import sys, os\n",
      "-import ConfigParser \n",
      "+import configparser \n",
      " import hashlib\n",
      " import optparse\n",
      " import subprocess\n",
      "@@ -51,7 +51,7 @@\n",
      "     sys.path.append(src_path)\n",
      " \n",
      "     # Read configuration file #     \n",
      "-    config = ConfigParser.ConfigParser()\n",
      "+    config = configparser.ConfigParser()\n",
      "     config_file = os.path.join(src_path, \"config.ini\")\n",
      "     config.read(config_file)\n",
      " \n",
      "@@ -84,7 +84,7 @@\n",
      "     # Gather the target folders\n",
      "     target_dirs = [f for f in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, f))]\n",
      "     print(target_dirs)\n",
      "-    print(len(target_dirs))\n",
      "+    print((len(target_dirs)))\n",
      " \n",
      "     # Get the parameters\n",
      "     pot_type = config.get(\"Parameters\", \"pot_type\")\n",
      "@@ -102,7 +102,7 @@\n",
      " \n",
      "             if limit: # Break the loop if a limit of jobs is introduced\n",
      "                 if l > limit:\n",
      "-                    print('The number of submitted jobs arrived to the limit of {}. The script will stop sending submissions!'.format(limit))\n",
      "+                    print(('The number of submitted jobs arrived to the limit of {}. The script will stop sending submissions!'.format(limit)))\n",
      "                     break\n",
      " \n",
      "             # Output dir for the specific file\n",
      "--- ./PPI/scripts/TransformMatrix.py\t(original)\n",
      "+++ ./PPI/scripts/TransformMatrix.py\t(refactored)\n",
      "@@ -97,7 +97,7 @@\n",
      "     # Normalize the scores of the protein A\n",
      "     for residue_a, energies_a in df_a.iterrows():\n",
      "         norm_energies = []\n",
      "-        for x in xrange(len(energies_a)):\n",
      "+        for x in range(len(energies_a)):\n",
      "             energy = float(energies_a[x])\n",
      "             column = df_a.iloc[:,x]\n",
      "             column = pd.to_numeric(column)\n",
      "@@ -114,7 +114,7 @@\n",
      "     # Normalize the scores of the protein B\n",
      "     for residue_b, energies_b in df_b.iterrows():\n",
      "         norm_energies = []\n",
      "-        for x in xrange(len(energies_b)):\n",
      "+        for x in range(len(energies_b)):\n",
      "             energy = float(energies_b[x])\n",
      "             column = df_a.iloc[:,x]\n",
      "             column = pd.to_numeric(column)\n",
      "@@ -140,7 +140,7 @@\n",
      "         for residue_b, energies_b in df_b_norm.iterrows():\n",
      " \n",
      "             pair = '{}-{}'.format(residue_a, residue_b)\n",
      "-            product = [  float(energies_a[x]) * float(energies_b[x])  for x in xrange(len(energies_a)) ]\n",
      "+            product = [  float(energies_a[x]) * float(energies_b[x])  for x in range(len(energies_a)) ]\n",
      "             df2 = pd.DataFrame([product], index=[pair])\n",
      "             df_pairs = df_pairs.append(df2)\n",
      " \n",
      "--- ./SBI/__init__.py\t(original)\n",
      "+++ ./SBI/__init__.py\t(refactored)\n",
      "@@ -422,7 +422,7 @@\n",
      "         @pdef:     object\n",
      "         @ptype:    {object}\n",
      "         '''\n",
      "-        if isinstance(source_object, basestring):\n",
      "+        if isinstance(source_object, str):\n",
      "             return '[' + source_object.upper() + ']: '\n",
      "         elif source_object is not None:\n",
      "             return '[' + source_object.__class__.__name__.upper() + ']: '\n",
      "--- ./SBI/beans/IndexedNum.py\t(original)\n",
      "+++ ./SBI/beans/IndexedNum.py\t(refactored)\n",
      "@@ -129,7 +129,7 @@\n",
      "             return self.number == other\n",
      "         elif isinstance(other, IndexedNum):\n",
      "             return self.number == other.number and self.index == other.index\n",
      "-        elif isinstance(other, basestring):\n",
      "+        elif isinstance(other, str):\n",
      "             return str(self).strip() == other.strip()\n",
      "         return NotImplemented\n",
      " \n",
      "--- ./SBI/beans/JSONer.py\t(original)\n",
      "+++ ./SBI/beans/JSONer.py\t(refactored)\n",
      "@@ -13,14 +13,13 @@\n",
      " from abc import ABCMeta, abstractmethod\n",
      " \n",
      " \n",
      "-class JSONer(object):\n",
      "+class JSONer(object, metaclass=ABCMeta):\n",
      "     '''\n",
      "     Children from this class have the json() method to be exported in json\n",
      "     format.\n",
      "     Every children must implement the as_dict() method that will be used to\n",
      "     format every attribute in a way that is json-compatible.\n",
      "     '''\n",
      "-    __metaclass__ = ABCMeta\n",
      " \n",
      "     @abstractmethod\n",
      "     def as_dict(self):\n",
      "--- ./SBI/beans/StorableObject.py\t(original)\n",
      "+++ ./SBI/beans/StorableObject.py\t(refactored)\n",
      "@@ -11,7 +11,7 @@\n",
      " '''\n",
      " from abc import ABCMeta\n",
      " try:\n",
      "-    import cPickle as pickle\n",
      "+    import pickle as pickle\n",
      " except:\n",
      "     import pickle\n",
      " \n",
      "@@ -19,7 +19,7 @@\n",
      " from .. import SBIglobals as SBIg\n",
      " \n",
      " \n",
      "-class StorableObject(object):\n",
      "+class StorableObject(object, metaclass=ABCMeta):\n",
      "     '''\n",
      "     An abstract \"dumping\" class.\n",
      " \n",
      "@@ -29,7 +29,6 @@\n",
      "     recovered afterwards.\n",
      " \n",
      "     '''\n",
      "-    __metaclass__ = ABCMeta\n",
      " \n",
      "     def dump(self, object_file, overwrite = None):\n",
      "         '''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Refactored ./CASP12_analysis/scripts/plot_residue_scores.py\n",
      "RefactoringTool: Refactored ./CASP12_analysis/scripts/plot_results_per_target.py\n",
      "RefactoringTool: No changes to ./CASP12_analysis/scripts/residue_score.py\n",
      "RefactoringTool: Refactored ./CASP12_analysis/scripts/run_CASP_targets_SPServer_cluster.py\n",
      "RefactoringTool: No changes to ./CASP12_analysis/scripts/zscore.py\n",
      "RefactoringTool: Refactored ./PPI/scripts/TransformMatrix.py\n",
      "RefactoringTool: Refactored ./SBI/__init__.py\n",
      "RefactoringTool: No changes to ./SBI/beans/Executable.py\n",
      "RefactoringTool: Refactored ./SBI/beans/IndexedNum.py\n",
      "RefactoringTool: Refactored ./SBI/beans/JSONer.py\n",
      "RefactoringTool: No changes to ./SBI/beans/Path.py\n",
      "RefactoringTool: Refactored ./SBI/beans/StorableObject.py\n",
      "RefactoringTool: No changes to ./SBI/beans/__init__.py\n",
      "RefactoringTool: Refactored ./SBI/beans/butler.py\n",
      "RefactoringTool: No changes to ./SBI/beans/file.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ./SBI/beans/butler.py\t(original)\n",
      "+++ ./SBI/beans/butler.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " '''\n",
      "-\\usepackage{hyperref}\n",
      "+\\\\usepackage{hyperref}\n",
      " \n",
      " @file: butler.py\n",
      " \n",
      "@@ -32,7 +32,7 @@\n",
      " from .singleton import Singleton\n",
      " \n",
      " \n",
      "-class Butler(object):\n",
      "+class Butler(object, metaclass=Singleton):\n",
      "     '''\n",
      "     Designed to work through all the \\textbf{SBI library}.\n",
      " \n",
      "@@ -46,7 +46,6 @@\n",
      " \n",
      "     Regarding the looging, it provides functions similar to those of a logger.\n",
      "     '''\n",
      "-    __metaclass__   = Singleton\n",
      " \n",
      "     _LOGNAME        = 'SBILOG'\n",
      "     _GENERAL_FORMAT = '[{0}] ' + '%(asctime)s - %(levelname)-7.7s - %(message)s'\n",
      "--- ./SBI/data/__init__.py\t(original)\n",
      "+++ ./SBI/data/__init__.py\t(refactored)\n",
      "@@ -55,7 +55,7 @@\n",
      "     'ACE': 'X', '3FG': 'X', 'UNK': 'X'\n",
      " }\n",
      " \n",
      "-aminoacids1to3 = dict([[v, k] for k, v in aminoacids3to1.items()])\n",
      "+aminoacids1to3 = dict([[v, k] for k, v in list(aminoacids3to1.items())])\n",
      " aminoacids1to3['A'] = 'ALA'\n",
      " aminoacids1to3['N'] = 'ASN'\n",
      " aminoacids1to3['R'] = 'ARG'\n",
      "--- ./SBI/databases/DrugBanklink.py\t(original)\n",
      "+++ ./SBI/databases/DrugBanklink.py\t(refactored)\n",
      "@@ -10,7 +10,7 @@\n",
      " Import Standard Libraries\n",
      " \"\"\"\n",
      " import os, re\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " \"\"\"\n",
      " Dependences in SBI library\n",
      "@@ -52,8 +52,8 @@\n",
      "             raise NameError('A local drugBank database directory must be defined.')\n",
      " \n",
      "         Path.mkdir(self.local)\n",
      "-        urllib.urlretrieve(drugBankftp['targets'], self._target)\n",
      "-        urllib.urlretrieve(drugBankftp['main'],    self._main)\n",
      "+        urllib.request.urlretrieve(drugBankftp['targets'], self._target)\n",
      "+        urllib.request.urlretrieve(drugBankftp['main'],    self._main)\n",
      " \n",
      "         self._process()\n",
      " \n",
      "@@ -77,11 +77,11 @@\n",
      "         drugFile.close()\n",
      " \n",
      "     def _process_targets(self):\n",
      "-        import csv, StringIO, zipfile\n",
      "+        import csv, io, zipfile\n",
      " \n",
      "         targets = {}\n",
      "         z       = zipfile.ZipFile(self._target, 'r')\n",
      "-        data    = StringIO.StringIO(z.read(z.namelist()[0]))\n",
      "+        data    = io.StringIO(z.read(z.namelist()[0]))\n",
      "         reader  = csv.reader(data)\n",
      "         for row in reader:\n",
      "             if row[5] != '':\n",
      "--- ./SBI/databases/Enzymelink.py\t(original)\n",
      "+++ ./SBI/databases/Enzymelink.py\t(refactored)\n",
      "@@ -10,7 +10,7 @@\n",
      " Import Standard Libraries\n",
      " \"\"\"\n",
      " import os, re\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " \"\"\"\n",
      " Dependences in SBI library\n",
      "@@ -62,8 +62,8 @@\n",
      "             raise NameError('A local Enzyme database directory must be defined.')\n",
      " \n",
      "         Path.mkdir(self.local)\n",
      "-        urllib.urlretrieve(Enzymeftp['dat'], self._dfile)\n",
      "-        urllib.urlretrieve(Enzymeftp['cls'], self._cfile)\n",
      "+        urllib.request.urlretrieve(Enzymeftp['dat'], self._dfile)\n",
      "+        urllib.request.urlretrieve(Enzymeftp['cls'], self._cfile)\n",
      " \n",
      "         self._process()\n",
      " \n",
      "--- ./SBI/databases/GOlink.py\t(original)\n",
      "+++ ./SBI/databases/GOlink.py\t(refactored)\n",
      "@@ -10,7 +10,7 @@\n",
      " Import Standard Libraries\n",
      " \"\"\"\n",
      " import os, re\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " \"\"\"\n",
      " Dependences in SBI library\n",
      "@@ -61,7 +61,7 @@\n",
      "         Path.mkdir(self.local)\n",
      "         destination = os.path.join(self.local, self._gfile)\n",
      " \n",
      "-        urllib.urlretrieve(GOftp['source'], destination)\n",
      "+        urllib.request.urlretrieve(GOftp['source'], destination)\n",
      "         self._process()\n",
      " \n",
      "         return True\n",
      "--- ./SBI/databases/PDBTMlink.py\t(original)\n",
      "+++ ./SBI/databases/PDBTMlink.py\t(refactored)\n",
      "@@ -10,7 +10,7 @@\n",
      " Import Standard Libraries\n",
      " \"\"\"\n",
      " import os, re\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " \"\"\"\n",
      " Dependences in SBI library\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: No changes to ./SBI/beans/singleton.py\n",
      "RefactoringTool: Refactored ./SBI/data/__init__.py\n",
      "RefactoringTool: Refactored ./SBI/databases/DrugBanklink.py\n",
      "RefactoringTool: Refactored ./SBI/databases/Enzymelink.py\n",
      "RefactoringTool: Refactored ./SBI/databases/GOlink.py\n",
      "RefactoringTool: Refactored ./SBI/databases/PDBTMlink.py\n",
      "RefactoringTool: Refactored ./SBI/databases/PDBeChemlink.py\n",
      "RefactoringTool: Refactored ./SBI/databases/PDBlink.py\n",
      "RefactoringTool: Refactored ./SBI/databases/SCOPlink.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ./SBI/databases/PDBeChemlink.py\t(original)\n",
      "+++ ./SBI/databases/PDBeChemlink.py\t(refactored)\n",
      "@@ -14,7 +14,7 @@\n",
      " import re\n",
      " import subprocess\n",
      " import warnings\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " \"\"\"\n",
      " Dependences in SBI library\n",
      "@@ -64,7 +64,7 @@\n",
      "         Path.mkdir(self.local)\n",
      "         destination = os.path.join(self.local, 'mmcif.tar.gz')\n",
      "         try:\n",
      "-            urllib.urlretrieve(PDBeChemftp['global'], destination)\n",
      "+            urllib.request.urlretrieve(PDBeChemftp['global'], destination)\n",
      "         except:\n",
      "             return False\n",
      "         command = ['tar', 'zxvf', destination, '-C', self.local]\n",
      "@@ -84,7 +84,7 @@\n",
      "         chem_file = chemID.upper() + '.cif'\n",
      "         source = PDBeChemftp['single'] + chem_file\n",
      "         try:\n",
      "-            urllib.urlretrieve(source, chem_file)\n",
      "+            urllib.request.urlretrieve(source, chem_file)\n",
      "         except:\n",
      "             return False\n",
      "         return os.path.abspath(chem_file)\n",
      "--- ./SBI/databases/PDBlink.py\t(original)\n",
      "+++ ./SBI/databases/PDBlink.py\t(refactored)\n",
      "@@ -12,7 +12,7 @@\n",
      " import sys, os, copy\n",
      " import subprocess\n",
      " import warnings\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " import ftplib\n",
      " \n",
      " \"\"\"\n",
      "@@ -64,7 +64,7 @@\n",
      "     \"\"\"METHODS\"\"\"\n",
      "     def get_PDBseq_filtered(self, resolution_threshold, output_file):\n",
      "         resolutions     = self.get_resolutions()\n",
      "-        names           = [k for k, v in resolutions.iteritems() if float(v) <= float(resolution_threshold)]\n",
      "+        names           = [k for k, v in resolutions.items() if float(v) <= float(resolution_threshold)]\n",
      "         sequences       = Fasta(os.path.join(self.PDBseq,'PDBseq.fa'))\n",
      "         selectedseq     = sequences.retrieve(copy.deepcopy(names), prefix_size = 4)\n",
      "         return Fasta.build_multifasta(output_file, selectedseq, True)\n",
      "@@ -182,7 +182,7 @@\n",
      "         pdb_file = 'pdb' + pdbID.lower() + '.ent.gz'\n",
      "         source = 'ftp://' + PDBftp['address'] + os.path.join(PDBftp['structures'], pdbID[1:3].lower(), pdb_file)\n",
      "         try:\n",
      "-            urllib.urlretrieve(source, pdb_file)\n",
      "+            urllib.request.urlretrieve(source, pdb_file)\n",
      "         except:\n",
      "             return False\n",
      "         return os.path.abspath(pdb_file)\n",
      "--- ./SBI/databases/SCOPlink.py\t(original)\n",
      "+++ ./SBI/databases/SCOPlink.py\t(refactored)\n",
      "@@ -10,7 +10,7 @@\n",
      " Import Standard Libraries\n",
      " \"\"\"\n",
      " import os, re\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " \"\"\"\n",
      " Dependences in SBI library\n",
      "@@ -64,7 +64,7 @@\n",
      "             raise NameError('A local SCOP database directory must be defined.')\n",
      " \n",
      "         Path.mkdir(self.local)\n",
      "-        urllib.urlretrieve(SCOPftp['desc'], self._desc)\n",
      "-        urllib.urlretrieve(SCOPftp['rel'],  self._rel)\n",
      "+        urllib.request.urlretrieve(SCOPftp['desc'], self._desc)\n",
      "+        urllib.request.urlretrieve(SCOPftp['rel'],  self._rel)\n",
      " \n",
      "         return True\n",
      "--- ./SBI/databases/TaxIDlink.py\t(original)\n",
      "+++ ./SBI/databases/TaxIDlink.py\t(refactored)\n",
      "@@ -12,7 +12,7 @@\n",
      " import sys, os, re\n",
      " import subprocess\n",
      " import warnings\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " \"\"\"\n",
      " Dependences in SBI library\n",
      "@@ -73,7 +73,7 @@\n",
      " \n",
      "         Path.mkdir(self.local)\n",
      "         destination = os.path.join(self.local, 'taxdmp.zip')\n",
      "-        urllib.urlretrieve(taxIDftp['global'], destination)\n",
      "+        urllib.request.urlretrieve(taxIDftp['global'], destination)\n",
      "         command = ['unzip', '-o', destination, '-d', self.local]\n",
      "         p = subprocess.Popen(command, stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
      "         out, err = p.communicate()\n",
      "--- ./SBI/databases/Uniprotlink.py\t(original)\n",
      "+++ ./SBI/databases/Uniprotlink.py\t(refactored)\n",
      "@@ -12,7 +12,7 @@\n",
      " import sys, os, re\n",
      " import subprocess\n",
      " import warnings\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " \"\"\"\n",
      " Dependences in SBI library\n",
      "@@ -84,9 +84,9 @@\n",
      " \n",
      "         Path.mkdir(self.local)\n",
      "         destination = os.path.join(self.local, 'uniprot_sprot.dat.gz')\n",
      "-        urllib.urlretrieve(Uniprotftp['swissprot'], destination)\n",
      "+        urllib.request.urlretrieve(Uniprotftp['swissprot'], destination)\n",
      "         destination = os.path.join(self.local, 'uniprot_trembl.dat.gz')\n",
      "-        urllib.urlretrieve(Uniprotftp['trembl'], destination)\n",
      "+        urllib.request.urlretrieve(Uniprotftp['trembl'], destination)\n",
      " \n",
      "         self._process()\n",
      " \n",
      "@@ -169,7 +169,7 @@\n",
      "         if self._ox is None:\n",
      "             self._ox = self._extract_taxid(value)\n",
      "         else:\n",
      "-            raise('More than one taxid! {0}'.format(value))\n",
      "+            raise 'More than one taxid! {0}'\n",
      " \n",
      "     @property\n",
      "     def hosts(self): return self._oc\n",
      "--- ./SBI/databases/__Uniprotlink.py\t(original)\n",
      "+++ ./SBI/databases/__Uniprotlink.py\t(refactored)\n",
      "@@ -12,7 +12,7 @@\n",
      " import sys, os, re\n",
      " import subprocess\n",
      " import warnings\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " \"\"\"\n",
      " Dependences in SBI library\n",
      "@@ -84,9 +84,9 @@\n",
      " \n",
      "         Path.mkdir(self.local)\n",
      "         destination = os.path.join(self.local, 'uniprot_sprot.dat.gz')\n",
      "-        urllib.urlretrieve(Uniprotftp['swissprot'], destination)\n",
      "+        urllib.request.urlretrieve(Uniprotftp['swissprot'], destination)\n",
      "         destination = os.path.join(self.local, 'uniprot_trembl.dat.gz')\n",
      "-        urllib.urlretrieve(Uniprotftp['trembl'], destination)\n",
      "+        urllib.request.urlretrieve(Uniprotftp['trembl'], destination)\n",
      " \n",
      "         self._process()\n",
      " \n",
      "@@ -169,7 +169,7 @@\n",
      "         if self._ox is None:\n",
      "             self._ox = self._extract_taxid(value)\n",
      "         else:\n",
      "-            raise('More than one taxid! {0}'.format(value))\n",
      "+            raise 'More than one taxid! {0}'\n",
      " \n",
      "     @property\n",
      "     def hosts(self): return self._oc\n",
      "--- ./SBI/databases/__init__.py\t(original)\n",
      "+++ ./SBI/databases/__init__.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " __all__ = [\"DBlink\"]\n",
      "-from dblink import DBlink\n",
      "+from .dblink import DBlink\n",
      " \n",
      " \"\"\"\n",
      " PDB:\n",
      "@@ -70,13 +70,13 @@\n",
      " \"\"\"\n",
      " INCLUDES\n",
      " \"\"\"\n",
      "-from PDBlink      import PDBlink\n",
      "-from PDBeChemlink import PDBeChemlink, PDBeChem\n",
      "-from GOlink       import GOlink, GOterm\n",
      "-from TaxIDlink    import TaxIDlink, TaxID\n",
      "-from Uniprotlink  import Uniprotlink, Uniprot\n",
      "-from Enzymelink   import Enzymelink, Enzyme\n",
      "-from DrugBanklink import DrugBanklink, Drug\n",
      "-from SCOPlink     import SCOPlink\n",
      "-from PDBTMlink    import PDBTMlink, TM\n",
      "+from .PDBlink      import PDBlink\n",
      "+from .PDBeChemlink import PDBeChemlink, PDBeChem\n",
      "+from .GOlink       import GOlink, GOterm\n",
      "+from .TaxIDlink    import TaxIDlink, TaxID\n",
      "+from .Uniprotlink  import Uniprotlink, Uniprot\n",
      "+from .Enzymelink   import Enzymelink, Enzyme\n",
      "+from .DrugBanklink import DrugBanklink, Drug\n",
      "+from .SCOPlink     import SCOPlink\n",
      "+from .PDBTMlink    import PDBTMlink, TM\n",
      " \n",
      "--- ./SBI/databases/dblink.py\t(original)\n",
      "+++ ./SBI/databases/dblink.py\t(refactored)\n",
      "@@ -12,7 +12,7 @@\n",
      " from abc import ABCMeta, abstractmethod\n",
      " from datetime import date\n",
      " # import time\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " import os\n",
      " import json\n",
      "@@ -22,12 +22,11 @@\n",
      " from SBI       import SBIglobals as SBIg\n",
      " \n",
      " \n",
      "-class DBlink(object):\n",
      "+class DBlink(object, metaclass=ABCMeta):\n",
      "     '''\n",
      "     Manages the connection to databases.\n",
      "     It creates a local copy of the DB that can be queried a posteriori.\n",
      "     '''\n",
      "-    __metaclass__ = ABCMeta\n",
      " \n",
      "     _MANDATORY_FILES = []\n",
      "     _ITEM_FILES      = []\n",
      "@@ -91,7 +90,7 @@\n",
      "         @yields: Object depending on the database.\n",
      "         '''\n",
      "         if not self.has_local:\n",
      "-            SBIg.throw(self, 'A local database needs to be build first', IOError)\n",
      "+            SBIg.throw(self('A local database needs to be build first').with_traceback(IOError))\n",
      " \n",
      "         for ifile in self._ITEM_FILES:\n",
      "             ifile = os.path.join(self.local, ifile)\n",
      "@@ -207,7 +206,7 @@\n",
      "             if download:\n",
      "                 SBIg.alert('verbose', self, 'Downloading {0} to {1}'.format(source, destination))\n",
      "                 SBIg.alert('verbose', self, 'Source file size is {0:.3f} MB.'.format(source_size))\n",
      "-                urllib.urlretrieve(source, destination)\n",
      "+                urllib.request.urlretrieve(source, destination)\n",
      " \n",
      "     def _clean_sources(self):\n",
      "         '''\n",
      "@@ -229,7 +228,7 @@\n",
      " \n",
      "         @return: {Float}\n",
      "         '''\n",
      "-        usock = urllib.urlopen(external_file)\n",
      "+        usock = urllib.request.urlopen(external_file)\n",
      "         size  = usock.info().get('Content-Length')\n",
      "         if size is None:\n",
      "             return 0\n",
      "--- ./SBI/databases/uniprot/__init__.py\t(original)\n",
      "+++ ./SBI/databases/uniprot/__init__.py\t(refactored)\n",
      "@@ -1,4 +1,4 @@\n",
      "-from connect import Connect\n",
      "-from uniprot import Uniprot\n",
      "+from .connect import Connect\n",
      "+from .uniprot import Uniprot\n",
      " \n",
      " __all__ = [\"Connect\", \"Uniprot\"]\n",
      "--- ./SBI/databases/uniprot/connect.py\t(original)\n",
      "+++ ./SBI/databases/uniprot/connect.py\t(refactored)\n",
      "@@ -2,7 +2,7 @@\n",
      " import re\n",
      " \n",
      " from SBI.databases import DBlink\n",
      "-from uniprot       import Uniprot\n",
      "+from .uniprot       import Uniprot\n",
      " from SBI.beans     import File\n",
      " from SBI           import SBIglobals as SBIg\n",
      " \n",
      "--- ./SBI/databases/uniprot/uniprot.py\t(original)\n",
      "+++ ./SBI/databases/uniprot/uniprot.py\t(refactored)\n",
      "@@ -272,7 +272,7 @@\n",
      " \n",
      "         @return: {Uniprot}\n",
      "         '''\n",
      "-        if isinstance(json_line, basestring):\n",
      "+        if isinstance(json_line, str):\n",
      "             json_line = json.loads(json_line.strip())\n",
      " \n",
      "         data = Uniprot('', '')\n",
      "--- ./SBI/external/ExternalExe.py\t(original)\n",
      "+++ ./SBI/external/ExternalExe.py\t(refactored)\n",
      "@@ -10,7 +10,7 @@\n",
      " @class: ExternalExe\n",
      " '''\n",
      " import os\n",
      "-import ConfigParser\n",
      "+import configparser\n",
      " from os.path import join     as j\n",
      " from os.path import normpath as n\n",
      " from os.path import dirname  as d\n",
      "@@ -19,7 +19,7 @@\n",
      " from SBI.beans import Executable\n",
      " \n",
      " \n",
      "-class ExternalExe(object):\n",
      "+class ExternalExe(object, metaclass=ABCMeta):\n",
      "     '''\n",
      "     Main class to derive others that will control the execution of external\n",
      "     programs.\n",
      "@@ -28,12 +28,11 @@\n",
      "     defined in a file linked to a environment variable called SBI_CONFIG_FILE.\n",
      " \n",
      "     '''\n",
      "-    __metaclass__ = ABCMeta\n",
      " \n",
      "     DEFAULT_CONFIG_FILE = j(n(d(__file__)), 'configSBI.txt')\n",
      " \n",
      "     # Executable configuration\n",
      "-    _CONFIG     = ConfigParser.RawConfigParser(allow_no_value=True)\n",
      "+    _CONFIG     = configparser.RawConfigParser(allow_no_value=True)\n",
      "     _EXE        = None\n",
      " \n",
      "     def __new__(cls, *args, **kwargs):\n",
      "--- ./SBI/external/__init__.py\t(original)\n",
      "+++ ./SBI/external/__init__.py\t(refactored)\n",
      "@@ -1,2 +1,2 @@\n",
      " __all__ = [\"ExternalExe\"]\n",
      "-from ExternalExe import ExternalExe\n",
      "+from .ExternalExe import ExternalExe\n",
      "--- ./SBI/external/CDhit/CDhitExe.py\t(original)\n",
      "+++ ./SBI/external/CDhit/CDhitExe.py\t(refactored)\n",
      "@@ -126,9 +126,8 @@\n",
      "         try:\n",
      "             e.execute()\n",
      "             e.clean_command()\n",
      "-        except SystemError, er:\n",
      "-            SBIg.throw(self, 'Some error occurred while executing cd-hit\\n{0}\\n'.format(er),\n",
      "-                       SystemError)\n",
      "+        except SystemError as er:\n",
      "+            SBIg.throw(self('Some error occurred while executing cd-hit\\n{0}\\n'.format(er)).with_traceback(SystemError))\n",
      " \n",
      "     def make_master_fasta(self, output_fasta = None, force = None):\n",
      "         '''\n",
      "@@ -250,6 +249,5 @@\n",
      "         try:\n",
      "             self._EXE.execute()\n",
      "             self._EXE.clean_command()\n",
      "-        except SystemError, e:\n",
      "-            SBIg.throw(self, 'Some error occurred while executing cd-hit\\n{0}\\n'.format(e),\n",
      "-                       SystemError)\n",
      "+        except SystemError as e:\n",
      "+            SBIg.throw(self('Some error occurred while executing cd-hit\\n{0}\\n'.format(e)).with_traceback(SystemError))\n",
      "--- ./SBI/external/CDhit/__init__.py\t(original)\n",
      "+++ ./SBI/external/CDhit/__init__.py\t(refactored)\n",
      "@@ -1,4 +1,4 @@\n",
      " __all__ = [\"CDhitList\", \"CDhitExe\"]\n",
      " \n",
      "-from CDhitExe  import CDhitExe\n",
      "-from CDhitList import CDhitList\n",
      "+from .CDhitExe  import CDhitExe\n",
      "+from .CDhitList import CDhitList\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Refactored ./SBI/databases/TaxIDlink.py\n",
      "RefactoringTool: Refactored ./SBI/databases/Uniprotlink.py\n",
      "RefactoringTool: Refactored ./SBI/databases/__Uniprotlink.py\n",
      "RefactoringTool: Refactored ./SBI/databases/__init__.py\n",
      "RefactoringTool: Refactored ./SBI/databases/dblink.py\n",
      "RefactoringTool: Refactored ./SBI/databases/uniprot/__init__.py\n",
      "RefactoringTool: Refactored ./SBI/databases/uniprot/connect.py\n",
      "RefactoringTool: Refactored ./SBI/databases/uniprot/uniprot.py\n",
      "RefactoringTool: Refactored ./SBI/external/ExternalExe.py\n",
      "RefactoringTool: Refactored ./SBI/external/__init__.py\n",
      "RefactoringTool: No changes to ./SBI/external/CDhit/CDhit.py\n",
      "RefactoringTool: Refactored ./SBI/external/CDhit/CDhitExe.py\n",
      "RefactoringTool: No changes to ./SBI/external/CDhit/CDhitHomolog.py\n",
      "RefactoringTool: No changes to ./SBI/external/CDhit/CDhitList.py\n",
      "RefactoringTool: Refactored ./SBI/external/CDhit/__init__.py\n",
      "RefactoringTool: No changes to ./SBI/external/DSSP/DSSP.py\n",
      "RefactoringTool: Refactored ./SBI/external/DSSP/DSSPExe.py\n",
      "RefactoringTool: Refactored ./SBI/external/DSSP/__init__.py\n",
      "RefactoringTool: Refactored ./SBI/external/blast/BlastExe.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ./SBI/external/DSSP/DSSPExe.py\t(original)\n",
      "+++ ./SBI/external/DSSP/DSSPExe.py\t(refactored)\n",
      "@@ -13,7 +13,7 @@\n",
      " import sys\n",
      " \n",
      " from SBI.external import ExternalExe\n",
      "-from DSSP         import DSSP\n",
      "+from .DSSP         import DSSP\n",
      " from SBI          import SBIglobals as SBIg\n",
      " from SBI.beans    import File\n",
      " \n",
      "@@ -121,9 +121,9 @@\n",
      "         self._EXE.add_parameter(self._dsspfile)\n",
      "         try:\n",
      "             self._EXE.execute(silent=True)\n",
      "-        except SystemError, e:\n",
      "+        except SystemError as e:\n",
      "             msg = 'Some error occurred while executing dssp\\n{0}\\n'.format(e)\n",
      "-            SBIg.throw(self, msg, e)\n",
      "+            SBIg.throw(self(msg).with_traceback(e))\n",
      " \n",
      "         self._EXE.clean_command()\n",
      " \n",
      "--- ./SBI/external/DSSP/__init__.py\t(original)\n",
      "+++ ./SBI/external/DSSP/__init__.py\t(refactored)\n",
      "@@ -1,4 +1,4 @@\n",
      " __all__ = [\"DSSPExe\", \"DSSP\"]\n",
      " \n",
      "-from DSSPExe import DSSPExe\n",
      "-from DSSP    import DSSP\n",
      "+from .DSSPExe import DSSPExe\n",
      "+from .DSSP    import DSSP\n",
      "--- ./SBI/external/blast/BlastExe.py\t(original)\n",
      "+++ ./SBI/external/blast/BlastExe.py\t(refactored)\n",
      "@@ -14,7 +14,7 @@\n",
      " import os\n",
      " import time\n",
      " import re\n",
      "-import ConfigParser\n",
      "+import configparser\n",
      " from abc import ABCMeta\n",
      " \n",
      " from bs4 import BeautifulSoup\n",
      "@@ -25,9 +25,9 @@\n",
      " from SBI.beans    import Path\n",
      " from SBI.sequence import Fasta\n",
      " from SBI          import SBIglobals as SBIg\n",
      "-from BlastResult  import BlastResult\n",
      "-from BlastResult  import BlastHeader\n",
      "-from BlastHit     import BlastHit\n",
      "+from .BlastResult  import BlastResult\n",
      "+from .BlastResult  import BlastHeader\n",
      "+from .BlastHit     import BlastHit\n",
      " \n",
      " \n",
      " class BlastExe(ExternalExe):\n",
      "@@ -321,7 +321,7 @@\n",
      " \n",
      "         @returns: {BlastResult}\n",
      "         '''\n",
      "-        if isinstance(query_file, basestring) or isinstance(query_file, File):\n",
      "+        if isinstance(query_file, str) or isinstance(query_file, File):\n",
      "             newFasta = Fasta(fasta_file = query_file)\n",
      "         elif isinstance(query_file, Fasta):\n",
      "             newFasta = query_file\n",
      "@@ -458,7 +458,7 @@\n",
      "         if len(formatdb_files) > 0:\n",
      "             try:\n",
      "                 self._format_database(database)\n",
      "-            except ConfigParser.NoOptionError as e:\n",
      "+            except configparser.NoOptionError as e:\n",
      "                 raise self._error.no_blast_format_exe(e)\n",
      "             except SystemError as e:\n",
      "                 raise self._error.wrong_db_format(database, e)\n",
      "@@ -520,11 +520,10 @@\n",
      "         return search_type\n",
      " \n",
      " \n",
      "-class BlastParser(object):\n",
      "+class BlastParser(object, metaclass=ABCMeta):\n",
      "     '''\n",
      "     Processes a blast xml formated output into a {BlastResult} object.\n",
      "     '''\n",
      "-    __metaclass__ = ABCMeta\n",
      " \n",
      "     @staticmethod\n",
      "     def parse(query_sequence, blast_output_file, self_hit, hitid_format):\n",
      "--- ./SBI/external/blast/BlastHit.py\t(original)\n",
      "+++ ./SBI/external/blast/BlastHit.py\t(refactored)\n",
      "@@ -275,7 +275,7 @@\n",
      "         complex_indexes = {}\n",
      "         for x in range(self.number_of_sequences):\n",
      "             seqInits[x] = self._sequence_position_from_alignment_position(x, index[0])\n",
      "-            complex_indexes[x] = list(filter(None, self._seq2ali[x]))\n",
      "+            complex_indexes[x] = list([_f for _f in self._seq2ali[x] if _f])\n",
      "             complex_indexes[x] = complex_indexes[x][complex_indexes[x].index(seqInits[x]):]\n",
      "         patt   = self._alipatt[index[0] - 1:index[1] - 1:index[2]]\n",
      "         newali = self.__class__(hit = [self._sequenceID, self._length],\n",
      "--- ./SBI/external/blast/BlastResult.py\t(original)\n",
      "+++ ./SBI/external/blast/BlastResult.py\t(refactored)\n",
      "@@ -14,7 +14,7 @@\n",
      " import os\n",
      " import copy\n",
      " \n",
      "-import BlastExe\n",
      "+from . import BlastExe\n",
      " from SBI.beans              import StorableObject\n",
      " from SBI.beans              import File\n",
      " from SBI.sequence           import Sequence\n",
      "@@ -654,7 +654,7 @@\n",
      "             output.write(\"%s\\n\" % self.str_compacted_blast())\n",
      "             output.close()\n",
      "         else:\n",
      "-            print self.str_compacted_blast()\n",
      "+            print(self.str_compacted_blast())\n",
      " \n",
      "     def print_representation(self, line_split = 160, out_file = None):\n",
      "         '''\n",
      "@@ -675,7 +675,7 @@\n",
      "             output.write(\"%s\\n\" % self.str_representation(line_split))\n",
      "             output.close()\n",
      "         else:\n",
      "-            print self.str_representation(line_split)\n",
      "+            print(self.str_representation(line_split))\n",
      " \n",
      "     @staticmethod\n",
      "     def read_compacted_blast(compacted_blast_file):\n",
      "@@ -689,7 +689,7 @@\n",
      " \n",
      "         @return: {BlastResult}\n",
      "         '''\n",
      "-        from BlastHit import BlastHit\n",
      "+        from .BlastHit import BlastHit\n",
      "         query_name, query_sequence     = None, None\n",
      "         version, matrix, database      = None, None, None\n",
      "         gap_open, gap_extend, self_hit = None, None, None\n",
      "@@ -753,9 +753,9 @@\n",
      "         '''\n",
      "         final_hits = []\n",
      "         final_hits.append(hitlist[0])\n",
      "-        for x in xrange(1, len(hitlist)):\n",
      "+        for x in range(1, len(hitlist)):\n",
      "             accepted = True\n",
      "-            for y in xrange(0, len(final_hits)):\n",
      "+            for y in range(0, len(final_hits)):\n",
      "                 if hitlist[x].overlap(final_hits[y]) > overlap:\n",
      "                     accepted = False\n",
      "                     break\n",
      "@@ -821,7 +821,7 @@\n",
      "         self.gap_open   = int(gap_open)\n",
      "         self.gap_extend = int(gap_extend)\n",
      "         self.database   = database\n",
      "-        if isinstance(self_hit, basestring):\n",
      "+        if isinstance(self_hit, str):\n",
      "             if self_hit.upper() == 'TRUE':\n",
      "                 self.self_hit = True\n",
      "             else:\n",
      "--- ./SBI/external/blast/__init__.py\t(original)\n",
      "+++ ./SBI/external/blast/__init__.py\t(refactored)\n",
      "@@ -1,4 +1,4 @@\n",
      " __all__ = [\"BlastResult\", \"BlastHit\", \"BlastExe\", \"BlastError\"]\n",
      "-from BlastExe    import BlastExe, BlastError\n",
      "-from BlastHit    import BlastHit\n",
      "-from BlastResult import BlastResult\n",
      "+from .BlastExe    import BlastExe, BlastError\n",
      "+from .BlastHit    import BlastHit\n",
      "+from .BlastResult import BlastResult\n",
      "--- ./SBI/external/hmmer/HmmExe.py\t(original)\n",
      "+++ ./SBI/external/hmmer/HmmExe.py\t(refactored)\n",
      "@@ -15,15 +15,15 @@\n",
      " import re\n",
      " import time\n",
      " import subprocess\n",
      "-import ConfigParser\n",
      "+import configparser\n",
      " from abc import ABCMeta\n",
      " \n",
      " from SBI.external import ExternalExe\n",
      " from SBI.beans    import Executable\n",
      " from SBI.beans    import File\n",
      " from SBI.sequence import Fasta\n",
      "-from HmmResult    import HmmResult\n",
      "-from HmmHit       import HmmHit\n",
      "+from .HmmResult    import HmmResult\n",
      "+from .HmmHit       import HmmHit\n",
      " from SBI          import SBIglobals as SBIg\n",
      " \n",
      " \n",
      "@@ -263,7 +263,7 @@\n",
      " \n",
      "         @returns: {HmmResult}\n",
      "         '''\n",
      "-        if isinstance(query_file, basestring) or isinstance(query_file, File):\n",
      "+        if isinstance(query_file, str) or isinstance(query_file, File):\n",
      "             newFasta = Fasta(fasta_file = query_file)\n",
      "         elif isinstance(query_file, Fasta):\n",
      "             newFasta = query_file\n",
      "@@ -368,7 +368,7 @@\n",
      "         if len(formatdb_files) > 0:\n",
      "             try:\n",
      "                 self._format_database(database)\n",
      "-            except ConfigParser.NoOptionError as e:\n",
      "+            except configparser.NoOptionError as e:\n",
      "                 raise self._error.no_hmmer_format_exe(e)\n",
      "             except SystemError as e:\n",
      "                 raise self._error.wrong_db_format(database, e)\n",
      "@@ -430,11 +430,10 @@\n",
      "                 os.unlink(temp_file)\n",
      " \n",
      " \n",
      "-class HmmParser(object):\n",
      "+class HmmParser(object, metaclass=ABCMeta):\n",
      "     '''\n",
      "     Processes a cd-hit output into a {HmmResult} object.\n",
      "     '''\n",
      "-    __metaclass__ = ABCMeta\n",
      " \n",
      "     @staticmethod\n",
      "     def parse(query_name, query_sequence, database, hmmer_output_file):\n",
      "--- ./SBI/external/hmmer/__init__.py\t(original)\n",
      "+++ ./SBI/external/hmmer/__init__.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " __all__ = [\"HmmExe\", \"HmmResult\", \"HmmHit\"]\n",
      " \n",
      "-from HmmExe    import HmmExe\n",
      "-from HmmResult import HmmResult\n",
      "-from HmmHit    import HmmHit\n",
      "+from .HmmExe    import HmmExe\n",
      "+from .HmmResult import HmmResult\n",
      "+from .HmmHit    import HmmHit\n",
      "--- ./SBI/math/__init__.py\t(original)\n",
      "+++ ./SBI/math/__init__.py\t(refactored)\n",
      "@@ -1 +1 @@\n",
      "-import stats\n",
      "+from . import stats\n",
      "--- ./SBI/sequence/Fasta.py\t(original)\n",
      "+++ ./SBI/sequence/Fasta.py\t(refactored)\n",
      "@@ -35,7 +35,7 @@\n",
      "         @pdefault: 10\n",
      "         @ptype:    {Integer}\n",
      "         '''\n",
      "-        if isinstance(fasta_file, basestring):\n",
      "+        if isinstance(fasta_file, str):\n",
      "             self._file = File(file_name = fasta_file, action = 'r')\n",
      "         elif isinstance(fasta_file, File):\n",
      "             self._file = File(file_name = fasta_file.full, action = 'r')\n",
      "@@ -93,7 +93,7 @@\n",
      "         @return: {List}\n",
      "         '''\n",
      "         if self.is_loaded:\n",
      "-            return self._sequenceID.keys()\n",
      "+            return list(self._sequenceID.keys())\n",
      "         else:\n",
      "             keys = []\n",
      "             for sequence in self.live_show():\n",
      "@@ -251,7 +251,7 @@\n",
      "         else:\n",
      "             SBIg.alert('debug', self, [info.format(x) for x in sequence_ids])\n",
      " \n",
      "-        if isinstance(sequence_ids, basestring):\n",
      "+        if isinstance(sequence_ids, str):\n",
      "             sequence_ids = set([sequence_ids])\n",
      "         if isinstance(sequence_ids, list):\n",
      "             sequence_ids = set(sequence_ids)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Refactored ./SBI/external/blast/BlastHit.py\n",
      "RefactoringTool: Refactored ./SBI/external/blast/BlastResult.py\n",
      "RefactoringTool: Refactored ./SBI/external/blast/__init__.py\n",
      "RefactoringTool: Refactored ./SBI/external/hmmer/HmmExe.py\n",
      "RefactoringTool: No changes to ./SBI/external/hmmer/HmmHit.py\n",
      "RefactoringTool: No changes to ./SBI/external/hmmer/HmmResult.py\n",
      "RefactoringTool: Refactored ./SBI/external/hmmer/__init__.py\n",
      "RefactoringTool: Refactored ./SBI/math/__init__.py\n",
      "RefactoringTool: No changes to ./SBI/math/stats.py\n",
      "RefactoringTool: Refactored ./SBI/sequence/Fasta.py\n",
      "RefactoringTool: Refactored ./SBI/sequence/Sequence.py\n",
      "RefactoringTool: Refactored ./SBI/sequence/__init__.py\n",
      "RefactoringTool: Refactored ./SBI/sequence/alignment/Needleman_Wunsch.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ./SBI/sequence/Sequence.py\t(original)\n",
      "+++ ./SBI/sequence/Sequence.py\t(refactored)\n",
      "@@ -184,7 +184,7 @@\n",
      "         @raises: {AttributeError} if sequence is the wrong type.\n",
      "         @return: {Boolean}\n",
      "         '''\n",
      "-        if isinstance(sequence, basestring):\n",
      "+        if isinstance(sequence, str):\n",
      "             return bool(re.search(sequence, self.sequence))\n",
      "         elif isinstance(sequence, list):\n",
      "             return bool(re.search(''.join(sequence), self.sequence))\n",
      "@@ -204,7 +204,7 @@\n",
      "         @raises: {AttributeError} if sequence is the wrong type.\n",
      "         @return: {Boolean}\n",
      "         '''\n",
      "-        if isinstance(sequence, basestring):\n",
      "+        if isinstance(sequence, str):\n",
      "             return bool(re.search(self.sequence, sequence))\n",
      "         elif isinstance(sequence, list):\n",
      "             return bool(re.search(self.sequence, ''.join(sequence)))\n",
      "@@ -224,7 +224,7 @@\n",
      "         @raises: {AttributeError} if sequence is the wrong type.\n",
      "         @return: {List} of {String}\n",
      "         '''\n",
      "-        if isinstance(sequence, basestring):\n",
      "+        if isinstance(sequence, str):\n",
      "             pass\n",
      "         elif isinstance(sequence, list):\n",
      "             sequence = ''.join(sequence)\n",
      "@@ -275,7 +275,7 @@\n",
      "         '''\n",
      "         if algorithm.upper() not in Sequence.AVAILABLE_ALIGN_ALGORITHMS:\n",
      "             raise AttributeError('Alignment algorithm not available')\n",
      "-        if isinstance(sequence, basestring):\n",
      "+        if isinstance(sequence, str):\n",
      "             pass\n",
      "         elif isinstance(sequence, list):\n",
      "             sequence = ''.join(sequence)\n",
      "@@ -285,7 +285,7 @@\n",
      "             return AttributeError('sequence must be string, list or Sequence')\n",
      " \n",
      "         if algorithm.upper() in Sequence.NEEDLEMAN_WUNSCH:\n",
      "-            from alignment.Needleman_Wunsch import needleman_wunsch as nw\n",
      "+            from .alignment.Needleman_Wunsch import needleman_wunsch as nw\n",
      "             return nw(self.sequence, sequence, similarity_matrix,\n",
      "                       gap_init, gap_penalty)\n",
      " \n",
      "@@ -337,7 +337,7 @@\n",
      "         '''\n",
      "         c = Counter(re.sub(Sequence.GAP_DEFINITION, '', self._sequence))\n",
      "         l = float(len(self))\n",
      "-        return dict([(x, y/l) for x, y in c.iteritems()])\n",
      "+        return dict([(x, y/l) for x, y in c.items()])\n",
      " \n",
      "     def duplicate(self, new_id = None):\n",
      "         '''\n",
      "@@ -380,7 +380,7 @@\n",
      " \n",
      "         @raises: {AttributeError} if sequence is the wrong type.\n",
      "         '''\n",
      "-        if isinstance(sequence, basestring):\n",
      "+        if isinstance(sequence, str):\n",
      "             pass\n",
      "         elif isinstance(sequence, list):\n",
      "             sequence = ''.join(sequence)\n",
      "--- ./SBI/sequence/__init__.py\t(original)\n",
      "+++ ./SBI/sequence/__init__.py\t(refactored)\n",
      "@@ -1,4 +1,4 @@\n",
      "-from Sequence import Sequence\n",
      "-from Fasta    import Fasta\n",
      "+from .Sequence import Sequence\n",
      "+from .Fasta    import Fasta\n",
      " \n",
      " __all__ = [\"Sequence\", \"Fasta\"]\n",
      "--- ./SBI/sequence/alignment/Needleman_Wunsch.py\t(original)\n",
      "+++ ./SBI/sequence/alignment/Needleman_Wunsch.py\t(refactored)\n",
      "@@ -13,8 +13,8 @@\n",
      " import numpy as np\n",
      " import re\n",
      " \n",
      "-from SimilarityMatrix import SimilarityMatrix as SM\n",
      "-from SeqAli import SeqAli\n",
      "+from .SimilarityMatrix import SimilarityMatrix as SM\n",
      "+from .SeqAli import SeqAli\n",
      " \n",
      " \n",
      " def needleman_wunsch(sequence1, sequence2, similarity_matrix,\n",
      "--- ./SBI/sequence/alignment/SeqAli.py\t(original)\n",
      "+++ ./SBI/sequence/alignment/SeqAli.py\t(refactored)\n",
      "@@ -85,7 +85,7 @@\n",
      "         for aliseq in sequences:\n",
      "             if isinstance(aliseq, Sequence):\n",
      "                 self._seq.append(aliseq)\n",
      "-            elif isinstance(aliseq, basestring):\n",
      "+            elif isinstance(aliseq, str):\n",
      "                 self._seq.append(Sequence(sequence = aliseq))\n",
      "             else:\n",
      "                 raise AttributeError('sequences must be specified as strings or Sequence objects.')\n",
      "@@ -382,7 +382,7 @@\n",
      "         @ptype:    {Integer}\n",
      "         '''\n",
      "         self._idx[refseq] += (new_index - 1)\n",
      "-        for x in xrange(len(self._segment[refseq])):\n",
      "+        for x in range(len(self._segment[refseq])):\n",
      "             self._segment[refseq][x] = self._segment[refseq][x] - 1 + self._idx[refseq]\n",
      " \n",
      "     def get_respective_coordinate(self, refseq, destseq, pos):\n",
      "@@ -453,7 +453,7 @@\n",
      "         coverage = 0\n",
      "         tk_seqs  = section._tokenize()\n",
      "         for x in range(len(tk_seqs[refseq])):\n",
      "-            if bool(tk_seqs[refseq][x]) and sum(map(lambda n: int(n[x]), tk_seqs)) > 1:\n",
      "+            if bool(tk_seqs[refseq][x]) and sum([int(n[x]) for n in tk_seqs]) > 1:\n",
      "                 coverage += 1\n",
      " \n",
      "         return float(coverage) / int(refseq_full_length)\n",
      "@@ -794,7 +794,7 @@\n",
      " \n",
      "         @return: {Integer}\n",
      "         '''\n",
      "-        for i in reversed(range(len(self))):\n",
      "+        for i in reversed(list(range(len(self)))):\n",
      "             if self._seq2ali[refseq][i] is not None:\n",
      "                 return self._seq2ali[refseq][i]\n",
      " \n",
      "@@ -872,7 +872,7 @@\n",
      "         tk_seqs = self._tokenize()\n",
      " \n",
      "         for x in range(len(self._seq[0])):\n",
      "-            profile_str = \"\".join(map(lambda n: n[x], tk_seqs))\n",
      "+            profile_str = \"\".join([n[x] for n in tk_seqs])\n",
      "             profile_vle = sum([int(i) for i in profile_str])\n",
      "             if profile_vle > 1:\n",
      "                 this_len += 1\n",
      "@@ -919,7 +919,7 @@\n",
      "             self._segment.append([])\n",
      "         tk_seqs = self._tokenize()\n",
      "         for x in range(len(self)):\n",
      "-            profile_str = \"\".join(map(lambda n: n[x], tk_seqs))\n",
      "+            profile_str = \"\".join([n[x] for n in tk_seqs])\n",
      " \n",
      "             for i in range(self._num_seq):\n",
      "                 p0 = self._sequence_position_id(i, x)\n",
      "@@ -973,7 +973,7 @@\n",
      "         complex_indexes = {}\n",
      "         for x in range(self.number_of_sequences):\n",
      "             seqInits[x] = self._sequence_position_from_alignment_position(x, index[0])\n",
      "-            complex_indexes[x] = list(filter(None, self._seq2ali[x]))\n",
      "+            complex_indexes[x] = list([_f for _f in self._seq2ali[x] if _f])\n",
      "             complex_indexes[x] = complex_indexes[x][complex_indexes[x].index(seqInits[x]):]\n",
      "         newali = self.__class__(sequences, seqInits)\n",
      "         for refident in complex_indexes:\n",
      "@@ -1073,7 +1073,7 @@\n",
      "         else:\n",
      "             if int(key) > len(self):\n",
      "                 raise IndexError\n",
      "-            return map(lambda n: n[int(key) - 1], self._seq)\n",
      "+            return [n[int(key) - 1] for n in self._seq]\n",
      " \n",
      "         index = key.indices(len(self) + 1)\n",
      " \n",
      "--- ./SBI/sequence/alignment/__init__.py\t(original)\n",
      "+++ ./SBI/sequence/alignment/__init__.py\t(refactored)\n",
      "@@ -1,3 +1,3 @@\n",
      " __all__ = [\"SeqAli\", \"Rost\"]\n",
      "-from SeqAli        import SeqAli\n",
      "-from SeqAli        import Rost\n",
      "+from .SeqAli        import SeqAli\n",
      "+from .SeqAli        import Rost\n",
      "--- ./SBI/structure/PDB.py\t(original)\n",
      "+++ ./SBI/structure/PDB.py\t(refactored)\n",
      "@@ -523,7 +523,7 @@\n",
      "         \"\"\"\n",
      "         Process and load crystal data from a PDB formated file\n",
      "         \"\"\"\n",
      "-        from parse_pdb import read_PDB_file, read_PDB_header\n",
      "+        from .parse_pdb import read_PDB_file, read_PDB_header\n",
      "         if header:\n",
      "             read_PDB_header(self)\n",
      "             self._pdb_file.close()\n",
      "--- ./SBI/structure/atom/__init__.py\t(original)\n",
      "+++ ./SBI/structure/atom/__init__.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " __all__ = ['Atom', 'AtomOfAminoAcid', 'AtomOfNucleotide']\n",
      " \n",
      "-from Atom             import Atom\n",
      "-from AtomOfAminoAcid  import AtomOfAminoAcid\n",
      "-from AtomOfNucleotide import AtomOfNucleotide\n",
      "+from .Atom             import Atom\n",
      "+from .AtomOfAminoAcid  import AtomOfAminoAcid\n",
      "+from .AtomOfNucleotide import AtomOfNucleotide\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Refactored ./SBI/sequence/alignment/SeqAli.py\n",
      "RefactoringTool: No changes to ./SBI/sequence/alignment/SimilarityMatrix.py\n",
      "RefactoringTool: Refactored ./SBI/sequence/alignment/__init__.py\n",
      "RefactoringTool: Refactored ./SBI/structure/PDB.py\n",
      "RefactoringTool: No changes to ./SBI/structure/parse_pdb.py\n",
      "RefactoringTool: No changes to ./SBI/structure/atom/Atom.py\n",
      "RefactoringTool: No changes to ./SBI/structure/atom/AtomOfAminoAcid.py\n",
      "RefactoringTool: No changes to ./SBI/structure/atom/AtomOfNucleotide.py\n",
      "RefactoringTool: Refactored ./SBI/structure/atom/__init__.py\n",
      "RefactoringTool: Refactored ./SBI/structure/chain/Chain.py\n",
      "RefactoringTool: No changes to ./SBI/structure/chain/ChainOfNucleotide.py\n",
      "RefactoringTool: No changes to ./SBI/structure/chain/ChainOfProtein.py\n",
      "RefactoringTool: Refactored ./SBI/structure/chain/__init__.py\n",
      "RefactoringTool: Refactored ./SBI/structure/contacts/Complex.py\n",
      "RefactoringTool: Refactored ./SBI/structure/contacts/InnerContacts.py\n",
      "RefactoringTool: Refactored ./SBI/structure/contacts/__init__.py\n",
      "RefactoringTool: No changes to ./SBI/structure/contacts/contact/Contact.py\n",
      "RefactoringTool: No changes to ./SBI/structure/contacts/contact/ContactAA.py\n",
      "RefactoringTool: No changes to ./SBI/structure/contacts/contact/ContactAH.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ./SBI/structure/chain/Chain.py\t(original)\n",
      "+++ ./SBI/structure/chain/Chain.py\t(refactored)\n",
      "@@ -494,7 +494,7 @@\n",
      " \n",
      "         @rtype: {Residue}\n",
      "         \"\"\"\n",
      "-        if self.dictitype.has_key(Rtype) and not self._term:\n",
      "+        if Rtype in self.dictitype and not self._term:\n",
      "             return self.resitype(number = number, version = version, Rtype = Rtype, mode = mode)\n",
      "         else:\n",
      "             return Residue(number = number, version = version, Rtype = Rtype, mode = mode)\n",
      "--- ./SBI/structure/chain/__init__.py\t(original)\n",
      "+++ ./SBI/structure/chain/__init__.py\t(refactored)\n",
      "@@ -1,3 +1,3 @@\n",
      "-from Chain             import Chain\n",
      "-from ChainOfProtein    import ChainOfProtein\n",
      "-from ChainOfNucleotide import ChainOfNucleotide\n",
      "+from .Chain             import Chain\n",
      "+from .ChainOfProtein    import ChainOfProtein\n",
      "+from .ChainOfNucleotide import ChainOfNucleotide\n",
      "--- ./SBI/structure/contacts/Complex.py\t(original)\n",
      "+++ ./SBI/structure/contacts/Complex.py\t(refactored)\n",
      "@@ -47,13 +47,13 @@\n",
      "     def biomolecule(self):  return self._biomolecule\n",
      " \n",
      "     @property\n",
      "-    def PPInterfaces(self): return self._PPInterface.values()\n",
      "+    def PPInterfaces(self): return list(self._PPInterface.values())\n",
      " \n",
      "     @property\n",
      "-    def PNInterfaces(self): return self._PNInterface.values()\n",
      "+    def PNInterfaces(self): return list(self._PNInterface.values())\n",
      " \n",
      "     @property\n",
      "-    def PHInterfaces(self): return self._PHInterface.values()\n",
      "+    def PHInterfaces(self): return list(self._PHInterface.values())\n",
      " \n",
      "     #\n",
      "     # PRIVATE FUNCTIONS\n",
      "@@ -94,13 +94,13 @@\n",
      " \n",
      "             SBIglobals.alert('debug', self, '\\tBiomolecule has {0:03} chains -> {1:03} max. Interfaces'.format(total_chains, (total_chains*(total_chains-1))/2))\n",
      " \n",
      "-            for i in xrange(len(protein_chains)):\n",
      "-                for j in xrange(i+1, len(protein_chains)):\n",
      "+            for i in range(len(protein_chains)):\n",
      "+                for j in range(i+1, len(protein_chains)):\n",
      "                     self._add_PPI(protein_chains[i], protein_chains[j], protein_pgeoms[i], protein_pgeoms[j])\n",
      "-                for j in xrange(len(protein_chains)):\n",
      "+                for j in range(len(protein_chains)):\n",
      "                     if i != j:\n",
      "                         self._add_PHI(protein_chains[i], protein_chains[j], protein_pgeoms[i], protein_hgeoms[j])\n",
      "-                for j in xrange(len(nucleotide_chains)):\n",
      "+                for j in range(len(nucleotide_chains)):\n",
      "                     self._add_PNI(protein_chains[i], nucleotide_chains[j], protein_pgeoms[i], nucleotide_ngeoms[j])\n",
      "                     self._add_PHI(protein_chains[i], nucleotide_chains[j], protein_pgeoms[i], nucleotide_hgeoms[j])\n",
      " \n",
      "--- ./SBI/structure/contacts/InnerContacts.py\t(original)\n",
      "+++ ./SBI/structure/contacts/InnerContacts.py\t(refactored)\n",
      "@@ -40,13 +40,13 @@\n",
      "     def pdb(self):        return self._pdb\n",
      " \n",
      "     @property\n",
      "-    def AAcontacts(self): return self._AAcontacts.values()\n",
      "+    def AAcontacts(self): return list(self._AAcontacts.values())\n",
      " \n",
      "     @property\n",
      "-    def NCcontacts(self): return self._NCcontacts.values()\n",
      "+    def NCcontacts(self): return list(self._NCcontacts.values())\n",
      " \n",
      "     @property\n",
      "-    def HTcontacts(self): return self._HTcontacts.values()\n",
      "+    def HTcontacts(self): return list(self._HTcontacts.values())\n",
      " \n",
      "     #\n",
      "     # PRIVATE FUNCTIONS\n",
      "--- ./SBI/structure/contacts/__init__.py\t(original)\n",
      "+++ ./SBI/structure/contacts/__init__.py\t(refactored)\n",
      "@@ -4,5 +4,5 @@\n",
      " \n",
      " from .inner        import PPInnerContact, PHInnerContact\n",
      " \n",
      "-from Complex       import Complex\n",
      "-from InnerContacts import InnerContacts\n",
      "+from .Complex       import Complex\n",
      "+from .InnerContacts import InnerContacts\n",
      "--- ./SBI/structure/contacts/contact/__init__.py\t(original)\n",
      "+++ ./SBI/structure/contacts/contact/__init__.py\t(refactored)\n",
      "@@ -1,4 +1,4 @@\n",
      "-from Contact        import Contact\n",
      "-from ContactAA      import ContactAA\n",
      "-from ContactAN      import ContactAN\n",
      "-from ContactAH      import ContactAH\n",
      "+from .Contact        import Contact\n",
      "+from .ContactAA      import ContactAA\n",
      "+from .ContactAN      import ContactAN\n",
      "+from .ContactAH      import ContactAH\n",
      "--- ./SBI/structure/contacts/inner/__init__.py\t(original)\n",
      "+++ ./SBI/structure/contacts/inner/__init__.py\t(refactored)\n",
      "@@ -1,2 +1,2 @@\n",
      "-from PPInnerContact import PPInnerContact\n",
      "-from PHInnerContact import PHInnerContact\n",
      "+from .PPInnerContact import PPInnerContact\n",
      "+from .PHInnerContact import PHInnerContact\n",
      "--- ./SBI/structure/contacts/interface/Interface.py\t(original)\n",
      "+++ ./SBI/structure/contacts/interface/Interface.py\t(refactored)\n",
      "@@ -65,7 +65,7 @@\n",
      " \n",
      "     def reverse(self):\n",
      "         (self._chain1, self._chain2) = (self._chain2, self._chain1)\n",
      "-        map(lambda x: x.reverse(), self._contacts)\n",
      "+        list(map(lambda x: x.reverse(), self._contacts))\n",
      " \n",
      "     #\n",
      "     # PRIVATE METHODS\n",
      "--- ./SBI/structure/contacts/interface/__init__.py\t(original)\n",
      "+++ ./SBI/structure/contacts/interface/__init__.py\t(refactored)\n",
      "@@ -1,4 +1,4 @@\n",
      "-from Interface      import Interface\n",
      "-from PPInterface    import PPInterface\n",
      "-from PNInterface    import PNInterface\n",
      "-from PHInterface    import PHInterface\n",
      "+from .Interface      import Interface\n",
      "+from .PPInterface    import PPInterface\n",
      "+from .PNInterface    import PNInterface\n",
      "+from .PHInterface    import PHInterface\n",
      "--- ./SBI/structure/header/DBreference.py\t(original)\n",
      "+++ ./SBI/structure/header/DBreference.py\t(refactored)\n",
      "@@ -48,7 +48,7 @@\n",
      "         @raise AttributeError if line does not start with DBREF\n",
      "         '''\n",
      "         if not pdb_line.startswith('DBREF '):\n",
      "-            SBIglobals.throw(self, '{0} cannot create DBref'.format(pdb_line))\n",
      "+            SBIglobals.throw(self('{0} cannot create DBref'.format(pdb_line)))\n",
      "         data          = self._process_line(pdb_line)\n",
      " \n",
      "         self._pdb     = data[0]\n",
      "@@ -101,8 +101,8 @@\n",
      "         db_minicode = db_minicode.upper()\n",
      "         if not db_minicode in self.valid_references:\n",
      "             line1 = '{0} is not a valid DBref code'.format(db_minicode)\n",
      "-            line2 = 'Available codes: {0}'.format(self.valid_references.keys())\n",
      "-            SBIglobals.throw(self, \"\\n\".join([line1, line2]))\n",
      "+            line2 = 'Available codes: {0}'.format(list(self.valid_references.keys()))\n",
      "+            SBIglobals.throw(self(\"\\n\".join([line1, line2])))\n",
      " \n",
      "         #SWS and TREMBL are parts of UNP\n",
      "         if db_minicode == 'UNP' and self._db in ['SWS', 'TREMBL']:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: No changes to ./SBI/structure/contacts/contact/ContactAN.py\n",
      "RefactoringTool: Refactored ./SBI/structure/contacts/contact/__init__.py\n",
      "RefactoringTool: No changes to ./SBI/structure/contacts/inner/PHInnerContact.py\n",
      "RefactoringTool: No changes to ./SBI/structure/contacts/inner/PPInnerContact.py\n",
      "RefactoringTool: Refactored ./SBI/structure/contacts/inner/__init__.py\n",
      "RefactoringTool: Refactored ./SBI/structure/contacts/interface/Interface.py\n",
      "RefactoringTool: No changes to ./SBI/structure/contacts/interface/PHInterface.py\n",
      "RefactoringTool: No changes to ./SBI/structure/contacts/interface/PNInterface.py\n",
      "RefactoringTool: No changes to ./SBI/structure/contacts/interface/PPInterface.py\n",
      "RefactoringTool: Refactored ./SBI/structure/contacts/interface/__init__.py\n",
      "RefactoringTool: No changes to ./SBI/structure/geometry/RMSD.py\n",
      "RefactoringTool: No changes to ./SBI/structure/geometry/basics.py\n",
      "RefactoringTool: No changes to ./SBI/structure/header/BioMolecule.py\n",
      "RefactoringTool: Refactored ./SBI/structure/header/DBreference.py\n",
      "RefactoringTool: No changes to ./SBI/structure/header/Experiment.py\n",
      "RefactoringTool: Refactored ./SBI/structure/header/Header.py\n",
      "RefactoringTool: No changes to ./SBI/structure/header/HeteroAtom.py\n",
      "RefactoringTool: No changes to ./SBI/structure/header/MiniRes.py\n",
      "RefactoringTool: No changes to ./SBI/structure/header/Molecule.py\n",
      "RefactoringTool: No changes to ./SBI/structure/header/SecondaryStructure.py\n",
      "RefactoringTool: No changes to ./SBI/structure/header/Site.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ./SBI/structure/header/Header.py\t(original)\n",
      "+++ ./SBI/structure/header/Header.py\t(refactored)\n",
      "@@ -130,7 +130,7 @@\n",
      " \n",
      "     @property\n",
      "     def chains(self):\n",
      "-        return self._chaindict.keys()\n",
      "+        return list(self._chaindict.keys())\n",
      " \n",
      "     @property\n",
      "     def sites(self):\n",
      "@@ -170,7 +170,7 @@\n",
      "     @property\n",
      "     def are_molecules_processed(self):\n",
      "         if len(self._molecules) > 0:\n",
      "-            k = self._molecules.keys()\n",
      "+            k = list(self._molecules.keys())\n",
      "             return self._molecules[k[0]].is_processed\n",
      "         else:\n",
      "             return True  # To avoid call the function\n",
      "@@ -319,11 +319,11 @@\n",
      "         if len(self._keywords) == 1 and self._keywords[0] == '':\n",
      "             self._keywords = []\n",
      "         # Assigning sites to chain dictionary\n",
      "-        for k, v in self.sites.iteritems():\n",
      "+        for k, v in self.sites.items():\n",
      "             for c in v.chains:\n",
      "                 if c not in self._chaindict:\n",
      "                     if len(self._molecules) > 0:\n",
      "-                        nm = max([int(x) for x in self._molecules.keys()]) + 1\n",
      "+                        nm = max([int(x) for x in list(self._molecules.keys())]) + 1\n",
      "                     else:\n",
      "                         nm = 1\n",
      "                     self._molecules[nm] = Molecule(self.pdb)\n",
      "@@ -334,11 +334,11 @@\n",
      "                 self._chaindict[c].setdefault('SITES', [])\n",
      "                 self._chaindict[c]['SITES'].append(v)\n",
      "         # Assigning heteroatoms to chain dictionary\n",
      "-        for k, v in self.hetero.iteritems():\n",
      "+        for k, v in self.hetero.items():\n",
      "             for c in v.chain:\n",
      "                 if c not in self._chaindict:\n",
      "                     if len(self._molecules) > 0:\n",
      "-                        nm = max([int(x) for x in self._molecules.keys()]) + 1\n",
      "+                        nm = max([int(x) for x in list(self._molecules.keys())]) + 1\n",
      "                     else:\n",
      "                         nm = 1\n",
      "                     self._molecules[nm] = Molecule(self.pdb)\n",
      "@@ -350,7 +350,7 @@\n",
      "         for ss in self.secondary_structures:\n",
      "             if ss.chain not in self._chaindict:\n",
      "                 if len(self._molecules) > 0:\n",
      "-                        nm = max([int(x) for x in self._molecules.keys()]) + 1\n",
      "+                        nm = max([int(x) for x in list(self._molecules.keys())]) + 1\n",
      "                 else:\n",
      "                     nm = 1\n",
      "                 self._molecules[nm] = Molecule(self.pdb)\n",
      "@@ -358,7 +358,7 @@\n",
      "             self._chaindict[ss.chain].setdefault('SSTRUC', [])\n",
      "             self._chaindict[ss.chain]['SSTRUC'].append(ss)\n",
      "         # Assigning all chains to Symmetry Matrix\n",
      "-        self._symmetryM.chains = self._chaindict.keys()\n",
      "+        self._symmetryM.chains = list(self._chaindict.keys())\n",
      " \n",
      "     #\n",
      "     # FUNCTIONS\n",
      "@@ -377,7 +377,7 @@\n",
      "                            self.pdb,               chain)\n",
      " \n",
      "     def has_hetero(self, heteroID):\n",
      "-        return heteroID in self.hetero.keys()\n",
      "+        return heteroID in list(self.hetero.keys())\n",
      " \n",
      "     def get_hetero_by_name_like(self, hetero_name):\n",
      "         myheteros = []\n",
      "@@ -403,7 +403,7 @@\n",
      "                 'replaces':       self.replaced\n",
      "                 }\n",
      " \n",
      "-        for k, x in self.molecules.iteritems():\n",
      "+        for k, x in self.molecules.items():\n",
      "             data['molecules'].append(x.as_dict())\n",
      " \n",
      "         if self.experiment is not None:\n",
      "@@ -413,10 +413,10 @@\n",
      "             data['dbrefs']  = [x.as_dict() for x in self.dbrefs]\n",
      " \n",
      "         if len(self.sites) > 0:\n",
      "-            data['sites']   = [x.as_dict() for k, x in self.sites.iteritems()]\n",
      "+            data['sites']   = [x.as_dict() for k, x in self.sites.items()]\n",
      " \n",
      "         if len(self.hetero) > 0:\n",
      "-            data['hetatm']  = [x.as_dict() for k, x in self.hetero.iteritems()]\n",
      "+            data['hetatm']  = [x.as_dict() for k, x in self.hetero.items()]\n",
      " \n",
      "         if len(self.secondary_structures) > 0:\n",
      "             data['sstruct'] = [x.as_dict() for x in self.secondary_structures]\n",
      "--- ./SBI/structure/protein/SShelper.py\t(original)\n",
      "+++ ./SBI/structure/protein/SShelper.py\t(refactored)\n",
      "@@ -3,8 +3,8 @@\n",
      " from random             import randint\n",
      " from ..atom             import AtomOfAminoAcid\n",
      " from SBI.external.DSSP  import DSSPExe\n",
      "-from Arch               import Arch\n",
      "-from SecondaryStructure import SecondaryStructure\n",
      "+from .Arch               import Arch\n",
      "+from .SecondaryStructure import SecondaryStructure\n",
      " \n",
      " \n",
      " def calculate_dssp(pdb, tmppdb=None, tmpdssp=None, cleanfiles=True):\n",
      "--- ./SBI/structure/protein/__init__.py\t(original)\n",
      "+++ ./SBI/structure/protein/__init__.py\t(refactored)\n",
      "@@ -1,6 +1,6 @@\n",
      " __all__ = [\"SecondaryStructure\", \"Arch\"]\n",
      " \n",
      "-from SecondaryStructure import SecondaryStructure\n",
      "-from Arch               import Arch\n",
      "-import SShelper\n",
      "-import Sequencer\n",
      "+from .SecondaryStructure import SecondaryStructure\n",
      "+from .Arch               import Arch\n",
      "+from . import SShelper\n",
      "+from . import Sequencer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: No changes to ./SBI/structure/header/__init__.py\n",
      "RefactoringTool: No changes to ./SBI/structure/protein/Arch.py\n",
      "RefactoringTool: Refactored ./SBI/structure/protein/SShelper.py\n",
      "RefactoringTool: No changes to ./SBI/structure/protein/SecondaryStructure.py\n",
      "RefactoringTool: No changes to ./SBI/structure/protein/Sequencer.py\n",
      "RefactoringTool: Refactored ./SBI/structure/protein/__init__.py\n",
      "RefactoringTool: No changes to ./SBI/structure/residue/Residue.py\n",
      "RefactoringTool: No changes to ./SBI/structure/residue/ResidueOfAminoAcid.py\n",
      "RefactoringTool: No changes to ./SBI/structure/residue/ResidueOfNucleotide.py\n",
      "RefactoringTool: Refactored ./SBI/structure/residue/__init__.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ./SBI/structure/residue/__init__.py\t(original)\n",
      "+++ ./SBI/structure/residue/__init__.py\t(refactored)\n",
      "@@ -1,3 +1,3 @@\n",
      "-from Residue             import Residue\n",
      "-from ResidueOfNucleotide import ResidueOfNucleotide\n",
      "-from ResidueOfAminoAcid  import ResidueOfAminoAcid\n",
      "+from .Residue             import Residue\n",
      "+from .ResidueOfNucleotide import ResidueOfNucleotide\n",
      "+from .ResidueOfAminoAcid  import ResidueOfAminoAcid\n",
      "--- ./collision_detection_program/collision_detection.py\t(original)\n",
      "+++ ./collision_detection_program/collision_detection.py\t(refactored)\n",
      "@@ -197,14 +197,14 @@\n",
      "     os.remove(\"fort.15\")\n",
      "     os.remove(gepol_input)\n",
      "     with open(\"fort.7\",\"r\") as myfile:\n",
      "-        for _ in xrange(8):\n",
      "+        for _ in range(8):\n",
      "             next(myfile)\n",
      "         for line in myfile:\n",
      "             atom_data.append(line.strip())\n",
      "         myfile.close()\n",
      "     os.remove(\"fort.7\")\n",
      "     with open(\"fort.8\",\"r\") as myfile:\n",
      "-        for _ in xrange(6):\n",
      "+        for _ in range(6):\n",
      "             next(myfile)\n",
      "         for line in myfile:\n",
      "             normal_data.append(line.strip())\n",
      "@@ -282,7 +282,7 @@\n",
      " \n",
      " def main(argv):\n",
      "     if len(sys.argv)!= 4 :\n",
      "-        print \"Usage : python collision_detection.py A.pdb B.pdb output_path\"\n",
      "+        print(\"Usage : python collision_detection.py A.pdb B.pdb output_path\")\n",
      "         sys.exit(1)\n",
      "     else :\n",
      "         A,B,output_path = argv\n",
      "@@ -298,7 +298,7 @@\n",
      " \n",
      "             Proteins = np.concatenate((Proteins_A,Proteins_B))\n",
      " \n",
      "-            print Proteins\n",
      "+            print(Proteins)\n",
      " \n",
      "             with open(os.path.join(output_path, \"Collision.log\"),\"w\") as f:\n",
      "                 f.write(np.array2string(Proteins, separator=', '))\n",
      "@@ -317,7 +317,7 @@\n",
      "                 f.write(\"The Acessibility Surface Area of Penetrating Secondary A : %f\\n\"%Area_B)\n",
      "             f.close()\n",
      "         else:\n",
      "-            print \"The molecules is not penetrating and its ready for calculating the respective potentials\"\n",
      "+            print(\"The molecules is not penetrating and its ready for calculating the respective potentials\")\n",
      " \n",
      " \n",
      " \n",
      "--- ./collision_detection_program/collision_detection_from_list.py\t(original)\n",
      "+++ ./collision_detection_program/collision_detection_from_list.py\t(refactored)\n",
      "@@ -200,13 +200,13 @@\n",
      "     os.remove(\"fort.15\")\n",
      "     os.remove(gepol_input)\n",
      "     with open(\"fort.7\",\"r\") as myfile:\n",
      "-        for _ in xrange(8):\n",
      "+        for _ in range(8):\n",
      "             next(myfile)\n",
      "         for line in myfile:\n",
      "             atom_data.append(line.strip())\n",
      "     os.remove(\"fort.7\")\n",
      "     with open(\"fort.8\",\"r\") as myfile:\n",
      "-        for _ in xrange(6):\n",
      "+        for _ in range(6):\n",
      "             next(myfile)\n",
      "         for line in myfile:\n",
      "             normal_data.append(line.strip())\n",
      "@@ -289,7 +289,7 @@\n",
      "         for chain_id in chains:\n",
      "             chain = struct.get_chain_by_id(chain_id)\n",
      "     else:\n",
      "-        print('The PDB file {} does not contain one chain!\\n'.format(pdb_file))\n",
      "+        print(('The PDB file {} does not contain one chain!\\n'.format(pdb_file)))\n",
      "         sys.exit(10)\n",
      "     return chain\n",
      " \n",
      "@@ -305,7 +305,7 @@\n",
      " \n",
      " def main(argv):\n",
      "     if len(sys.argv)!= 3 :\n",
      "-        print \"Usage : python collision_detection.py collision_list_file output_path\"\n",
      "+        print(\"Usage : python collision_detection.py collision_list_file output_path\")\n",
      "         sys.exit(1)\n",
      "     else :\n",
      "         collision_list_file, output_path = argv\n",
      "--- ./collision_detection_program/SBI/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/__init__.py\t(refactored)\n",
      "@@ -422,7 +422,7 @@\n",
      "         @pdef:     object\n",
      "         @ptype:    {object}\n",
      "         '''\n",
      "-        if isinstance(source_object, basestring):\n",
      "+        if isinstance(source_object, str):\n",
      "             return '[' + source_object.upper() + ']: '\n",
      "         elif source_object is not None:\n",
      "             return '[' + source_object.__class__.__name__.upper() + ']: '\n",
      "--- ./collision_detection_program/SBI/beans/IndexedNum.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/beans/IndexedNum.py\t(refactored)\n",
      "@@ -129,7 +129,7 @@\n",
      "             return self.number == other\n",
      "         elif isinstance(other, IndexedNum):\n",
      "             return self.number == other.number and self.index == other.index\n",
      "-        elif isinstance(other, basestring):\n",
      "+        elif isinstance(other, str):\n",
      "             return str(self).strip() == other.strip()\n",
      "         return NotImplemented\n",
      " \n",
      "--- ./collision_detection_program/SBI/beans/JSONer.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/beans/JSONer.py\t(refactored)\n",
      "@@ -13,14 +13,13 @@\n",
      " from abc import ABCMeta, abstractmethod\n",
      " \n",
      " \n",
      "-class JSONer(object):\n",
      "+class JSONer(object, metaclass=ABCMeta):\n",
      "     '''\n",
      "     Children from this class have the json() method to be exported in json\n",
      "     format.\n",
      "     Every children must implement the as_dict() method that will be used to\n",
      "     format every attribute in a way that is json-compatible.\n",
      "     '''\n",
      "-    __metaclass__ = ABCMeta\n",
      " \n",
      "     @abstractmethod\n",
      "     def as_dict(self):\n",
      "--- ./collision_detection_program/SBI/beans/StorableObject.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/beans/StorableObject.py\t(refactored)\n",
      "@@ -11,7 +11,7 @@\n",
      " '''\n",
      " from abc import ABCMeta\n",
      " try:\n",
      "-    import cPickle as pickle\n",
      "+    import pickle as pickle\n",
      " except:\n",
      "     import pickle\n",
      " \n",
      "@@ -19,7 +19,7 @@\n",
      " from .. import SBIglobals as SBIg\n",
      " \n",
      " \n",
      "-class StorableObject(object):\n",
      "+class StorableObject(object, metaclass=ABCMeta):\n",
      "     '''\n",
      "     An abstract \"dumping\" class.\n",
      " \n",
      "@@ -29,7 +29,6 @@\n",
      "     recovered afterwards.\n",
      " \n",
      "     '''\n",
      "-    __metaclass__ = ABCMeta\n",
      " \n",
      "     def dump(self, object_file, overwrite = None):\n",
      "         '''\n",
      "--- ./collision_detection_program/SBI/beans/butler.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/beans/butler.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " '''\n",
      "-\\usepackage{hyperref}\n",
      "+\\\\usepackage{hyperref}\n",
      " \n",
      " @file: butler.py\n",
      " \n",
      "@@ -32,7 +32,7 @@\n",
      " from .singleton import Singleton\n",
      " \n",
      " \n",
      "-class Butler(object):\n",
      "+class Butler(object, metaclass=Singleton):\n",
      "     '''\n",
      "     Designed to work through all the \\textbf{SBI library}.\n",
      " \n",
      "@@ -46,7 +46,6 @@\n",
      " \n",
      "     Regarding the looging, it provides functions similar to those of a logger.\n",
      "     '''\n",
      "-    __metaclass__   = Singleton\n",
      " \n",
      "     _LOGNAME        = 'SBILOG'\n",
      "     _GENERAL_FORMAT = '[{0}] ' + '%(asctime)s - %(levelname)-7.7s - %(message)s'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Refactored ./collision_detection_program/collision_detection.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/collision_detection_from_list.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/__init__.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/beans/Executable.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/beans/IndexedNum.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/beans/JSONer.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/beans/Path.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/beans/StorableObject.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/beans/__init__.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/beans/butler.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/beans/file.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/beans/singleton.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/data/__init__.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/databases/DrugBanklink.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/databases/Enzymelink.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/databases/GOlink.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/databases/PDBTMlink.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/databases/PDBeChemlink.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/databases/PDBlink.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/databases/SCOPlink.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/databases/TaxIDlink.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ./collision_detection_program/SBI/data/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/data/__init__.py\t(refactored)\n",
      "@@ -55,7 +55,7 @@\n",
      "     'ACE': 'X', '3FG': 'X', 'UNK': 'X'\n",
      " }\n",
      " \n",
      "-aminoacids1to3 = dict([[v, k] for k, v in aminoacids3to1.items()])\n",
      "+aminoacids1to3 = dict([[v, k] for k, v in list(aminoacids3to1.items())])\n",
      " aminoacids1to3['A'] = 'ALA'\n",
      " aminoacids1to3['N'] = 'ASN'\n",
      " aminoacids1to3['R'] = 'ARG'\n",
      "--- ./collision_detection_program/SBI/databases/DrugBanklink.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/databases/DrugBanklink.py\t(refactored)\n",
      "@@ -10,7 +10,7 @@\n",
      " Import Standard Libraries\n",
      " \"\"\"\n",
      " import os, re\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " \"\"\"\n",
      " Dependences in SBI library\n",
      "@@ -52,8 +52,8 @@\n",
      "             raise NameError('A local drugBank database directory must be defined.')\n",
      " \n",
      "         Path.mkdir(self.local)\n",
      "-        urllib.urlretrieve(drugBankftp['targets'], self._target)\n",
      "-        urllib.urlretrieve(drugBankftp['main'],    self._main)\n",
      "+        urllib.request.urlretrieve(drugBankftp['targets'], self._target)\n",
      "+        urllib.request.urlretrieve(drugBankftp['main'],    self._main)\n",
      " \n",
      "         self._process()\n",
      " \n",
      "@@ -77,11 +77,11 @@\n",
      "         drugFile.close()\n",
      " \n",
      "     def _process_targets(self):\n",
      "-        import csv, StringIO, zipfile\n",
      "+        import csv, io, zipfile\n",
      " \n",
      "         targets = {}\n",
      "         z       = zipfile.ZipFile(self._target, 'r')\n",
      "-        data    = StringIO.StringIO(z.read(z.namelist()[0]))\n",
      "+        data    = io.StringIO(z.read(z.namelist()[0]))\n",
      "         reader  = csv.reader(data)\n",
      "         for row in reader:\n",
      "             if row[5] != '':\n",
      "--- ./collision_detection_program/SBI/databases/Enzymelink.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/databases/Enzymelink.py\t(refactored)\n",
      "@@ -10,7 +10,7 @@\n",
      " Import Standard Libraries\n",
      " \"\"\"\n",
      " import os, re\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " \"\"\"\n",
      " Dependences in SBI library\n",
      "@@ -62,8 +62,8 @@\n",
      "             raise NameError('A local Enzyme database directory must be defined.')\n",
      " \n",
      "         Path.mkdir(self.local)\n",
      "-        urllib.urlretrieve(Enzymeftp['dat'], self._dfile)\n",
      "-        urllib.urlretrieve(Enzymeftp['cls'], self._cfile)\n",
      "+        urllib.request.urlretrieve(Enzymeftp['dat'], self._dfile)\n",
      "+        urllib.request.urlretrieve(Enzymeftp['cls'], self._cfile)\n",
      " \n",
      "         self._process()\n",
      " \n",
      "--- ./collision_detection_program/SBI/databases/GOlink.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/databases/GOlink.py\t(refactored)\n",
      "@@ -10,7 +10,7 @@\n",
      " Import Standard Libraries\n",
      " \"\"\"\n",
      " import os, re\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " \"\"\"\n",
      " Dependences in SBI library\n",
      "@@ -61,7 +61,7 @@\n",
      "         Path.mkdir(self.local)\n",
      "         destination = os.path.join(self.local, self._gfile)\n",
      " \n",
      "-        urllib.urlretrieve(GOftp['source'], destination)\n",
      "+        urllib.request.urlretrieve(GOftp['source'], destination)\n",
      "         self._process()\n",
      " \n",
      "         return True\n",
      "--- ./collision_detection_program/SBI/databases/PDBTMlink.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/databases/PDBTMlink.py\t(refactored)\n",
      "@@ -10,7 +10,7 @@\n",
      " Import Standard Libraries\n",
      " \"\"\"\n",
      " import os, re\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " \"\"\"\n",
      " Dependences in SBI library\n",
      "--- ./collision_detection_program/SBI/databases/PDBeChemlink.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/databases/PDBeChemlink.py\t(refactored)\n",
      "@@ -14,7 +14,7 @@\n",
      " import re\n",
      " import subprocess\n",
      " import warnings\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " \"\"\"\n",
      " Dependences in SBI library\n",
      "@@ -64,7 +64,7 @@\n",
      "         Path.mkdir(self.local)\n",
      "         destination = os.path.join(self.local, 'mmcif.tar.gz')\n",
      "         try:\n",
      "-            urllib.urlretrieve(PDBeChemftp['global'], destination)\n",
      "+            urllib.request.urlretrieve(PDBeChemftp['global'], destination)\n",
      "         except:\n",
      "             return False\n",
      "         command = ['tar', 'zxvf', destination, '-C', self.local]\n",
      "@@ -84,7 +84,7 @@\n",
      "         chem_file = chemID.upper() + '.cif'\n",
      "         source = PDBeChemftp['single'] + chem_file\n",
      "         try:\n",
      "-            urllib.urlretrieve(source, chem_file)\n",
      "+            urllib.request.urlretrieve(source, chem_file)\n",
      "         except:\n",
      "             return False\n",
      "         return os.path.abspath(chem_file)\n",
      "--- ./collision_detection_program/SBI/databases/PDBlink.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/databases/PDBlink.py\t(refactored)\n",
      "@@ -12,7 +12,7 @@\n",
      " import sys, os, copy\n",
      " import subprocess\n",
      " import warnings\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " import ftplib\n",
      " \n",
      " \"\"\"\n",
      "@@ -64,7 +64,7 @@\n",
      "     \"\"\"METHODS\"\"\"\n",
      "     def get_PDBseq_filtered(self, resolution_threshold, output_file):\n",
      "         resolutions     = self.get_resolutions()\n",
      "-        names           = [k for k, v in resolutions.iteritems() if float(v) <= float(resolution_threshold)]\n",
      "+        names           = [k for k, v in resolutions.items() if float(v) <= float(resolution_threshold)]\n",
      "         sequences       = Fasta(os.path.join(self.PDBseq,'PDBseq.fa'))\n",
      "         selectedseq     = sequences.retrieve(copy.deepcopy(names), prefix_size = 4)\n",
      "         return Fasta.build_multifasta(output_file, selectedseq, True)\n",
      "@@ -182,7 +182,7 @@\n",
      "         pdb_file = 'pdb' + pdbID.lower() + '.ent.gz'\n",
      "         source = 'ftp://' + PDBftp['address'] + os.path.join(PDBftp['structures'], pdbID[1:3].lower(), pdb_file)\n",
      "         try:\n",
      "-            urllib.urlretrieve(source, pdb_file)\n",
      "+            urllib.request.urlretrieve(source, pdb_file)\n",
      "         except:\n",
      "             return False\n",
      "         return os.path.abspath(pdb_file)\n",
      "--- ./collision_detection_program/SBI/databases/SCOPlink.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/databases/SCOPlink.py\t(refactored)\n",
      "@@ -10,7 +10,7 @@\n",
      " Import Standard Libraries\n",
      " \"\"\"\n",
      " import os, re\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " \"\"\"\n",
      " Dependences in SBI library\n",
      "@@ -64,7 +64,7 @@\n",
      "             raise NameError('A local SCOP database directory must be defined.')\n",
      " \n",
      "         Path.mkdir(self.local)\n",
      "-        urllib.urlretrieve(SCOPftp['desc'], self._desc)\n",
      "-        urllib.urlretrieve(SCOPftp['rel'],  self._rel)\n",
      "+        urllib.request.urlretrieve(SCOPftp['desc'], self._desc)\n",
      "+        urllib.request.urlretrieve(SCOPftp['rel'],  self._rel)\n",
      " \n",
      "         return True\n",
      "--- ./collision_detection_program/SBI/databases/TaxIDlink.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/databases/TaxIDlink.py\t(refactored)\n",
      "@@ -12,7 +12,7 @@\n",
      " import sys, os, re\n",
      " import subprocess\n",
      " import warnings\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " \"\"\"\n",
      " Dependences in SBI library\n",
      "@@ -73,7 +73,7 @@\n",
      " \n",
      "         Path.mkdir(self.local)\n",
      "         destination = os.path.join(self.local, 'taxdmp.zip')\n",
      "-        urllib.urlretrieve(taxIDftp['global'], destination)\n",
      "+        urllib.request.urlretrieve(taxIDftp['global'], destination)\n",
      "         command = ['unzip', '-o', destination, '-d', self.local]\n",
      "         p = subprocess.Popen(command, stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
      "         out, err = p.communicate()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Refactored ./collision_detection_program/SBI/databases/Uniprotlink.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/databases/__Uniprotlink.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/databases/__init__.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/databases/dblink.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/databases/uniprot/__init__.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/databases/uniprot/connect.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/databases/uniprot/uniprot.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/external/ExternalExe.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/external/__init__.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/external/CDhit/CDhit.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/external/CDhit/CDhitExe.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/external/CDhit/CDhitHomolog.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/external/CDhit/CDhitList.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/external/CDhit/__init__.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/external/DSSP/DSSP.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/external/DSSP/DSSPExe.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/external/DSSP/__init__.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/external/blast/BlastExe.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/external/blast/BlastHit.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ./collision_detection_program/SBI/databases/Uniprotlink.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/databases/Uniprotlink.py\t(refactored)\n",
      "@@ -12,7 +12,7 @@\n",
      " import sys, os, re\n",
      " import subprocess\n",
      " import warnings\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " \"\"\"\n",
      " Dependences in SBI library\n",
      "@@ -84,9 +84,9 @@\n",
      " \n",
      "         Path.mkdir(self.local)\n",
      "         destination = os.path.join(self.local, 'uniprot_sprot.dat.gz')\n",
      "-        urllib.urlretrieve(Uniprotftp['swissprot'], destination)\n",
      "+        urllib.request.urlretrieve(Uniprotftp['swissprot'], destination)\n",
      "         destination = os.path.join(self.local, 'uniprot_trembl.dat.gz')\n",
      "-        urllib.urlretrieve(Uniprotftp['trembl'], destination)\n",
      "+        urllib.request.urlretrieve(Uniprotftp['trembl'], destination)\n",
      " \n",
      "         self._process()\n",
      " \n",
      "@@ -169,7 +169,7 @@\n",
      "         if self._ox is None:\n",
      "             self._ox = self._extract_taxid(value)\n",
      "         else:\n",
      "-            raise('More than one taxid! {0}'.format(value))\n",
      "+            raise 'More than one taxid! {0}'\n",
      " \n",
      "     @property\n",
      "     def hosts(self): return self._oc\n",
      "--- ./collision_detection_program/SBI/databases/__Uniprotlink.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/databases/__Uniprotlink.py\t(refactored)\n",
      "@@ -12,7 +12,7 @@\n",
      " import sys, os, re\n",
      " import subprocess\n",
      " import warnings\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " \"\"\"\n",
      " Dependences in SBI library\n",
      "@@ -84,9 +84,9 @@\n",
      " \n",
      "         Path.mkdir(self.local)\n",
      "         destination = os.path.join(self.local, 'uniprot_sprot.dat.gz')\n",
      "-        urllib.urlretrieve(Uniprotftp['swissprot'], destination)\n",
      "+        urllib.request.urlretrieve(Uniprotftp['swissprot'], destination)\n",
      "         destination = os.path.join(self.local, 'uniprot_trembl.dat.gz')\n",
      "-        urllib.urlretrieve(Uniprotftp['trembl'], destination)\n",
      "+        urllib.request.urlretrieve(Uniprotftp['trembl'], destination)\n",
      " \n",
      "         self._process()\n",
      " \n",
      "@@ -169,7 +169,7 @@\n",
      "         if self._ox is None:\n",
      "             self._ox = self._extract_taxid(value)\n",
      "         else:\n",
      "-            raise('More than one taxid! {0}'.format(value))\n",
      "+            raise 'More than one taxid! {0}'\n",
      " \n",
      "     @property\n",
      "     def hosts(self): return self._oc\n",
      "--- ./collision_detection_program/SBI/databases/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/databases/__init__.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " __all__ = [\"DBlink\"]\n",
      "-from dblink import DBlink\n",
      "+from .dblink import DBlink\n",
      " \n",
      " \"\"\"\n",
      " PDB:\n",
      "@@ -70,13 +70,13 @@\n",
      " \"\"\"\n",
      " INCLUDES\n",
      " \"\"\"\n",
      "-from PDBlink      import PDBlink\n",
      "-from PDBeChemlink import PDBeChemlink, PDBeChem\n",
      "-from GOlink       import GOlink, GOterm\n",
      "-from TaxIDlink    import TaxIDlink, TaxID\n",
      "-from Uniprotlink  import Uniprotlink, Uniprot\n",
      "-from Enzymelink   import Enzymelink, Enzyme\n",
      "-from DrugBanklink import DrugBanklink, Drug\n",
      "-from SCOPlink     import SCOPlink\n",
      "-from PDBTMlink    import PDBTMlink, TM\n",
      "+from .PDBlink      import PDBlink\n",
      "+from .PDBeChemlink import PDBeChemlink, PDBeChem\n",
      "+from .GOlink       import GOlink, GOterm\n",
      "+from .TaxIDlink    import TaxIDlink, TaxID\n",
      "+from .Uniprotlink  import Uniprotlink, Uniprot\n",
      "+from .Enzymelink   import Enzymelink, Enzyme\n",
      "+from .DrugBanklink import DrugBanklink, Drug\n",
      "+from .SCOPlink     import SCOPlink\n",
      "+from .PDBTMlink    import PDBTMlink, TM\n",
      " \n",
      "--- ./collision_detection_program/SBI/databases/dblink.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/databases/dblink.py\t(refactored)\n",
      "@@ -12,7 +12,7 @@\n",
      " from abc import ABCMeta, abstractmethod\n",
      " from datetime import date\n",
      " # import time\n",
      "-import urllib\n",
      "+import urllib.request, urllib.parse, urllib.error\n",
      " \n",
      " import os\n",
      " import json\n",
      "@@ -22,12 +22,11 @@\n",
      " from SBI       import SBIglobals as SBIg\n",
      " \n",
      " \n",
      "-class DBlink(object):\n",
      "+class DBlink(object, metaclass=ABCMeta):\n",
      "     '''\n",
      "     Manages the connection to databases.\n",
      "     It creates a local copy of the DB that can be queried a posteriori.\n",
      "     '''\n",
      "-    __metaclass__ = ABCMeta\n",
      " \n",
      "     _MANDATORY_FILES = []\n",
      "     _ITEM_FILES      = []\n",
      "@@ -91,7 +90,7 @@\n",
      "         @yields: Object depending on the database.\n",
      "         '''\n",
      "         if not self.has_local:\n",
      "-            SBIg.throw(self, 'A local database needs to be build first', IOError)\n",
      "+            SBIg.throw(self('A local database needs to be build first').with_traceback(IOError))\n",
      " \n",
      "         for ifile in self._ITEM_FILES:\n",
      "             ifile = os.path.join(self.local, ifile)\n",
      "@@ -207,7 +206,7 @@\n",
      "             if download:\n",
      "                 SBIg.alert('verbose', self, 'Downloading {0} to {1}'.format(source, destination))\n",
      "                 SBIg.alert('verbose', self, 'Source file size is {0:.3f} MB.'.format(source_size))\n",
      "-                urllib.urlretrieve(source, destination)\n",
      "+                urllib.request.urlretrieve(source, destination)\n",
      " \n",
      "     def _clean_sources(self):\n",
      "         '''\n",
      "@@ -229,7 +228,7 @@\n",
      " \n",
      "         @return: {Float}\n",
      "         '''\n",
      "-        usock = urllib.urlopen(external_file)\n",
      "+        usock = urllib.request.urlopen(external_file)\n",
      "         size  = usock.info().get('Content-Length')\n",
      "         if size is None:\n",
      "             return 0\n",
      "--- ./collision_detection_program/SBI/databases/uniprot/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/databases/uniprot/__init__.py\t(refactored)\n",
      "@@ -1,4 +1,4 @@\n",
      "-from connect import Connect\n",
      "-from uniprot import Uniprot\n",
      "+from .connect import Connect\n",
      "+from .uniprot import Uniprot\n",
      " \n",
      " __all__ = [\"Connect\", \"Uniprot\"]\n",
      "--- ./collision_detection_program/SBI/databases/uniprot/connect.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/databases/uniprot/connect.py\t(refactored)\n",
      "@@ -2,7 +2,7 @@\n",
      " import re\n",
      " \n",
      " from SBI.databases import DBlink\n",
      "-from uniprot       import Uniprot\n",
      "+from .uniprot       import Uniprot\n",
      " from SBI.beans     import File\n",
      " from SBI           import SBIglobals as SBIg\n",
      " \n",
      "--- ./collision_detection_program/SBI/databases/uniprot/uniprot.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/databases/uniprot/uniprot.py\t(refactored)\n",
      "@@ -272,7 +272,7 @@\n",
      " \n",
      "         @return: {Uniprot}\n",
      "         '''\n",
      "-        if isinstance(json_line, basestring):\n",
      "+        if isinstance(json_line, str):\n",
      "             json_line = json.loads(json_line.strip())\n",
      " \n",
      "         data = Uniprot('', '')\n",
      "--- ./collision_detection_program/SBI/external/ExternalExe.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/external/ExternalExe.py\t(refactored)\n",
      "@@ -10,7 +10,7 @@\n",
      " @class: ExternalExe\n",
      " '''\n",
      " import os\n",
      "-import ConfigParser\n",
      "+import configparser\n",
      " from os.path import join     as j\n",
      " from os.path import normpath as n\n",
      " from os.path import dirname  as d\n",
      "@@ -19,7 +19,7 @@\n",
      " from SBI.beans import Executable\n",
      " \n",
      " \n",
      "-class ExternalExe(object):\n",
      "+class ExternalExe(object, metaclass=ABCMeta):\n",
      "     '''\n",
      "     Main class to derive others that will control the execution of external\n",
      "     programs.\n",
      "@@ -28,12 +28,11 @@\n",
      "     defined in a file linked to a environment variable called SBI_CONFIG_FILE.\n",
      " \n",
      "     '''\n",
      "-    __metaclass__ = ABCMeta\n",
      " \n",
      "     DEFAULT_CONFIG_FILE = j(n(d(__file__)), 'configSBI.txt')\n",
      " \n",
      "     # Executable configuration\n",
      "-    _CONFIG     = ConfigParser.RawConfigParser(allow_no_value=True)\n",
      "+    _CONFIG     = configparser.RawConfigParser(allow_no_value=True)\n",
      "     _EXE        = None\n",
      " \n",
      "     def __new__(cls, *args, **kwargs):\n",
      "--- ./collision_detection_program/SBI/external/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/external/__init__.py\t(refactored)\n",
      "@@ -1,2 +1,2 @@\n",
      " __all__ = [\"ExternalExe\"]\n",
      "-from ExternalExe import ExternalExe\n",
      "+from .ExternalExe import ExternalExe\n",
      "--- ./collision_detection_program/SBI/external/CDhit/CDhitExe.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/external/CDhit/CDhitExe.py\t(refactored)\n",
      "@@ -126,9 +126,8 @@\n",
      "         try:\n",
      "             e.execute()\n",
      "             e.clean_command()\n",
      "-        except SystemError, er:\n",
      "-            SBIg.throw(self, 'Some error occurred while executing cd-hit\\n{0}\\n'.format(er),\n",
      "-                       SystemError)\n",
      "+        except SystemError as er:\n",
      "+            SBIg.throw(self('Some error occurred while executing cd-hit\\n{0}\\n'.format(er)).with_traceback(SystemError))\n",
      " \n",
      "     def make_master_fasta(self, output_fasta = None, force = None):\n",
      "         '''\n",
      "@@ -250,6 +249,5 @@\n",
      "         try:\n",
      "             self._EXE.execute()\n",
      "             self._EXE.clean_command()\n",
      "-        except SystemError, e:\n",
      "-            SBIg.throw(self, 'Some error occurred while executing cd-hit\\n{0}\\n'.format(e),\n",
      "-                       SystemError)\n",
      "+        except SystemError as e:\n",
      "+            SBIg.throw(self('Some error occurred while executing cd-hit\\n{0}\\n'.format(e)).with_traceback(SystemError))\n",
      "--- ./collision_detection_program/SBI/external/CDhit/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/external/CDhit/__init__.py\t(refactored)\n",
      "@@ -1,4 +1,4 @@\n",
      " __all__ = [\"CDhitList\", \"CDhitExe\"]\n",
      " \n",
      "-from CDhitExe  import CDhitExe\n",
      "-from CDhitList import CDhitList\n",
      "+from .CDhitExe  import CDhitExe\n",
      "+from .CDhitList import CDhitList\n",
      "--- ./collision_detection_program/SBI/external/DSSP/DSSPExe.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/external/DSSP/DSSPExe.py\t(refactored)\n",
      "@@ -13,7 +13,7 @@\n",
      " import sys\n",
      " \n",
      " from SBI.external import ExternalExe\n",
      "-from DSSP         import DSSP\n",
      "+from .DSSP         import DSSP\n",
      " from SBI          import SBIglobals as SBIg\n",
      " from SBI.beans    import File\n",
      " \n",
      "@@ -121,9 +121,9 @@\n",
      "         self._EXE.add_parameter(self._dsspfile)\n",
      "         try:\n",
      "             self._EXE.execute(silent=True)\n",
      "-        except SystemError, e:\n",
      "+        except SystemError as e:\n",
      "             msg = 'Some error occurred while executing dssp\\n{0}\\n'.format(e)\n",
      "-            SBIg.throw(self, msg, e)\n",
      "+            SBIg.throw(self(msg).with_traceback(e))\n",
      " \n",
      "         self._EXE.clean_command()\n",
      " \n",
      "--- ./collision_detection_program/SBI/external/DSSP/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/external/DSSP/__init__.py\t(refactored)\n",
      "@@ -1,4 +1,4 @@\n",
      " __all__ = [\"DSSPExe\", \"DSSP\"]\n",
      " \n",
      "-from DSSPExe import DSSPExe\n",
      "-from DSSP    import DSSP\n",
      "+from .DSSPExe import DSSPExe\n",
      "+from .DSSP    import DSSP\n",
      "--- ./collision_detection_program/SBI/external/blast/BlastExe.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/external/blast/BlastExe.py\t(refactored)\n",
      "@@ -14,7 +14,7 @@\n",
      " import os\n",
      " import time\n",
      " import re\n",
      "-import ConfigParser\n",
      "+import configparser\n",
      " from abc import ABCMeta\n",
      " \n",
      " from bs4 import BeautifulSoup\n",
      "@@ -25,9 +25,9 @@\n",
      " from SBI.beans    import Path\n",
      " from SBI.sequence import Fasta\n",
      " from SBI          import SBIglobals as SBIg\n",
      "-from BlastResult  import BlastResult\n",
      "-from BlastResult  import BlastHeader\n",
      "-from BlastHit     import BlastHit\n",
      "+from .BlastResult  import BlastResult\n",
      "+from .BlastResult  import BlastHeader\n",
      "+from .BlastHit     import BlastHit\n",
      " \n",
      " \n",
      " class BlastExe(ExternalExe):\n",
      "@@ -321,7 +321,7 @@\n",
      " \n",
      "         @returns: {BlastResult}\n",
      "         '''\n",
      "-        if isinstance(query_file, basestring) or isinstance(query_file, File):\n",
      "+        if isinstance(query_file, str) or isinstance(query_file, File):\n",
      "             newFasta = Fasta(fasta_file = query_file)\n",
      "         elif isinstance(query_file, Fasta):\n",
      "             newFasta = query_file\n",
      "@@ -458,7 +458,7 @@\n",
      "         if len(formatdb_files) > 0:\n",
      "             try:\n",
      "                 self._format_database(database)\n",
      "-            except ConfigParser.NoOptionError as e:\n",
      "+            except configparser.NoOptionError as e:\n",
      "                 raise self._error.no_blast_format_exe(e)\n",
      "             except SystemError as e:\n",
      "                 raise self._error.wrong_db_format(database, e)\n",
      "@@ -520,11 +520,10 @@\n",
      "         return search_type\n",
      " \n",
      " \n",
      "-class BlastParser(object):\n",
      "+class BlastParser(object, metaclass=ABCMeta):\n",
      "     '''\n",
      "     Processes a blast xml formated output into a {BlastResult} object.\n",
      "     '''\n",
      "-    __metaclass__ = ABCMeta\n",
      " \n",
      "     @staticmethod\n",
      "     def parse(query_sequence, blast_output_file, self_hit, hitid_format):\n",
      "--- ./collision_detection_program/SBI/external/blast/BlastHit.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/external/blast/BlastHit.py\t(refactored)\n",
      "@@ -275,7 +275,7 @@\n",
      "         complex_indexes = {}\n",
      "         for x in range(self.number_of_sequences):\n",
      "             seqInits[x] = self._sequence_position_from_alignment_position(x, index[0])\n",
      "-            complex_indexes[x] = list(filter(None, self._seq2ali[x]))\n",
      "+            complex_indexes[x] = list([_f for _f in self._seq2ali[x] if _f])\n",
      "             complex_indexes[x] = complex_indexes[x][complex_indexes[x].index(seqInits[x]):]\n",
      "         patt   = self._alipatt[index[0] - 1:index[1] - 1:index[2]]\n",
      "         newali = self.__class__(hit = [self._sequenceID, self._length],\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Refactored ./collision_detection_program/SBI/external/blast/BlastResult.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/external/blast/__init__.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/external/hmmer/HmmExe.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/external/hmmer/HmmHit.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/external/hmmer/HmmResult.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/external/hmmer/__init__.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/math/__init__.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/math/stats.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/sequence/Fasta.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/sequence/Sequence.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/sequence/__init__.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/sequence/alignment/Needleman_Wunsch.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/sequence/alignment/SeqAli.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/sequence/alignment/SimilarityMatrix.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/sequence/alignment/__init__.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ./collision_detection_program/SBI/external/blast/BlastResult.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/external/blast/BlastResult.py\t(refactored)\n",
      "@@ -14,7 +14,7 @@\n",
      " import os\n",
      " import copy\n",
      " \n",
      "-import BlastExe\n",
      "+from . import BlastExe\n",
      " from SBI.beans              import StorableObject\n",
      " from SBI.beans              import File\n",
      " from SBI.sequence           import Sequence\n",
      "@@ -654,7 +654,7 @@\n",
      "             output.write(\"%s\\n\" % self.str_compacted_blast())\n",
      "             output.close()\n",
      "         else:\n",
      "-            print self.str_compacted_blast()\n",
      "+            print(self.str_compacted_blast())\n",
      " \n",
      "     def print_representation(self, line_split = 160, out_file = None):\n",
      "         '''\n",
      "@@ -675,7 +675,7 @@\n",
      "             output.write(\"%s\\n\" % self.str_representation(line_split))\n",
      "             output.close()\n",
      "         else:\n",
      "-            print self.str_representation(line_split)\n",
      "+            print(self.str_representation(line_split))\n",
      " \n",
      "     @staticmethod\n",
      "     def read_compacted_blast(compacted_blast_file):\n",
      "@@ -689,7 +689,7 @@\n",
      " \n",
      "         @return: {BlastResult}\n",
      "         '''\n",
      "-        from BlastHit import BlastHit\n",
      "+        from .BlastHit import BlastHit\n",
      "         query_name, query_sequence     = None, None\n",
      "         version, matrix, database      = None, None, None\n",
      "         gap_open, gap_extend, self_hit = None, None, None\n",
      "@@ -753,9 +753,9 @@\n",
      "         '''\n",
      "         final_hits = []\n",
      "         final_hits.append(hitlist[0])\n",
      "-        for x in xrange(1, len(hitlist)):\n",
      "+        for x in range(1, len(hitlist)):\n",
      "             accepted = True\n",
      "-            for y in xrange(0, len(final_hits)):\n",
      "+            for y in range(0, len(final_hits)):\n",
      "                 if hitlist[x].overlap(final_hits[y]) > overlap:\n",
      "                     accepted = False\n",
      "                     break\n",
      "@@ -821,7 +821,7 @@\n",
      "         self.gap_open   = int(gap_open)\n",
      "         self.gap_extend = int(gap_extend)\n",
      "         self.database   = database\n",
      "-        if isinstance(self_hit, basestring):\n",
      "+        if isinstance(self_hit, str):\n",
      "             if self_hit.upper() == 'TRUE':\n",
      "                 self.self_hit = True\n",
      "             else:\n",
      "--- ./collision_detection_program/SBI/external/blast/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/external/blast/__init__.py\t(refactored)\n",
      "@@ -1,4 +1,4 @@\n",
      " __all__ = [\"BlastResult\", \"BlastHit\", \"BlastExe\", \"BlastError\"]\n",
      "-from BlastExe    import BlastExe, BlastError\n",
      "-from BlastHit    import BlastHit\n",
      "-from BlastResult import BlastResult\n",
      "+from .BlastExe    import BlastExe, BlastError\n",
      "+from .BlastHit    import BlastHit\n",
      "+from .BlastResult import BlastResult\n",
      "--- ./collision_detection_program/SBI/external/hmmer/HmmExe.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/external/hmmer/HmmExe.py\t(refactored)\n",
      "@@ -15,15 +15,15 @@\n",
      " import re\n",
      " import time\n",
      " import subprocess\n",
      "-import ConfigParser\n",
      "+import configparser\n",
      " from abc import ABCMeta\n",
      " \n",
      " from SBI.external import ExternalExe\n",
      " from SBI.beans    import Executable\n",
      " from SBI.beans    import File\n",
      " from SBI.sequence import Fasta\n",
      "-from HmmResult    import HmmResult\n",
      "-from HmmHit       import HmmHit\n",
      "+from .HmmResult    import HmmResult\n",
      "+from .HmmHit       import HmmHit\n",
      " from SBI          import SBIglobals as SBIg\n",
      " \n",
      " \n",
      "@@ -263,7 +263,7 @@\n",
      " \n",
      "         @returns: {HmmResult}\n",
      "         '''\n",
      "-        if isinstance(query_file, basestring) or isinstance(query_file, File):\n",
      "+        if isinstance(query_file, str) or isinstance(query_file, File):\n",
      "             newFasta = Fasta(fasta_file = query_file)\n",
      "         elif isinstance(query_file, Fasta):\n",
      "             newFasta = query_file\n",
      "@@ -368,7 +368,7 @@\n",
      "         if len(formatdb_files) > 0:\n",
      "             try:\n",
      "                 self._format_database(database)\n",
      "-            except ConfigParser.NoOptionError as e:\n",
      "+            except configparser.NoOptionError as e:\n",
      "                 raise self._error.no_hmmer_format_exe(e)\n",
      "             except SystemError as e:\n",
      "                 raise self._error.wrong_db_format(database, e)\n",
      "@@ -430,11 +430,10 @@\n",
      "                 os.unlink(temp_file)\n",
      " \n",
      " \n",
      "-class HmmParser(object):\n",
      "+class HmmParser(object, metaclass=ABCMeta):\n",
      "     '''\n",
      "     Processes a cd-hit output into a {HmmResult} object.\n",
      "     '''\n",
      "-    __metaclass__ = ABCMeta\n",
      " \n",
      "     @staticmethod\n",
      "     def parse(query_name, query_sequence, database, hmmer_output_file):\n",
      "--- ./collision_detection_program/SBI/external/hmmer/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/external/hmmer/__init__.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " __all__ = [\"HmmExe\", \"HmmResult\", \"HmmHit\"]\n",
      " \n",
      "-from HmmExe    import HmmExe\n",
      "-from HmmResult import HmmResult\n",
      "-from HmmHit    import HmmHit\n",
      "+from .HmmExe    import HmmExe\n",
      "+from .HmmResult import HmmResult\n",
      "+from .HmmHit    import HmmHit\n",
      "--- ./collision_detection_program/SBI/math/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/math/__init__.py\t(refactored)\n",
      "@@ -1 +1 @@\n",
      "-import stats\n",
      "+from . import stats\n",
      "--- ./collision_detection_program/SBI/sequence/Fasta.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/sequence/Fasta.py\t(refactored)\n",
      "@@ -35,7 +35,7 @@\n",
      "         @pdefault: 10\n",
      "         @ptype:    {Integer}\n",
      "         '''\n",
      "-        if isinstance(fasta_file, basestring):\n",
      "+        if isinstance(fasta_file, str):\n",
      "             self._file = File(file_name = fasta_file, action = 'r')\n",
      "         elif isinstance(fasta_file, File):\n",
      "             self._file = File(file_name = fasta_file.full, action = 'r')\n",
      "@@ -93,7 +93,7 @@\n",
      "         @return: {List}\n",
      "         '''\n",
      "         if self.is_loaded:\n",
      "-            return self._sequenceID.keys()\n",
      "+            return list(self._sequenceID.keys())\n",
      "         else:\n",
      "             keys = []\n",
      "             for sequence in self.live_show():\n",
      "@@ -251,7 +251,7 @@\n",
      "         else:\n",
      "             SBIg.alert('debug', self, [info.format(x) for x in sequence_ids])\n",
      " \n",
      "-        if isinstance(sequence_ids, basestring):\n",
      "+        if isinstance(sequence_ids, str):\n",
      "             sequence_ids = set([sequence_ids])\n",
      "         if isinstance(sequence_ids, list):\n",
      "             sequence_ids = set(sequence_ids)\n",
      "--- ./collision_detection_program/SBI/sequence/Sequence.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/sequence/Sequence.py\t(refactored)\n",
      "@@ -184,7 +184,7 @@\n",
      "         @raises: {AttributeError} if sequence is the wrong type.\n",
      "         @return: {Boolean}\n",
      "         '''\n",
      "-        if isinstance(sequence, basestring):\n",
      "+        if isinstance(sequence, str):\n",
      "             return bool(re.search(sequence, self.sequence))\n",
      "         elif isinstance(sequence, list):\n",
      "             return bool(re.search(''.join(sequence), self.sequence))\n",
      "@@ -204,7 +204,7 @@\n",
      "         @raises: {AttributeError} if sequence is the wrong type.\n",
      "         @return: {Boolean}\n",
      "         '''\n",
      "-        if isinstance(sequence, basestring):\n",
      "+        if isinstance(sequence, str):\n",
      "             return bool(re.search(self.sequence, sequence))\n",
      "         elif isinstance(sequence, list):\n",
      "             return bool(re.search(self.sequence, ''.join(sequence)))\n",
      "@@ -224,7 +224,7 @@\n",
      "         @raises: {AttributeError} if sequence is the wrong type.\n",
      "         @return: {List} of {String}\n",
      "         '''\n",
      "-        if isinstance(sequence, basestring):\n",
      "+        if isinstance(sequence, str):\n",
      "             pass\n",
      "         elif isinstance(sequence, list):\n",
      "             sequence = ''.join(sequence)\n",
      "@@ -275,7 +275,7 @@\n",
      "         '''\n",
      "         if algorithm.upper() not in Sequence.AVAILABLE_ALIGN_ALGORITHMS:\n",
      "             raise AttributeError('Alignment algorithm not available')\n",
      "-        if isinstance(sequence, basestring):\n",
      "+        if isinstance(sequence, str):\n",
      "             pass\n",
      "         elif isinstance(sequence, list):\n",
      "             sequence = ''.join(sequence)\n",
      "@@ -285,7 +285,7 @@\n",
      "             return AttributeError('sequence must be string, list or Sequence')\n",
      " \n",
      "         if algorithm.upper() in Sequence.NEEDLEMAN_WUNSCH:\n",
      "-            from alignment.Needleman_Wunsch import needleman_wunsch as nw\n",
      "+            from .alignment.Needleman_Wunsch import needleman_wunsch as nw\n",
      "             return nw(self.sequence, sequence, similarity_matrix,\n",
      "                       gap_init, gap_penalty)\n",
      " \n",
      "@@ -337,7 +337,7 @@\n",
      "         '''\n",
      "         c = Counter(re.sub(Sequence.GAP_DEFINITION, '', self._sequence))\n",
      "         l = float(len(self))\n",
      "-        return dict([(x, y/l) for x, y in c.iteritems()])\n",
      "+        return dict([(x, y/l) for x, y in c.items()])\n",
      " \n",
      "     def duplicate(self, new_id = None):\n",
      "         '''\n",
      "@@ -380,7 +380,7 @@\n",
      " \n",
      "         @raises: {AttributeError} if sequence is the wrong type.\n",
      "         '''\n",
      "-        if isinstance(sequence, basestring):\n",
      "+        if isinstance(sequence, str):\n",
      "             pass\n",
      "         elif isinstance(sequence, list):\n",
      "             sequence = ''.join(sequence)\n",
      "--- ./collision_detection_program/SBI/sequence/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/sequence/__init__.py\t(refactored)\n",
      "@@ -1,4 +1,4 @@\n",
      "-from Sequence import Sequence\n",
      "-from Fasta    import Fasta\n",
      "+from .Sequence import Sequence\n",
      "+from .Fasta    import Fasta\n",
      " \n",
      " __all__ = [\"Sequence\", \"Fasta\"]\n",
      "--- ./collision_detection_program/SBI/sequence/alignment/Needleman_Wunsch.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/sequence/alignment/Needleman_Wunsch.py\t(refactored)\n",
      "@@ -13,8 +13,8 @@\n",
      " import numpy as np\n",
      " import re\n",
      " \n",
      "-from SimilarityMatrix import SimilarityMatrix as SM\n",
      "-from SeqAli import SeqAli\n",
      "+from .SimilarityMatrix import SimilarityMatrix as SM\n",
      "+from .SeqAli import SeqAli\n",
      " \n",
      " \n",
      " def needleman_wunsch(sequence1, sequence2, similarity_matrix,\n",
      "--- ./collision_detection_program/SBI/sequence/alignment/SeqAli.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/sequence/alignment/SeqAli.py\t(refactored)\n",
      "@@ -85,7 +85,7 @@\n",
      "         for aliseq in sequences:\n",
      "             if isinstance(aliseq, Sequence):\n",
      "                 self._seq.append(aliseq)\n",
      "-            elif isinstance(aliseq, basestring):\n",
      "+            elif isinstance(aliseq, str):\n",
      "                 self._seq.append(Sequence(sequence = aliseq))\n",
      "             else:\n",
      "                 raise AttributeError('sequences must be specified as strings or Sequence objects.')\n",
      "@@ -382,7 +382,7 @@\n",
      "         @ptype:    {Integer}\n",
      "         '''\n",
      "         self._idx[refseq] += (new_index - 1)\n",
      "-        for x in xrange(len(self._segment[refseq])):\n",
      "+        for x in range(len(self._segment[refseq])):\n",
      "             self._segment[refseq][x] = self._segment[refseq][x] - 1 + self._idx[refseq]\n",
      " \n",
      "     def get_respective_coordinate(self, refseq, destseq, pos):\n",
      "@@ -453,7 +453,7 @@\n",
      "         coverage = 0\n",
      "         tk_seqs  = section._tokenize()\n",
      "         for x in range(len(tk_seqs[refseq])):\n",
      "-            if bool(tk_seqs[refseq][x]) and sum(map(lambda n: int(n[x]), tk_seqs)) > 1:\n",
      "+            if bool(tk_seqs[refseq][x]) and sum([int(n[x]) for n in tk_seqs]) > 1:\n",
      "                 coverage += 1\n",
      " \n",
      "         return float(coverage) / int(refseq_full_length)\n",
      "@@ -794,7 +794,7 @@\n",
      " \n",
      "         @return: {Integer}\n",
      "         '''\n",
      "-        for i in reversed(range(len(self))):\n",
      "+        for i in reversed(list(range(len(self)))):\n",
      "             if self._seq2ali[refseq][i] is not None:\n",
      "                 return self._seq2ali[refseq][i]\n",
      " \n",
      "@@ -872,7 +872,7 @@\n",
      "         tk_seqs = self._tokenize()\n",
      " \n",
      "         for x in range(len(self._seq[0])):\n",
      "-            profile_str = \"\".join(map(lambda n: n[x], tk_seqs))\n",
      "+            profile_str = \"\".join([n[x] for n in tk_seqs])\n",
      "             profile_vle = sum([int(i) for i in profile_str])\n",
      "             if profile_vle > 1:\n",
      "                 this_len += 1\n",
      "@@ -919,7 +919,7 @@\n",
      "             self._segment.append([])\n",
      "         tk_seqs = self._tokenize()\n",
      "         for x in range(len(self)):\n",
      "-            profile_str = \"\".join(map(lambda n: n[x], tk_seqs))\n",
      "+            profile_str = \"\".join([n[x] for n in tk_seqs])\n",
      " \n",
      "             for i in range(self._num_seq):\n",
      "                 p0 = self._sequence_position_id(i, x)\n",
      "@@ -973,7 +973,7 @@\n",
      "         complex_indexes = {}\n",
      "         for x in range(self.number_of_sequences):\n",
      "             seqInits[x] = self._sequence_position_from_alignment_position(x, index[0])\n",
      "-            complex_indexes[x] = list(filter(None, self._seq2ali[x]))\n",
      "+            complex_indexes[x] = list([_f for _f in self._seq2ali[x] if _f])\n",
      "             complex_indexes[x] = complex_indexes[x][complex_indexes[x].index(seqInits[x]):]\n",
      "         newali = self.__class__(sequences, seqInits)\n",
      "         for refident in complex_indexes:\n",
      "@@ -1073,7 +1073,7 @@\n",
      "         else:\n",
      "             if int(key) > len(self):\n",
      "                 raise IndexError\n",
      "-            return map(lambda n: n[int(key) - 1], self._seq)\n",
      "+            return [n[int(key) - 1] for n in self._seq]\n",
      " \n",
      "         index = key.indices(len(self) + 1)\n",
      " \n",
      "--- ./collision_detection_program/SBI/sequence/alignment/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/sequence/alignment/__init__.py\t(refactored)\n",
      "@@ -1,3 +1,3 @@\n",
      " __all__ = [\"SeqAli\", \"Rost\"]\n",
      "-from SeqAli        import SeqAli\n",
      "-from SeqAli        import Rost\n",
      "+from .SeqAli        import SeqAli\n",
      "+from .SeqAli        import Rost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Refactored ./collision_detection_program/SBI/structure/PDB.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/parse_pdb.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/atom/Atom.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/atom/AtomOfAminoAcid.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/atom/AtomOfNucleotide.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/structure/atom/__init__.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/structure/chain/Chain.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/chain/ChainOfNucleotide.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/chain/ChainOfProtein.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/structure/chain/__init__.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/structure/contacts/Complex.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/structure/contacts/InnerContacts.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ./collision_detection_program/SBI/structure/PDB.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/structure/PDB.py\t(refactored)\n",
      "@@ -523,7 +523,7 @@\n",
      "         \"\"\"\n",
      "         Process and load crystal data from a PDB formated file\n",
      "         \"\"\"\n",
      "-        from parse_pdb import read_PDB_file, read_PDB_header\n",
      "+        from .parse_pdb import read_PDB_file, read_PDB_header\n",
      "         if header:\n",
      "             read_PDB_header(self)\n",
      "             self._pdb_file.close()\n",
      "--- ./collision_detection_program/SBI/structure/atom/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/structure/atom/__init__.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " __all__ = ['Atom', 'AtomOfAminoAcid', 'AtomOfNucleotide']\n",
      " \n",
      "-from Atom             import Atom\n",
      "-from AtomOfAminoAcid  import AtomOfAminoAcid\n",
      "-from AtomOfNucleotide import AtomOfNucleotide\n",
      "+from .Atom             import Atom\n",
      "+from .AtomOfAminoAcid  import AtomOfAminoAcid\n",
      "+from .AtomOfNucleotide import AtomOfNucleotide\n",
      "--- ./collision_detection_program/SBI/structure/chain/Chain.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/structure/chain/Chain.py\t(refactored)\n",
      "@@ -494,7 +494,7 @@\n",
      " \n",
      "         @rtype: {Residue}\n",
      "         \"\"\"\n",
      "-        if self.dictitype.has_key(Rtype) and not self._term:\n",
      "+        if Rtype in self.dictitype and not self._term:\n",
      "             return self.resitype(number = number, version = version, Rtype = Rtype, mode = mode)\n",
      "         else:\n",
      "             return Residue(number = number, version = version, Rtype = Rtype, mode = mode)\n",
      "--- ./collision_detection_program/SBI/structure/chain/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/structure/chain/__init__.py\t(refactored)\n",
      "@@ -1,3 +1,3 @@\n",
      "-from Chain             import Chain\n",
      "-from ChainOfProtein    import ChainOfProtein\n",
      "-from ChainOfNucleotide import ChainOfNucleotide\n",
      "+from .Chain             import Chain\n",
      "+from .ChainOfProtein    import ChainOfProtein\n",
      "+from .ChainOfNucleotide import ChainOfNucleotide\n",
      "--- ./collision_detection_program/SBI/structure/contacts/Complex.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/structure/contacts/Complex.py\t(refactored)\n",
      "@@ -47,13 +47,13 @@\n",
      "     def biomolecule(self):  return self._biomolecule\n",
      " \n",
      "     @property\n",
      "-    def PPInterfaces(self): return self._PPInterface.values()\n",
      "+    def PPInterfaces(self): return list(self._PPInterface.values())\n",
      " \n",
      "     @property\n",
      "-    def PNInterfaces(self): return self._PNInterface.values()\n",
      "+    def PNInterfaces(self): return list(self._PNInterface.values())\n",
      " \n",
      "     @property\n",
      "-    def PHInterfaces(self): return self._PHInterface.values()\n",
      "+    def PHInterfaces(self): return list(self._PHInterface.values())\n",
      " \n",
      "     #\n",
      "     # PRIVATE FUNCTIONS\n",
      "@@ -94,13 +94,13 @@\n",
      " \n",
      "             SBIglobals.alert('debug', self, '\\tBiomolecule has {0:03} chains -> {1:03} max. Interfaces'.format(total_chains, (total_chains*(total_chains-1))/2))\n",
      " \n",
      "-            for i in xrange(len(protein_chains)):\n",
      "-                for j in xrange(i+1, len(protein_chains)):\n",
      "+            for i in range(len(protein_chains)):\n",
      "+                for j in range(i+1, len(protein_chains)):\n",
      "                     self._add_PPI(protein_chains[i], protein_chains[j], protein_pgeoms[i], protein_pgeoms[j])\n",
      "-                for j in xrange(len(protein_chains)):\n",
      "+                for j in range(len(protein_chains)):\n",
      "                     if i != j:\n",
      "                         self._add_PHI(protein_chains[i], protein_chains[j], protein_pgeoms[i], protein_hgeoms[j])\n",
      "-                for j in xrange(len(nucleotide_chains)):\n",
      "+                for j in range(len(nucleotide_chains)):\n",
      "                     self._add_PNI(protein_chains[i], nucleotide_chains[j], protein_pgeoms[i], nucleotide_ngeoms[j])\n",
      "                     self._add_PHI(protein_chains[i], nucleotide_chains[j], protein_pgeoms[i], nucleotide_hgeoms[j])\n",
      " \n",
      "--- ./collision_detection_program/SBI/structure/contacts/InnerContacts.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/structure/contacts/InnerContacts.py\t(refactored)\n",
      "@@ -40,13 +40,13 @@\n",
      "     def pdb(self):        return self._pdb\n",
      " \n",
      "     @property\n",
      "-    def AAcontacts(self): return self._AAcontacts.values()\n",
      "+    def AAcontacts(self): return list(self._AAcontacts.values())\n",
      " \n",
      "     @property\n",
      "-    def NCcontacts(self): return self._NCcontacts.values()\n",
      "+    def NCcontacts(self): return list(self._NCcontacts.values())\n",
      " \n",
      "     @property\n",
      "-    def HTcontacts(self): return self._HTcontacts.values()\n",
      "+    def HTcontacts(self): return list(self._HTcontacts.values())\n",
      " \n",
      "     #\n",
      "     # PRIVATE FUNCTIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Refactored ./collision_detection_program/SBI/structure/contacts/__init__.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/contacts/contact/Contact.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/contacts/contact/ContactAA.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/contacts/contact/ContactAH.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/contacts/contact/ContactAN.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/structure/contacts/contact/__init__.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/contacts/inner/PHInnerContact.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/contacts/inner/PPInnerContact.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/structure/contacts/inner/__init__.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/structure/contacts/interface/Interface.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/contacts/interface/PHInterface.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/contacts/interface/PNInterface.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/contacts/interface/PPInterface.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/structure/contacts/interface/__init__.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/geometry/RMSD.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/geometry/basics.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/header/BioMolecule.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/structure/header/DBreference.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/header/Experiment.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ./collision_detection_program/SBI/structure/contacts/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/structure/contacts/__init__.py\t(refactored)\n",
      "@@ -4,5 +4,5 @@\n",
      " \n",
      " from .inner        import PPInnerContact, PHInnerContact\n",
      " \n",
      "-from Complex       import Complex\n",
      "-from InnerContacts import InnerContacts\n",
      "+from .Complex       import Complex\n",
      "+from .InnerContacts import InnerContacts\n",
      "--- ./collision_detection_program/SBI/structure/contacts/contact/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/structure/contacts/contact/__init__.py\t(refactored)\n",
      "@@ -1,4 +1,4 @@\n",
      "-from Contact        import Contact\n",
      "-from ContactAA      import ContactAA\n",
      "-from ContactAN      import ContactAN\n",
      "-from ContactAH      import ContactAH\n",
      "+from .Contact        import Contact\n",
      "+from .ContactAA      import ContactAA\n",
      "+from .ContactAN      import ContactAN\n",
      "+from .ContactAH      import ContactAH\n",
      "--- ./collision_detection_program/SBI/structure/contacts/inner/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/structure/contacts/inner/__init__.py\t(refactored)\n",
      "@@ -1,2 +1,2 @@\n",
      "-from PPInnerContact import PPInnerContact\n",
      "-from PHInnerContact import PHInnerContact\n",
      "+from .PPInnerContact import PPInnerContact\n",
      "+from .PHInnerContact import PHInnerContact\n",
      "--- ./collision_detection_program/SBI/structure/contacts/interface/Interface.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/structure/contacts/interface/Interface.py\t(refactored)\n",
      "@@ -65,7 +65,7 @@\n",
      " \n",
      "     def reverse(self):\n",
      "         (self._chain1, self._chain2) = (self._chain2, self._chain1)\n",
      "-        map(lambda x: x.reverse(), self._contacts)\n",
      "+        list(map(lambda x: x.reverse(), self._contacts))\n",
      " \n",
      "     #\n",
      "     # PRIVATE METHODS\n",
      "--- ./collision_detection_program/SBI/structure/contacts/interface/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/structure/contacts/interface/__init__.py\t(refactored)\n",
      "@@ -1,4 +1,4 @@\n",
      "-from Interface      import Interface\n",
      "-from PPInterface    import PPInterface\n",
      "-from PNInterface    import PNInterface\n",
      "-from PHInterface    import PHInterface\n",
      "+from .Interface      import Interface\n",
      "+from .PPInterface    import PPInterface\n",
      "+from .PNInterface    import PNInterface\n",
      "+from .PHInterface    import PHInterface\n",
      "--- ./collision_detection_program/SBI/structure/header/DBreference.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/structure/header/DBreference.py\t(refactored)\n",
      "@@ -48,7 +48,7 @@\n",
      "         @raise AttributeError if line does not start with DBREF\n",
      "         '''\n",
      "         if not pdb_line.startswith('DBREF '):\n",
      "-            SBIglobals.throw(self, '{0} cannot create DBref'.format(pdb_line))\n",
      "+            SBIglobals.throw(self('{0} cannot create DBref'.format(pdb_line)))\n",
      "         data          = self._process_line(pdb_line)\n",
      " \n",
      "         self._pdb     = data[0]\n",
      "@@ -101,8 +101,8 @@\n",
      "         db_minicode = db_minicode.upper()\n",
      "         if not db_minicode in self.valid_references:\n",
      "             line1 = '{0} is not a valid DBref code'.format(db_minicode)\n",
      "-            line2 = 'Available codes: {0}'.format(self.valid_references.keys())\n",
      "-            SBIglobals.throw(self, \"\\n\".join([line1, line2]))\n",
      "+            line2 = 'Available codes: {0}'.format(list(self.valid_references.keys()))\n",
      "+            SBIglobals.throw(self(\"\\n\".join([line1, line2])))\n",
      " \n",
      "         #SWS and TREMBL are parts of UNP\n",
      "         if db_minicode == 'UNP' and self._db in ['SWS', 'TREMBL']:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Refactored ./collision_detection_program/SBI/structure/header/Header.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/header/HeteroAtom.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/header/MiniRes.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/header/Molecule.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/header/SecondaryStructure.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/header/Site.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/header/__init__.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/protein/Arch.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ./collision_detection_program/SBI/structure/header/Header.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/structure/header/Header.py\t(refactored)\n",
      "@@ -130,7 +130,7 @@\n",
      " \n",
      "     @property\n",
      "     def chains(self):\n",
      "-        return self._chaindict.keys()\n",
      "+        return list(self._chaindict.keys())\n",
      " \n",
      "     @property\n",
      "     def sites(self):\n",
      "@@ -170,7 +170,7 @@\n",
      "     @property\n",
      "     def are_molecules_processed(self):\n",
      "         if len(self._molecules) > 0:\n",
      "-            k = self._molecules.keys()\n",
      "+            k = list(self._molecules.keys())\n",
      "             return self._molecules[k[0]].is_processed\n",
      "         else:\n",
      "             return True  # To avoid call the function\n",
      "@@ -319,11 +319,11 @@\n",
      "         if len(self._keywords) == 1 and self._keywords[0] == '':\n",
      "             self._keywords = []\n",
      "         # Assigning sites to chain dictionary\n",
      "-        for k, v in self.sites.iteritems():\n",
      "+        for k, v in self.sites.items():\n",
      "             for c in v.chains:\n",
      "                 if c not in self._chaindict:\n",
      "                     if len(self._molecules) > 0:\n",
      "-                        nm = max([int(x) for x in self._molecules.keys()]) + 1\n",
      "+                        nm = max([int(x) for x in list(self._molecules.keys())]) + 1\n",
      "                     else:\n",
      "                         nm = 1\n",
      "                     self._molecules[nm] = Molecule(self.pdb)\n",
      "@@ -334,11 +334,11 @@\n",
      "                 self._chaindict[c].setdefault('SITES', [])\n",
      "                 self._chaindict[c]['SITES'].append(v)\n",
      "         # Assigning heteroatoms to chain dictionary\n",
      "-        for k, v in self.hetero.iteritems():\n",
      "+        for k, v in self.hetero.items():\n",
      "             for c in v.chain:\n",
      "                 if c not in self._chaindict:\n",
      "                     if len(self._molecules) > 0:\n",
      "-                        nm = max([int(x) for x in self._molecules.keys()]) + 1\n",
      "+                        nm = max([int(x) for x in list(self._molecules.keys())]) + 1\n",
      "                     else:\n",
      "                         nm = 1\n",
      "                     self._molecules[nm] = Molecule(self.pdb)\n",
      "@@ -350,7 +350,7 @@\n",
      "         for ss in self.secondary_structures:\n",
      "             if ss.chain not in self._chaindict:\n",
      "                 if len(self._molecules) > 0:\n",
      "-                        nm = max([int(x) for x in self._molecules.keys()]) + 1\n",
      "+                        nm = max([int(x) for x in list(self._molecules.keys())]) + 1\n",
      "                 else:\n",
      "                     nm = 1\n",
      "                 self._molecules[nm] = Molecule(self.pdb)\n",
      "@@ -358,7 +358,7 @@\n",
      "             self._chaindict[ss.chain].setdefault('SSTRUC', [])\n",
      "             self._chaindict[ss.chain]['SSTRUC'].append(ss)\n",
      "         # Assigning all chains to Symmetry Matrix\n",
      "-        self._symmetryM.chains = self._chaindict.keys()\n",
      "+        self._symmetryM.chains = list(self._chaindict.keys())\n",
      " \n",
      "     #\n",
      "     # FUNCTIONS\n",
      "@@ -377,7 +377,7 @@\n",
      "                            self.pdb,               chain)\n",
      " \n",
      "     def has_hetero(self, heteroID):\n",
      "-        return heteroID in self.hetero.keys()\n",
      "+        return heteroID in list(self.hetero.keys())\n",
      " \n",
      "     def get_hetero_by_name_like(self, hetero_name):\n",
      "         myheteros = []\n",
      "@@ -403,7 +403,7 @@\n",
      "                 'replaces':       self.replaced\n",
      "                 }\n",
      " \n",
      "-        for k, x in self.molecules.iteritems():\n",
      "+        for k, x in self.molecules.items():\n",
      "             data['molecules'].append(x.as_dict())\n",
      " \n",
      "         if self.experiment is not None:\n",
      "@@ -413,10 +413,10 @@\n",
      "             data['dbrefs']  = [x.as_dict() for x in self.dbrefs]\n",
      " \n",
      "         if len(self.sites) > 0:\n",
      "-            data['sites']   = [x.as_dict() for k, x in self.sites.iteritems()]\n",
      "+            data['sites']   = [x.as_dict() for k, x in self.sites.items()]\n",
      " \n",
      "         if len(self.hetero) > 0:\n",
      "-            data['hetatm']  = [x.as_dict() for k, x in self.hetero.iteritems()]\n",
      "+            data['hetatm']  = [x.as_dict() for k, x in self.hetero.items()]\n",
      " \n",
      "         if len(self.secondary_structures) > 0:\n",
      "             data['sstruct'] = [x.as_dict() for x in self.secondary_structures]\n",
      "--- ./collision_detection_program/SBI/structure/protein/SShelper.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/structure/protein/SShelper.py\t(refactored)\n",
      "@@ -3,8 +3,8 @@\n",
      " from random             import randint\n",
      " from ..atom             import AtomOfAminoAcid\n",
      " from SBI.external.DSSP  import DSSPExe\n",
      "-from Arch               import Arch\n",
      "-from SecondaryStructure import SecondaryStructure\n",
      "+from .Arch               import Arch\n",
      "+from .SecondaryStructure import SecondaryStructure\n",
      " \n",
      " \n",
      " def calculate_dssp(pdb, tmppdb=None, tmpdssp=None, cleanfiles=True):\n",
      "--- ./collision_detection_program/SBI/structure/protein/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/structure/protein/__init__.py\t(refactored)\n",
      "@@ -1,6 +1,6 @@\n",
      " __all__ = [\"SecondaryStructure\", \"Arch\"]\n",
      " \n",
      "-from SecondaryStructure import SecondaryStructure\n",
      "-from Arch               import Arch\n",
      "-import SShelper\n",
      "-import Sequencer\n",
      "+from .SecondaryStructure import SecondaryStructure\n",
      "+from .Arch               import Arch\n",
      "+from . import SShelper\n",
      "+from . import Sequencer\n",
      "--- ./collision_detection_program/SBI/structure/residue/__init__.py\t(original)\n",
      "+++ ./collision_detection_program/SBI/structure/residue/__init__.py\t(refactored)\n",
      "@@ -1,3 +1,3 @@\n",
      "-from Residue             import Residue\n",
      "-from ResidueOfNucleotide import ResidueOfNucleotide\n",
      "-from ResidueOfAminoAcid  import ResidueOfAminoAcid\n",
      "+from .Residue             import Residue\n",
      "+from .ResidueOfNucleotide import ResidueOfNucleotide\n",
      "+from .ResidueOfAminoAcid  import ResidueOfAminoAcid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Refactored ./collision_detection_program/SBI/structure/protein/SShelper.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/protein/SecondaryStructure.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/protein/Sequencer.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/structure/protein/__init__.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/residue/Residue.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/residue/ResidueOfAminoAcid.py\n",
      "RefactoringTool: No changes to ./collision_detection_program/SBI/structure/residue/ResidueOfNucleotide.py\n",
      "RefactoringTool: Refactored ./collision_detection_program/SBI/structure/residue/__init__.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: ./SPServerFold.py\n",
      "RefactoringTool: ./SPServerPPI.py\n",
      "RefactoringTool: ./cif2pdb.py\n",
      "RefactoringTool: ./BioLib/__init__.py\n",
      "RefactoringTool: ./BioLib/Algebra/Transforms.py\n",
      "RefactoringTool: ./BioLib/Algebra/__init__.py\n",
      "RefactoringTool: ./BioLib/Docking/FTDock.py\n",
      "RefactoringTool: ./BioLib/Docking/HEXDock.py\n",
      "RefactoringTool: ./BioLib/Docking/PATCHDock.py\n",
      "RefactoringTool: ./BioLib/Docking/ScoringFunctions.py\n",
      "RefactoringTool: ./BioLib/Docking/SplitPotentials.py\n",
      "RefactoringTool: ./BioLib/Docking/SplitPotentialsPPI.py\n",
      "RefactoringTool: ./BioLib/Docking/ZDock.py\n",
      "RefactoringTool: ./BioLib/Docking/__init__.py\n",
      "RefactoringTool: ./BioLib/Fold/SplitPotentialsFold.py\n",
      "RefactoringTool: ./BioLib/Fold/__init__.py\n",
      "RefactoringTool: ./BioLib/ILoops/ILoopsParser.py\n",
      "RefactoringTool: ./BioLib/ILoops/ILoopsParserMinidom.py\n",
      "RefactoringTool: ./BioLib/ILoops/__init__.py\n",
      "RefactoringTool: ./BioLib/Linker/linkerTools.py\n",
      "RefactoringTool: ./BioLib/Sequence/FastaParser.py\n",
      "RefactoringTool: ./BioLib/Sequence/__init__.py\n",
      "RefactoringTool: ./BioLib/Structure/Atom.py\n",
      "RefactoringTool: ./BioLib/Structure/Interaction.py\n",
      "RefactoringTool: ./BioLib/Structure/Interaction_old.py\n",
      "RefactoringTool: ./BioLib/Structure/Loop.py\n",
      "RefactoringTool: ./BioLib/Structure/PDB.py\n",
      "RefactoringTool: ./BioLib/Structure/Residue.py\n",
      "RefactoringTool: ./BioLib/Structure/Structure.py\n",
      "RefactoringTool: ./BioLib/Structure/__init__.py\n",
      "RefactoringTool: ./BioLib/Tools/BioExceptions.py\n",
      "RefactoringTool: ./BioLib/Tools/Dssp.py\n",
      "RefactoringTool: ./BioLib/Tools/Submitters.py\n",
      "RefactoringTool: ./BioLib/Tools/__init__.py\n",
      "RefactoringTool: ./CASP12_analysis/scripts/DOPE_Extractor.py\n",
      "RefactoringTool: ./CASP12_analysis/scripts/GDT_Extractor.py\n",
      "RefactoringTool: ./CASP12_analysis/scripts/PROSA_extract.py\n",
      "RefactoringTool: ./CASP12_analysis/scripts/SPS_residues.py\n",
      "RefactoringTool: ./CASP12_analysis/scripts/analyze_casp_subset.py\n",
      "RefactoringTool: ./CASP12_analysis/scripts/make_bootstrapping.py\n",
      "RefactoringTool: ./CASP12_analysis/scripts/parse_casp_results.py\n",
      "RefactoringTool: ./CASP12_analysis/scripts/plot_global_scores.py\n",
      "RefactoringTool: ./CASP12_analysis/scripts/plot_residue_scores.py\n",
      "RefactoringTool: ./CASP12_analysis/scripts/plot_results_per_target.py\n",
      "RefactoringTool: ./CASP12_analysis/scripts/residue_score.py\n",
      "RefactoringTool: ./CASP12_analysis/scripts/run_CASP_targets_SPServer_cluster.py\n",
      "RefactoringTool: ./CASP12_analysis/scripts/zscore.py\n",
      "RefactoringTool: ./PPI/scripts/TransformMatrix.py\n",
      "RefactoringTool: ./SBI/__init__.py\n",
      "RefactoringTool: ./SBI/beans/Executable.py\n",
      "RefactoringTool: ./SBI/beans/IndexedNum.py\n",
      "RefactoringTool: ./SBI/beans/JSONer.py\n",
      "RefactoringTool: ./SBI/beans/Path.py\n",
      "RefactoringTool: ./SBI/beans/StorableObject.py\n",
      "RefactoringTool: ./SBI/beans/__init__.py\n",
      "RefactoringTool: ./SBI/beans/butler.py\n",
      "RefactoringTool: ./SBI/beans/file.py\n",
      "RefactoringTool: ./SBI/beans/singleton.py\n",
      "RefactoringTool: ./SBI/data/__init__.py\n",
      "RefactoringTool: ./SBI/databases/DrugBanklink.py\n",
      "RefactoringTool: ./SBI/databases/Enzymelink.py\n",
      "RefactoringTool: ./SBI/databases/GOlink.py\n",
      "RefactoringTool: ./SBI/databases/PDBTMlink.py\n",
      "RefactoringTool: ./SBI/databases/PDBeChemlink.py\n",
      "RefactoringTool: ./SBI/databases/PDBlink.py\n",
      "RefactoringTool: ./SBI/databases/SCOPlink.py\n",
      "RefactoringTool: ./SBI/databases/TaxIDlink.py\n",
      "RefactoringTool: ./SBI/databases/Uniprotlink.py\n",
      "RefactoringTool: ./SBI/databases/__Uniprotlink.py\n",
      "RefactoringTool: ./SBI/databases/__init__.py\n",
      "RefactoringTool: ./SBI/databases/dblink.py\n",
      "RefactoringTool: ./SBI/databases/uniprot/__init__.py\n",
      "RefactoringTool: ./SBI/databases/uniprot/connect.py\n",
      "RefactoringTool: ./SBI/databases/uniprot/uniprot.py\n",
      "RefactoringTool: ./SBI/external/ExternalExe.py\n",
      "RefactoringTool: ./SBI/external/__init__.py\n",
      "RefactoringTool: ./SBI/external/CDhit/CDhit.py\n",
      "RefactoringTool: ./SBI/external/CDhit/CDhitExe.py\n",
      "RefactoringTool: ./SBI/external/CDhit/CDhitHomolog.py\n",
      "RefactoringTool: ./SBI/external/CDhit/CDhitList.py\n",
      "RefactoringTool: ./SBI/external/CDhit/__init__.py\n",
      "RefactoringTool: ./SBI/external/DSSP/DSSP.py\n",
      "RefactoringTool: ./SBI/external/DSSP/DSSPExe.py\n",
      "RefactoringTool: ./SBI/external/DSSP/__init__.py\n",
      "RefactoringTool: ./SBI/external/blast/BlastExe.py\n",
      "RefactoringTool: ./SBI/external/blast/BlastHit.py\n",
      "RefactoringTool: ./SBI/external/blast/BlastResult.py\n",
      "RefactoringTool: ./SBI/external/blast/__init__.py\n",
      "RefactoringTool: ./SBI/external/hmmer/HmmExe.py\n",
      "RefactoringTool: ./SBI/external/hmmer/HmmHit.py\n",
      "RefactoringTool: ./SBI/external/hmmer/HmmResult.py\n",
      "RefactoringTool: ./SBI/external/hmmer/__init__.py\n",
      "RefactoringTool: ./SBI/math/__init__.py\n",
      "RefactoringTool: ./SBI/math/stats.py\n",
      "RefactoringTool: ./SBI/sequence/Fasta.py\n",
      "RefactoringTool: ./SBI/sequence/Sequence.py\n",
      "RefactoringTool: ./SBI/sequence/__init__.py\n",
      "RefactoringTool: ./SBI/sequence/alignment/Needleman_Wunsch.py\n",
      "RefactoringTool: ./SBI/sequence/alignment/SeqAli.py\n",
      "RefactoringTool: ./SBI/sequence/alignment/SimilarityMatrix.py\n",
      "RefactoringTool: ./SBI/sequence/alignment/__init__.py\n",
      "RefactoringTool: ./SBI/structure/PDB.py\n",
      "RefactoringTool: ./SBI/structure/parse_pdb.py\n",
      "RefactoringTool: ./SBI/structure/atom/Atom.py\n",
      "RefactoringTool: ./SBI/structure/atom/AtomOfAminoAcid.py\n",
      "RefactoringTool: ./SBI/structure/atom/AtomOfNucleotide.py\n",
      "RefactoringTool: ./SBI/structure/atom/__init__.py\n",
      "RefactoringTool: ./SBI/structure/chain/Chain.py\n",
      "RefactoringTool: ./SBI/structure/chain/ChainOfNucleotide.py\n",
      "RefactoringTool: ./SBI/structure/chain/ChainOfProtein.py\n",
      "RefactoringTool: ./SBI/structure/chain/__init__.py\n",
      "RefactoringTool: ./SBI/structure/contacts/Complex.py\n",
      "RefactoringTool: ./SBI/structure/contacts/InnerContacts.py\n",
      "RefactoringTool: ./SBI/structure/contacts/__init__.py\n",
      "RefactoringTool: ./SBI/structure/contacts/contact/Contact.py\n",
      "RefactoringTool: ./SBI/structure/contacts/contact/ContactAA.py\n",
      "RefactoringTool: ./SBI/structure/contacts/contact/ContactAH.py\n",
      "RefactoringTool: ./SBI/structure/contacts/contact/ContactAN.py\n",
      "RefactoringTool: ./SBI/structure/contacts/contact/__init__.py\n",
      "RefactoringTool: ./SBI/structure/contacts/inner/PHInnerContact.py\n",
      "RefactoringTool: ./SBI/structure/contacts/inner/PPInnerContact.py\n",
      "RefactoringTool: ./SBI/structure/contacts/inner/__init__.py\n",
      "RefactoringTool: ./SBI/structure/contacts/interface/Interface.py\n",
      "RefactoringTool: ./SBI/structure/contacts/interface/PHInterface.py\n",
      "RefactoringTool: ./SBI/structure/contacts/interface/PNInterface.py\n",
      "RefactoringTool: ./SBI/structure/contacts/interface/PPInterface.py\n",
      "RefactoringTool: ./SBI/structure/contacts/interface/__init__.py\n",
      "RefactoringTool: ./SBI/structure/geometry/RMSD.py\n",
      "RefactoringTool: ./SBI/structure/geometry/basics.py\n",
      "RefactoringTool: ./SBI/structure/header/BioMolecule.py\n",
      "RefactoringTool: ./SBI/structure/header/DBreference.py\n",
      "RefactoringTool: ./SBI/structure/header/Experiment.py\n",
      "RefactoringTool: ./SBI/structure/header/Header.py\n",
      "RefactoringTool: ./SBI/structure/header/HeteroAtom.py\n",
      "RefactoringTool: ./SBI/structure/header/MiniRes.py\n",
      "RefactoringTool: ./SBI/structure/header/Molecule.py\n",
      "RefactoringTool: ./SBI/structure/header/SecondaryStructure.py\n",
      "RefactoringTool: ./SBI/structure/header/Site.py\n",
      "RefactoringTool: ./SBI/structure/header/__init__.py\n",
      "RefactoringTool: ./SBI/structure/protein/Arch.py\n",
      "RefactoringTool: ./SBI/structure/protein/SShelper.py\n",
      "RefactoringTool: ./SBI/structure/protein/SecondaryStructure.py\n",
      "RefactoringTool: ./SBI/structure/protein/Sequencer.py\n",
      "RefactoringTool: ./SBI/structure/protein/__init__.py\n",
      "RefactoringTool: ./SBI/structure/residue/Residue.py\n",
      "RefactoringTool: ./SBI/structure/residue/ResidueOfAminoAcid.py\n",
      "RefactoringTool: ./SBI/structure/residue/ResidueOfNucleotide.py\n",
      "RefactoringTool: ./SBI/structure/residue/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/collision_detection.py\n",
      "RefactoringTool: ./collision_detection_program/collision_detection_from_list.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/beans/Executable.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/beans/IndexedNum.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/beans/JSONer.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/beans/Path.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/beans/StorableObject.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/beans/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/beans/butler.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/beans/file.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/beans/singleton.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/data/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/databases/DrugBanklink.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/databases/Enzymelink.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/databases/GOlink.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/databases/PDBTMlink.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/databases/PDBeChemlink.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/databases/PDBlink.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/databases/SCOPlink.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/databases/TaxIDlink.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/databases/Uniprotlink.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/databases/__Uniprotlink.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/databases/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/databases/dblink.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/databases/uniprot/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/databases/uniprot/connect.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/databases/uniprot/uniprot.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/external/ExternalExe.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/external/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/external/CDhit/CDhit.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/external/CDhit/CDhitExe.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/external/CDhit/CDhitHomolog.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/external/CDhit/CDhitList.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/external/CDhit/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/external/DSSP/DSSP.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/external/DSSP/DSSPExe.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/external/DSSP/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/external/blast/BlastExe.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/external/blast/BlastHit.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/external/blast/BlastResult.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/external/blast/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/external/hmmer/HmmExe.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/external/hmmer/HmmHit.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/external/hmmer/HmmResult.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/external/hmmer/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/math/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/math/stats.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/sequence/Fasta.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/sequence/Sequence.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/sequence/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/sequence/alignment/Needleman_Wunsch.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/sequence/alignment/SeqAli.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/sequence/alignment/SimilarityMatrix.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/sequence/alignment/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/PDB.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/parse_pdb.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/atom/Atom.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/atom/AtomOfAminoAcid.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/atom/AtomOfNucleotide.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/atom/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/chain/Chain.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/chain/ChainOfNucleotide.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/chain/ChainOfProtein.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/chain/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/contacts/Complex.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/contacts/InnerContacts.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/contacts/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/contacts/contact/Contact.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/contacts/contact/ContactAA.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/contacts/contact/ContactAH.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/contacts/contact/ContactAN.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/contacts/contact/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/contacts/inner/PHInnerContact.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/contacts/inner/PPInnerContact.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/contacts/inner/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/contacts/interface/Interface.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/contacts/interface/PHInterface.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/contacts/interface/PNInterface.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/contacts/interface/PPInterface.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/contacts/interface/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/geometry/RMSD.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/geometry/basics.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/header/BioMolecule.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/header/DBreference.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/header/Experiment.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/header/Header.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/header/HeteroAtom.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/header/MiniRes.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/header/Molecule.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/header/SecondaryStructure.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/header/Site.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/header/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/protein/Arch.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/protein/SShelper.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/protein/SecondaryStructure.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/protein/Sequencer.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/protein/__init__.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/residue/Residue.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/residue/ResidueOfAminoAcid.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/residue/ResidueOfNucleotide.py\n",
      "RefactoringTool: ./collision_detection_program/SBI/structure/residue/__init__.py\n",
      "RefactoringTool: Warnings/messages while refactoring:\n",
      "RefactoringTool: ### In file ./SBI/sequence/alignment/Needleman_Wunsch.py ###\n",
      "RefactoringTool: Line 79: could not convert: raise 'Not Possible'\n",
      "RefactoringTool: Python 3 does not support string exceptions\n",
      "RefactoringTool: ### In file ./SBI/sequence/alignment/SimilarityMatrix.py ###\n",
      "RefactoringTool: Line 18: could not convert: raise \"NO!\"\n",
      "RefactoringTool: Python 3 does not support string exceptions\n",
      "RefactoringTool: ### In file ./SBI/structure/contacts/interface/Interface.py ###\n",
      "RefactoringTool: Line 68: You should use a for loop here\n",
      "RefactoringTool: ### In file ./SBI/structure/geometry/basics.py ###\n",
      "RefactoringTool: Line 21: could not convert: raise 'vectors have different shape'\n",
      "RefactoringTool: Python 3 does not support string exceptions\n",
      "RefactoringTool: ### In file ./collision_detection_program/SBI/sequence/alignment/Needleman_Wunsch.py ###\n",
      "RefactoringTool: Line 79: could not convert: raise 'Not Possible'\n",
      "RefactoringTool: Python 3 does not support string exceptions\n",
      "RefactoringTool: ### In file ./collision_detection_program/SBI/sequence/alignment/SimilarityMatrix.py ###\n",
      "RefactoringTool: Line 18: could not convert: raise \"NO!\"\n",
      "RefactoringTool: Python 3 does not support string exceptions\n",
      "RefactoringTool: ### In file ./collision_detection_program/SBI/structure/contacts/interface/Interface.py ###\n",
      "RefactoringTool: Line 68: You should use a for loop here\n",
      "RefactoringTool: ### In file ./collision_detection_program/SBI/structure/geometry/basics.py ###\n",
      "RefactoringTool: Line 21: could not convert: raise 'vectors have different shape'\n",
      "RefactoringTool: Python 3 does not support string exceptions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['2to3', '-w', '.'], returncode=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"2to3\", \"-w\", \".\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after running change Dssp.py line 26 to\n",
    "# /content/SPServer/BioLib/Tools/Dssp.py\n",
    "# dssp_out, dssp_err = p.communicate(str(structure).encode())\n",
    "# line 29 to\n",
    "# for line in dssp_out.decode().split('\\n'):\n",
    "\n",
    "# fix indentation mess up\n",
    "# /content/SPServer/SBI/structure/residue/ResidueOfNucleotide.py\n",
    "\n",
    "# /content/SPServer/SBI/beans/butler.py:295: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
    "#   if callerID is '__main__':\n",
    "# /content/SPServer/SBI/beans/butler.py:327: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
    "#   if callerID is '__main__':\n",
    "# /content/SPServer/SBI/beans/butler.py:365: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
    "#   if callerID is '__main__':\n",
    "# change to ==\n",
    "\n",
    "# change line 510 in SPServerPPI.py to\n",
    "# os.makedirs(directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = [\n",
    "    \"python\",\n",
    "    \"SPServerPPI.py\",\n",
    "    \"-i\", \"test_unrelaxed_rank_001_alphafold2_multimer_v3_model_1_seed_000.pdb\",\n",
    "    \"-r\", \"A\",\n",
    "    \"-l\", \"A\",\n",
    "    \"-s\", \"pdb_together\",\n",
    "    \"-o\", \"output\",\n",
    "    \"-j\", \"test_reference\",\n",
    "    \"-p\", \"CB\",\n",
    "    \"-c\"\n",
    "]\n",
    "subprocess.run(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
